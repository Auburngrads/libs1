---
title: '`r SMRD:::info("book")`'
subtitle: '`r SMRD:::info("chapter9")`'
author: '`r SMRD:::info("authors")`'
date: '`r format(Sys.Date(), "%d %B %Y")`'
footer: '`r paste("SMRD: ", SMRD:::info("chapter9"))`'
output:
  slidy_presentation:
    smart: false
    fig_caption: yes
graphics: yes
runtime: shiny
---

```{r echo=FALSE,message=FALSE, warning=FALSE}
source(jkf::SCRIPT('setup.R'))
library(SMRD)
```

# CHAPTER OVERVIEW

```{r}
shiny::includeCSS(jkf::SCRIPT('flat-slidy.css'))
shiny::includeScript(jkf::SCRIPT("jquery.min.js"))
shiny::includeScript(jkf::SCRIPT("jkf-scroll.js"))
```

## This chapter explains...

- The use of <red>computer simulation</red> to obtain confidence intervals.  Such intervals are known as bootstrap intervals

- Methods for generating bootstrap samples

- How to obtain and interpret simulation-based <u>__parametric pointwise__</u> bootstrap confidence intervals

- How to obtain and interpret simulation-based <u>__nonparametric pointwise__</u> bootstrap confidence intervals

# 9.1 - Introduction

## Comparing CI Methods
 
- CI's based on the normal approximation 

    + Are adequate for informal analyses

    + Have good coverage probabilities when the sample size is large

- CI's based on the relative likelihood

    + Have coverage probabilities closer to $1-\alpha$ than CI's based on normal approximation

    + Can be unreasonably computationally intensive

- CI's based on simulation (bootstrapping)

    + Have better coverage probabilities than those based on the normal approximation

    + Have coverage probabilities that can be competitive with likelihood-based methods

# 9.2 - Bootstrap Sampling

# 9.2.1 - General Idea
 
<font color="blue">__Objective:__</font><br>Want to base the CI on the <u>___true___</u> distribution of the estimator, not a large sample approximation

<font color="blue">__Problem:__</font><br>Observed sample data provides limited information on the overall population <u>___(a snapshot)___</u>

<font color="blue">__Solution:__</font><br>If the sample data was produced by the same process governing the overall population we can... 

- Repeatedly simulate the sampling process that generated the original data (create many snapshots)
- Use the collective information from these snapshots to compute a confidence interval
- This collection of snapshots is called a bootstrap sample

<div class='example'>
### Bootstrapping Example

## Background

- Suppose we're interested in computing a $100(1-\alpha)\%$ CI for some value $\theta \in \mathbb{R}^+$

- Since $\theta$ is strictly positive, a CI based on the normal approximation would assume

$$Z_{\log(\hat{\theta)}}=\frac{\log(\hat{\theta})-\log(\theta)}{\widehat{se}_{\log(hat{\theta})}}\sim NOR(0,1)$$

- But, what if this assumption was not valid for some situation?

    + Simulation allows us to base the CI on the _actual_ distribution of $Z_{\log(\hat{\theta)}}$ 
    
    + <focus>Note</focus> using the $\log$ transformation can provide advantages even in simulation
</div>

# 9.2.2 - Bootstrap Sampling Methods
 
## Several bootstrap sampling methods exist

- We'll focus on two of them

    + Parametric bootstrap sampling

    + Nonparametric bootstrap sampling

- First, let's make sure that we've defined all our terms

    + $\;\;F(t|\theta) \rightarrow$ The population cdf based on the true, but unknown value of $\theta$

    + $\;\;\;\;\;\;\;\;\;n \rightarrow$ The size of our observed data (the initial snapshot)

    + $\;F(t|\hat{\theta})\rightarrow$ The population cdf based on our estimate of $\theta$

    + $F(t|\hat{\theta}^*_i)\rightarrow$ The population cdf based on the $i^{th}$ bootstrap estimate of $\theta$

    + $\;\;\;\;\;\;\;\;\;B \rightarrow$ The size of the bootstrap sample (number of simulations)  

## Procedure to compute a bootstrap CI 

1) Observe $n$ observations generated from some process with unknown distrubution $F(t|\mathbf{\theta})$

2) Use maximum likelihood to estimate the form of $F(t)$ and the value(s) of $\hat{\theta}$

3) Generate bootstrap samples of size $n$ from the estimated distribution

    + For bootstrap sample $i = 1$ simulate $n$ observations from $F(t|\mathbf{\hat{\theta}})$
    
    + Use ML to estimate $\hat{\theta}^*_1$ for bootstrap sample $1$

    + Repeat this process of sampling & estimating until $B$ samples have been generated and $B$ estimates of $\hat{\theta}^*_i, i = 1,...,B$ have been computed
    
    + $B$ should be a large number <focus>10,000 is a small number of bootstrap sample</focus>

4) Order the estimated values $\hat{\theta}^*_i, i = 1,...,B$ from smallest to largest

5) The upper and lower endpoints of the $95\%$ CI for $\theta$ using the bootstrap method are found as

    + The lower endpoint is the value of $\hat{\theta}^*_i$ that is the $B \times 2.5%$ smallest

    + The upper endpoint is the value of $\hat{\theta}^*_i$ that is the $B \times 97.5%$ smallest 

# 9.3 - Exponential Distribution Confidence Intervals

# 9.3.1 - Bootstrap CI for $\theta$
 

# 9.3.2 - Bootstrap CI for $f(\theta)$
 

# 9.3.3 - Comparison of CI methods
 

# 9.4 - Weibull, Lognormal and Loglogistic Distribution CI's

# 9.4.1 - Construction of CI for parameters $\bar{p}$
 

# 9.4.2 - Construction of CI for $f(\bar{p})$
 

# 9.4.3 - Comparison of CI methods
 

# 9.5 - Nonparametric Bootstrap Confidence Intervals

# 9.5.1 - Nonparametric bootstrap sampling
 

# 9.5.2 - Warning on NP bootstrap estimates
 

# 9.5.3 - Distribution of bootstrap statistics
 

# 9.5.4 - Pointwise NP bootstrap CI's for $F(t_i)$
 

# 9.6 - Percentile Bootstrap Method
