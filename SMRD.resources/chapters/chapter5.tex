%chapter 5
%original by wqmeeker  12 Jan 94
%edited by wqmeeker  19 mar 94 
%edited by wqmeeker  26 mar 94 
%edited by driker 28 mar 94
%edited by driker 30 mar 94
%edited by wqmeeker  30/31 mar 94 adding logistic and other stuff
%edited by lae  april 10, correct typos on exercises 5 and 7 (a)
%edited by wqmeeker  1 june 94
%edited by driker 13 july 94
%edited by wqmeeker  2 aug 94
%edited by wqmeeker  7 aug 94
%edited by wqmeeker  9 aug 94 split from old chapter 4
%edited by wqmeeker  14 sept 94 adding new dists
%edited by wqmeeker  20 sept 94 adding moment plot
%edited by wqmeeker  22 sept 94 reorganization
%edited by wqmeeker  4/5 oct 94 integrate new stuff from luis
%edited by driker 22 nov 94
%edited by wqmeeker 27 nov 94
%edited by driker 2 feb 95
%edited by wqmeeker 15 feb 95
%edited by driker 18 aug 95
%edited by driker 11 nov 95
%edited by driker 25 may 96 Wayne's comments
%edited by wqmeeker 29 june 96 tuning
%edited by driker 1 july 96
%edited by lae    april 9  1997 change GNG \GENG and GNF \GENF
%edited by lae    april 20 1997 bring in new psnups stuff
%edited by lae    april 25 1997 additional changes suggested by meeker
\setcounter{chapter}{4}

\chapter{Other Parametric Distributions}
\label{chapter:other.parametric.models}

\input{\chapterhome/common_heading.tex}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Objectives}
This chapter explains:
\begin{itemize}
\item 
The properties and the importance
of various parametric distributions 
that cannot be transformed into a location-scale distribution.
\item 
Threshold-parameter distributions.
\item
How some statistical distributions can be determined 
by applying basic ideas of probability theory to
physical properties of a failure process, system, or population
of units.
\end{itemize}


%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Overview}
This chapter is a continuation of
Chapter~\ref{chapter:ls.parametric.models}, describing more advanced
parametric distributions. This chapter is a prerequisite only for
Chapter~\ref{chapter:ml.other.parametric} and may otherwise be
omitted.  Section~\ref{section:gamma.distribution} describes the
gamma distribution while Section~\ref{section:gng.distribution}
describes the generalized gamma and the extended generalized gamma
distributions.  The generalized gamma distribution contains the
lognormal and Weibull distributions as special cases and is thus
useful for statistical assessment of the best fitting distribution.
Sections~\ref{section:gen.f} through \ref{section:gets.dist}
describe and compare a variety of other potentially useful
parametric distributions. Some of these were developed on the basis
of physical theory.  Section~\ref{section:other.deriv} describes
other methods of deriving useful probability distributions from
physical or other considerations.

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Introduction}

Chapter~\ref{chapter:ls.parametric.models} introduced a number of
important probability distributions, all of which belong to or could
be transformed into a distribution that belongs to the location-scale
family of distributions.  This chapter describes a number of
additional probability distributions that have also been useful in
reliability data analysis.

It is not necessary to be familiar with all of the formulas and other
details in this chapter in order to use these distributions in
reliability applications. They are included to show some of the
connections among various probability distributions that are commonly
used in reliability modeling and to provide background for the
application of these distributions in
Chapters~\ref{chapter:probability.plotting} and
\ref{chapter:ml.other.parametric}.


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Gamma Distribution}
\label{section:gamma.distribution}
\subsection{Gamma cdf, pdf, and hazard function}
When $\rv$ has a gamma distribution, 
we indicate this by $\rv \sim \GAM(\theta, \gammashape)$.
The gamma distribution
cdf and pdf are
\begin{eqnarray}  
\label{equation:gamma.cdf} 
 F(\realrv;\theta, \gammashape)&=&\incgamma
 \left (
 \frac{
 \realrv
      }
      {
      \theta
      };  \gammashape
 \right ) \\ 
\label{equation:gamma.pdf}
 f(\realrv;\theta, \gammashape)&=&
\frac{1}{ \Gamma(\gammashape)\,\theta} \,
          \left (
          \frac{t}{\theta}
          \right )^{\gammashape-1}
          \exp \left ( -
          \frac{t}{\theta}
               \right ), \quad \realrv > 0
\end{eqnarray}
where $\theta > 0$ is a scale
parameter and $\gammashape > 0$ is a shape parameter. Here
$\incgamma(v;\gammashape)$ is the incomplete gamma function defined by
\begin{equation}
\label{equation:incomplete.gamma.def}
\incgamma(v;\gammashape)=\frac{\int^{v}_{0} x^{\gammashape-1} \exp(-x)\,dx}
                                   {\Gamma(\gammashape)},
\quad v > 0. 
\end{equation}
The gamma pdf, cdf, and hazard functions
are graphed in Figure~\ref{figure:distplot.gamma.ps}.
The gamma distribution can be useful for modeling certain life
distributions.  Letting $\gammashape=1$, gives the
exponential distribution as a special case.
As shown in Figure~\ref{figure:distplot.gamma.ps}, the
gamma distribution hazard function can be either decreasing (when
$\gammashape<1$) or
increasing (when $\gammashape>1$), in either case
approaching a constant level late in life (i.e.,
$\lim_{\realrv \to \infty} h(\realrv;\theta,
\gammashape)=1/\theta$). Also, the sum of
$\kappa$ independent exponential random variables with mean $\theta$
has a gamma distribution with parameters $\theta$ and
$\gammashape$. This property can be used to motivate the use of the
gamma distribution in some applications.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.gamma.ps}
\caption{Gamma
cdf, pdf, and hf for $\theta=1$ and $\kappa=$ .8, 1, and 2.}
\label{figure:distplot.gamma.ps}
\end{figure}
%-------------------------------------------------------------------

%----------------------------------------------------------------------
\subsection{Gamma moments and quantiles}
\label{section:gamma.moment.quant}
For any $m \geq 0,$ $
\E(\rv^{m})=  \theta^{m} \, \Gamma\left(
			m + \gammashape
\right) / 		 \Gamma(\gammashape).  $
From this it follows that $\E(\rv)=\theta \gammashape$, and
	$\var(\rv)=\theta^{2} \gammashape $.
The gamma $p$ quantile
is $\rvquan_{p}=\theta \,  \incgamma^{-1}(p;\gammashape)$
where $\incgamma^{-1}(p;\gammashape)$ is the
inverse of the incomplete gamma function defined in
(\ref{equation:incomplete.gamma.def}).	That is, $\incgamma
\left [\incgamma^{-1}(p;\gammashape);\gammashape \right ]=
p$.



%----------------------------------------------------------------------
\subsection{Gamma standardized parameterization}
\label{section:gamma.std.par}
The gamma cdf and pdf  also can be written as follows:
\begin{eqnarray*}  
 F(\realrv;\theta, \gammashape)&=&
\Phi_{\rm lg}\left [\log(\realrv)-\mu; \gammashape \,
\right ]\\
 f(\realrv;\theta, \gammashape)&=&\frac{1}{ \realrv} \, \phi_{\rm lg}
\left [ 
{\log(\realrv)-\mu}; \gammashape \right],  \quad \realrv > 0
\end{eqnarray*} 
where $\mu=\log(\theta)$ and 
\begin{eqnarray*}
\Phi_{\rm lg}(z;\gammashape)&=&\incgamma[\exp( z);\gammashape]
\\
\phi_{\rm lg}(z;\gammashape)&=&
\frac{1}{\Gamma \left(\gammashape \right )} 
\exp \left [
{\gammashape z-\exp( z)}
     \right ]
\end{eqnarray*}
are, respectively, the cdf and pdf for the
standardized loggamma variable 
$Z=\log(\rv/\theta)=\log(\rv)-\mu$.
Unlike the standardized distributions used in
Chapter~\ref{chapter:ls.parametric.models},
these standardized distributions depend on the shape
parameter $\gammashape$.

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Generalized Gamma Distribution}
\label{section:gng.distribution}
\subsection{Generalized gamma cdf and pdf}
The generalized gamma distribution 
contains the exponential, gamma, Weibull, and lognormal distributions
as special cases. When $\rv$ has a generalized gamma distribution
we indicate this by $\rv \sim \GENG(\theta,\beta,\gammashape)$. As will
be demonstrated in Chapter~\ref{chapter:ml.other.parametric}, 
the $\GENG$ distribution is useful for
helping to choose among  these special-case distributions. 
The cdf and pdf for the generalized gamma distribution are
\begin{eqnarray}  
\label{equation:gng.cdf}  
 F(\realrv;\theta,\beta,\gammashape)&=&\incgamma
	\left [
	\left (
          \frac{\realrv}{\theta}
	\right)^{\beta};  \gammashape
	\right ] \\
 f(\realrv;\theta,\beta,\gammashape)&=&
 \frac{\beta}{ \Gamma(\gammashape)\,\theta} \,
          \left (
          \frac{t}{\theta}
          \right )^{\gammashape\beta-1}
          \exp \left [ -
	   \left (
          \frac{t}{\theta}
          \right )^{\beta}    
               \right ] \nonumber , \quad \realrv > 0
\end{eqnarray} 
where $\theta>0$ is a scale
parameter, $\beta>0$ and $\gammashape>0$ are shape parameters,
and $\incgamma(v,\gammashape)$ is the incomplete gamma 
function given in (\ref{equation:incomplete.gamma.def}).

%----------------------------------------------------------------------
\subsection{Generalized gamma moments and quantiles}
\label{section:gng.moment.quant}
For $m \geq 0,$ $\E(\rv^{m})=   {\theta^{m} \, \Gamma\left(
m/\beta + \gammashape 			 \right) 	 }
/ 		 { 		 \Gamma(\gammashape) 		 }.  $
From this,
\begin{eqnarray*}
	\E(\rv)&=& 
	     \frac{\theta \, \Gamma\left(
			1/\beta + \gammashape 
			 \right)
	          }
		  {
		   \Gamma(\gammashape)
		  }
\\
	\var(\rv)&=& \theta^{2} 		 \left [
\frac{\Gamma\left( 			2/\beta + \gammashape
\right) 	 } 		 { 		 \Gamma(\gammashape)
} -
\frac{\Gamma^{2}\left(
			1/\beta + \gammashape 
			 \right)
	          }
		  {
		   \Gamma^{2}(\gammashape)
		  }
		   \right ] .
\end{eqnarray*}
The $p$ quantile is
$\rvquan_{p}=\theta \left[ \incgamma^{-1}(p;\gammashape) \right ]^{1/\beta}$.

%-------------------------------------------------------------------
\subsection{Special cases of the generalized gamma distribution}
\label{section:gng.special.cases}
In this section we show the relationship between the 
$\GENG(\theta,\beta,\gammashape)$
and the well-known distributions that are special cases.
\begin{itemize} 	
\item 	
When $\beta=1$,
$\rv \sim \GAM(\theta,\gammashape)$.  	
\item 
When $\gammashape=1$,
$\rv \sim \WEIB(\theta,\beta)$.
\item
When $(\beta,\gammashape)=(1,1)$,
$\rv \sim \EXP(\theta)$.  	
\item 
As $\gammashape \rightarrow \infty$, $\rv \approxdist
\LOGNOR[\log(\theta)+ \log(\gammashape)/\beta,1/(\beta \sqrt{\gammashape})]$.
  \end{itemize}


%-------------------------------------------------------------------
\subsection{Generalized gamma reparameterization for 
numerical stability}
\label{section:gng.reparameterization}
The parameterization in terms of $(\theta, \beta, \gammashape)$
is generally numerically unstable for fitting the distribution to data. 
Farewell and Prentice~(1977) recommend the alternative
parameterization
\begin{displaymath}
\mu=\log(\theta)+\frac{1}{\beta}
\log(\egengshape^{-2}),
\quad
\sigma=\frac{1}{\beta \sqrt{\kappa}},
\quad
\egengshape=\frac{1}{\sqrt{\kappa}}.
\end{displaymath}
This parameterization is numerically stable if
there is little or no censoring.
Using the cdf in (\ref{equation:gng.cdf})
and defining $\omega=\left [\log(\realrv)-\mu \right ]/\sigma$ gives
\begin{eqnarray*}  
 F(\realrv;\theta,\beta,\gammashape)&=&
\incgamma \left [
	     \egengshape^{-2}  \exp
			\left (
			 \egengshape w
			\right ); \egengshape^{-2}
	    \right ]
=
\Phi_{\rm lg} \left [\egengshape \omega+
\log(\egengshape^{-2});\egengshape^{-2}
\right ]
\\ 
 f(\realrv;\theta,\beta,\gammashape)&=&
\frac{\egengshape}{\sigma \realrv} \, \phi_{\rm lg}
\left [
\egengshape \omega+
\log(\egengshape^{-2});\egengshape^{-2}
\right ], \quad \realrv > 0
\end{eqnarray*} 
where $-\infty<\mu<\infty$, $\sigma>0$ and $\egengshape>0$.  In this
parameterization the quantile function is
\begin{displaymath}
\rvquan_{p}=\exp \left \{\mu + \frac{\sigma}{\egengshape} \log
\left [ \egengshape^{2}\incgamma^{-1} \left (
p; \frac{1}{\egengshape^{2}} 				 \right )
\right ] \right \}.
\end{displaymath}


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Extended Generalized Gamma Distribution}
\label{section:egeng.parameterization}
Using the alternative stable parameterization and allowing
$\egengshape$ to become negative generalizes the $\GENG$ to what we
will call the extended generalized gamma distribution, enlarging the
family to include other distributions as special cases. Then $\rv$ has
an $\EGENG(\mu,\sigma,\egengshape)$ distribution with pdf and cdf
given by
\begin{eqnarray}
\label{equation:egeng.cdf}
 F(\realrv;\mu,\sigma,\egengshape)&=&
\left \{
\begin{array}{ll}
\Phi_{\rm lg} \left [\egengshape \omega+
\log(\egengshape^{-2});\egengshape^{-2}
\right ]
& \mbox{if $\egengshape>0$}
\\[1ex]
\Phi_{\nor}(\omega)
& \mbox{if $\egengshape=0$}
\\[1ex]
1-\Phi_{\rm lg} \left [\egengshape \omega+
\log(\egengshape^{-2});\egengshape^{-2}
\right ]
& \mbox{if $\egengshape<0$}
\end{array}
\right. 
\\[1ex]
\label{equation:egeng.pdf}
 f(\realrv;\mu,\sigma,\egengshape)&=&
\left \{
\begin{array}{ll}
\frac{|\egengshape|}{\sigma \realrv} \, \phi_{\rm lg}
\left [
\egengshape \omega+
\log(\egengshape^{-2});\egengshape^{-2}
\right ] & \mbox{if $\egengshape \ne 0$}
\\[2ex]
\frac{1}
     {\sigma \realrv}
     \phi_{\nor}(\omega)
& \mbox{if $\egengshape=0$}
\end{array}
\right.
\end{eqnarray}
where $t>0$, $\omega=\left [\log(\realrv)-\mu \right ]/\sigma$,
$-\infty<\mu<\infty$, $-\infty<\egengshape<\infty$ and $\sigma>0$.
Note that if $\rv \sim \EGENG(\mu,\sigma,\egengshape)$ and $c >0$
then $c \rv \sim \EGENG[\mu + \log(c),\sigma,\egengshape]$.  Thus,
$\exp(\mu)$ is a scale parameter and $\sigma$ and $\egengshape$ are
shape parameters. For any given fixed value of $\egengshape$, the EGENG
distribution is a log-location-scale distribution.

%----------------------------------------------------------------------
\subsection{Extended generalized gamma moments and quantiles}
The moments for the $\EGENG$ can be
obtained using
\begin{displaymath}
\displaystyle
\E(\rv^{m})=
\left \{
\begin{array}{lll} \frac{
	          {\exp(m \mu) \, \left ( 
			\egengshape^{2}
			\right )^{m \sigma/\egengshape}
                    \, \Gamma
		         \left [\egengshape^{-1}(
			m \sigma + \egengshape^{-1})
			 \right]
	          }
		       }
		  {
		   \Gamma(\egengshape^{-2})
		  }
&\mbox{if} & m\egengshape \sigma+ 1>0, \egengshape \ne 0
\\
+\infty     & \mbox{if}&  m\egengshape \sigma + 1 \le 0,\egengshape \ne 0 .
\end{array}
\right.
\end{displaymath} 
When $\egengshape = 0$, $\E(\rv^{m})=\exp \left [m \mu+ (1/2)(m
\sigma)^{2} \right ].$ Using these, it is easy to compute the mean,
variance and other central moments for the $\EGENG$ distribution.

Inverting (\ref{equation:egeng.cdf}) gives the $\EGENG$ $p$ quantile
\begin{displaymath}
\rvquan_{p}=\exp \left [
\mu+\sigma \times \omega(p;\egengshape)
                 \right ]
\end{displaymath}
where $\omega(p;\egengshape)$ is the $p$ quantile
of $[\log(\rv)-\mu]/\sigma$ given by
\begin{displaymath}
\omega(p;\egengshape)=
\left \{
\begin{array}{ll}
(1/{\egengshape})\log \left[
 {\egengshape}^{2} \incgamma^{-1}(p; \egengshape^{-2})
     \right ]
& \mbox{if $\egengshape>0$}
\\[1ex]
\Phi_{\nor}^{-1}(p)
& \mbox{if $\egengshape=0$}
\\[1ex]
(1/{\egengshape})\log \left[
 {\egengshape}^{2} \incgamma^{-1}(1-p; \egengshape^{-2})
     \right ]
& \mbox{if $\egengshape<0$}.
\end{array}
\right.
\end{displaymath}

%-------------------------------------------------------------------
\subsection{Special cases of the extended generalized gamma distribution}
The $\EGENG(\mu,\sigma,\egengshape)$ distribution has the following
important special cases: 
\begin{itemize}
\item
If $\egengshape>0$ then $\EGENG(\mu,\sigma,\egengshape)=\GENG(\mu,\sigma,\egengshape)$.
\item
If $\egengshape=1$,
$\rv \sim \WEIB(\mu,\sigma)$. 
\item
If $\egengshape=0$,
$\rv \sim \LOGNOR(\mu,\sigma)$. 
\item
If $\egengshape=-1$,
$1/\rv \sim \WEIB(-\mu,\sigma)$, 
(i.e., $\rv$ has a reciprocal Weibull, also known as the 
Fr\'echet distribution of {\em maxima}).
\item 
When $\egengshape=\sigma$, 
$\rv \sim \GAM(\theta,\kappa)$, where 
$\theta=\egengshape^{2} \exp(\mu)$
and $\kappa=\egengshape^{-2}$.
\item
When $\egengshape=\sigma=1$,
$\rv \sim \EXP(\theta)$, where
 $\theta= \exp(\mu)$.	
\end{itemize}

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Generalized F Distribution}
\label{section:gen.f}
%-------------------------------------------------------------------
\subsection{Background}
The generalized F distribution is a 4-parameter distribution
that includes the $\GENG$ family and the loglogistic, among other
distributions, as special cases. The distribution is useful for
choosing among the special case distributions.
\subsection{Generalized F cdf and pdf functions}
When $\rv$ has a  generalized F distribution
we write $\rv \sim \GENF(\mu,\sigma,\kappa,r)$. The $\GENF$ cdf and pdf are
\begin{eqnarray*}  
 F(\realrv;\mu, \sigma, \kappa,r)
&=&\Phi_{\rm lf}\left [\frac{\log(\realrv)-\mu}{\sigma}; \kappa,r
\right ]\\
 f(\realrv;\mu, \sigma, \kappa,r)&=&\frac{1}{ \sigma \realrv} \, 
\phi_{\rm lf}
\left [ 
\frac{\log(\realrv)-\mu}{\sigma}; \kappa,r \right], \quad t>0
\end{eqnarray*} 
where 
\begin{displaymath}
\phi_{\rm lf}(z;\kappa,r)=
\frac{\Gamma(\kappa+r)}{\Gamma(\kappa)\,\Gamma(r)}
\,\,
\frac{
 (\kappa/r)^{\kappa}
\,
\exp 
\left (
\kappa z
\right )
     } {
\left [
1+ (\kappa/r) \exp(z)
\right ]^{\kappa+r}
     }
\end{displaymath} 
is the pdf of the central log F distribution with $2\kappa$ numerator
and $2r$ denominator
degrees of freedom and $\Phi_{\rm lf}$ is the corresponding cdf
(for which there is, in general, no closed-form expression).  Also,
$\phi_{\rm lf}(z;\kappa,r)$ and $\Phi_{\rm lf}(z;\kappa,r)$ are the
pdf and cdf of $Z=[\log(\rv)-\mu]/\sigma$.  Note that $\exp(\mu)$ is a
scale parameter while $\sigma > 0$, $\kappa>0$, and $r>0$ are shape
parameters. When $\kappa=r$, $\phi_{\rm lf}(z;\kappa,r)$ is symmetric
about $z=0$.

%-------------------------------------------------------------------
\subsection{Generalized F moments and quantiles}

For $m \ge 0$,
\begin{eqnarray*}
\E(\rv^{m})&=&
\left \{
\begin{array}{ll}
\frac{
\exp(m \mu) 
\,
 \Gamma( \kappa + m \sigma) \, \Gamma( r-m \sigma)
     }
     {
 \Gamma( \kappa ) \, \Gamma( r)
     }
\,
 \left (
\frac{
     r
     }
     {
     \kappa
     }
\right )^{m \sigma},
& \,\, \mbox{if} \,\, r> m\sigma
\\
\infty & \quad \mbox{otherwise}.
\end{array}
\right.
\end{eqnarray*}
$\E(\rv)$ and $\var(\rv)$ can be computed directly from this expression.
The $p$ quantile of the generalized F is $\rvquan_{p}= \exp(\mu) \left
[{\cal F}_{(p; 2\kappa, 2r)}\right]^{\sigma}$ where 
${\cal F}_{(p; 2\kappa, 2r)}$ is
the $p$ quantile of an $\mbox{F}$ distribution with $(2\kappa,2r)$
degrees of freedom.  The expression for the quantile follows directly
from the fact that if $\rv \sim \GENF(\mu, \sigma, \kappa,r)$, then
$\rv= \exp(\mu) V^{\sigma}$ or equivalently $\log(\rv)=\mu+ \sigma
\log(V)$ where $V$ has an F distribution with $(2\kappa, 2r)$ degrees
of freedom.
Finally, observe that 
for fixed $(\kappa,r)$, the variable
$\rv$ has a log-location-scale distribution where 
the ``standardized'' distribution is the log of an F random variable
with $(2\kappa, 2r)$ degrees of freedom.

%-------------------------------------------------------------------
\subsection{Special cases of the generalized F distribution}
The $\GENF(\mu, \sigma, \kappa,r)$ has a number of important special cases:
\begin{itemize}
\item 
$1 / \rv \sim
\GENF(-\mu, \sigma, r, \kappa)$ (i.e., the reciprocal of $T$ is also $\GENF$).
\item 
When $(\mu,\sigma)=(0,1)$ then $\rv$ has an F distribution
(sometimes known as ``Snedecor's F distribution'') with $2\kappa$
numerator and $2r$ denominator degrees of freedom.
\item 
When $(\kappa,r)=(1,1)$, $\GENF(\mu, \sigma, \kappa,r) =
\LOGLOGIS(\mu, \sigma)$. 
\item 
As $r \rightarrow
\infty$, in the limit, $\rv \sim
          \GENG[\exp(\mu)/\kappa^{\sigma}, 1/\sigma, \kappa]$.
\item
For $\kappa=1$ and as $r \rightarrow
\infty$, in the limit, $\rv \sim \WEIB(\exp(\mu) , 1/\sigma)$.
\item
When $\kappa=1$, $\rv$ has a Burr type XII distribution with cdf
\begin{displaymath}
F(\realrv; \mu, \sigma, r) = 1 -\frac{1}{ \left [1 + \frac{1}{r} \left[
 \frac{t}{ \exp(\mu)} \right]^{\frac{1}{ \sigma }} \right ]^{r}}, \quad t>0
\end{displaymath}
where $r > 0$ and $\sigma > 0$ are shape parameters and $\exp(\mu)$ is 
a scale parameter.
\item 
As $\kappa \to \infty$ and $r \to
\infty$, $\rv \approxdist \LOGNOR \left ( \mu, \sigma \sqrt{( \kappa
+r)/ (\kappa r}) \right )$.

\end{itemize}


%----------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Inverse Gaussian Distribution}

%-------------------------------------------------------------------
\subsection{Background}
\label{section:igau.dist}
A common parameterization for the cdf of the inverse Gaussian (IGAU)
distribution is (e.g., Chhikara and Folks 1989) is
\begin{eqnarray}  
\label{equation:igau.traditional.cdf}
\Pr(\rv \le \realrv;\theta,\lambda) &=&
\Phi_{\nor}\left [\frac{(\realrv-\theta) \sqrt{\lambda}}
                       {\theta\, \sqrt{\realrv}}
\right ]+ 
\exp \left (
	\frac{2 \lambda}{ \theta}
     \right )
\Phi_{\nor}\left [-\, \frac{(\realrv+\theta) \sqrt{\lambda}}
                           {\theta\, \sqrt{\realrv}}
\right ], \quad t>0
 \end{eqnarray} 
where $\theta>0$ and $\lambda>0$ are parameters having the
same units as $\rv$. 

The Inverse Gaussian distribution was originally given by
Schr\"{o}dinger~(1915) as the distribution of the first passage time
in Brownian motion. The parameters $\theta$ and $\lambda$ relate to
the Brownian motion parameters as follows.  Consider a Brownian
process $B(t)=c t +d W(t), t>0$ where $c$ and $d$ are constants and $W(t)$
is a Wiener process. Let $\rv$ be the first passage time of a
specified level $b_{0}$ (i.e., the first time that $ B(t) \ge b_{0}
$).
%say
%\begin{displaymath}
%	T=\inf \left \{t; B(t) \ge b_{0} \right \}
%\end{displaymath}
This leads directly to 
(\ref{equation:igau.traditional.cdf})
where $\theta=b_{0}/c$ and $\sqrt{\lambda}=b_{0}/d$.
Tweedie~(1945) gives more details on this approach to deriving the
$\IGAU$ distribution. Wald~(1947)
derived the inverse Gaussian as a limiting form for the
distribution of the sample size for a sequential probability ratio
test.

%-------------------------------------------------------------------
\subsection{Inverse Gaussian cdf and pdf}
\label{section:igau.pdfcdf}
For life data analysis (and other modeling applications)
it is often more convenient to reparameterize the distribution
so that it has a scale parameter
and a shape parameter (instead of having two shape parameters that
depend on the units in which time is measured).
If $\rv$ has an inverse Gaussian distribution,
we denote this by $\rv \sim \IGAU(\theta, \beta)$. The IGAU cdf and
pdf are
\begin{eqnarray}  
\label{equation:igau.cdf}
F(\realrv;\theta, \beta) &=&
\Phi_{\rm ligau}\left [ \log(\realrv/\theta); \beta 
	     \right ]
\\
 f(\realrv;\theta, \beta)&=&
	\frac{1}{\realrv} \, 
\phi_{\rm ligau}\left [ \log(\realrv/\theta); \beta 
	     \right ], \quad t>0 \nonumber \end{eqnarray} where
 $\theta>0$ is a scale parameter and $\beta=\lambda/\theta>0$ is a
 unitless shape parameter.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.igau.ps}
\caption{Inverse Gaussian cdf, pdf, 
and hf for $\beta=$ 1, 2, 4 and $\theta=1$.}
\label{figure:distplot.igau.ps}
\end{figure}
%-------------------------------------------------------------------
Here
\begin{eqnarray*}  
\Phi_{\rm ligau}(z;\beta) &=&
\Phi_{\nor}\left \{ \sqrt{\beta}  \left [
\frac{ \exp(z) -1 }  {\exp(z/2)  } \right ] \right \} +
\exp \left (2 \beta \right )\Phi_{\nor}\left \{-\, \sqrt{\beta}  \left [
\frac{ \exp(z) +1 }  {\exp(z/2) } \right ]\right \}
\\
 \phi_{\rm ligau}(z;\beta)&=&
	   \frac{\sqrt{\beta }}
		{\exp(z/2)}
       \,\,
\phi_{\nor}\left \{
		 \sqrt{\beta} 		 \left [
\frac{ 		 \exp(z) -1 		 } 		 {
\exp(z/2) 		 } 		 \right ]
\right \},
\,\,\,\,\, -\infty< z < \infty
\end{eqnarray*}
are the cdf and pdf, respectively, of  $\log(\rv/\theta)$,
the log standardized inverse Gaussian distribution.
Figure~\ref{figure:distplot.igau.ps} shows the IGAU cdf, pdf, and
hazard functions.
%-------------------------------------------------------------------
\subsection{Inverse Gaussian moments and quantiles}
For integer $m>0$,
\begin{displaymath}
	\E(\rv^{m})=
\theta^{m} \,
	\sum_{i=0}^{m-1} \frac{(m-1+i)!}
			       {i!\,(m-1-i)!}
	\, \left (
	   \frac{1}
		{2 \beta}
	   \right )^{i}.
\end{displaymath}
From this  $\E(\rv)= \theta$ and $\var(\rv)=
\theta^{2}/\beta$.  
The $p$ quantile is $\rvquan_{p}=\theta \, \exp[\Phi^{-1}_{\rm
ligau}(p;\beta)]$.  There is no simple closed form equation for
$\Phi^{-1}_{\rm ligau}(p;\beta)$, so it must be computed by inverting
$p=\Phi_{\rm ligau}(z;\beta)$ numerically.

%-------------------------------------------------------------------
\subsection{Inverse Gaussian distribution properties}
The inverse Gaussian distribution has the following properties.
 \begin{itemize}
\item 
If $\rv \sim \IGAU(\theta, \beta)$ and $c>0$ then $c \rv \sim
\IGAU(c\theta, \beta)$.
\item 
The $\IGAU$ hazard function $h(\realrv;\theta,\beta)$ is unimodal,
$h(0;\theta,\beta)=0$, and $\lim_{\realrv \to
\infty}h(t;\theta,\beta)=\beta/(2 \theta)$.
\item 
For large values of $\beta$, the $\IGAU$ distribution is very similar
to a $\NOR( \theta, \theta/\sqrt{\beta})$.
 \end{itemize}

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Birnbaum-Saunders Distribution}
\label{section:bisa.dist}
\subsection{Background}
The Birnbaum-Saunders distribution was derived by Birnbaum and
Saunders~(1969) based on a model for the number of cycles necessary
to force a fatigue crack to grow to a critical size that would cause
fracture.

%----------------------------------------------------------------------
\subsection{Birnbaum-Saunders cdf and pdf}
\label{section:bisa.pdfcdf}
If $\rv$ has a Birnbaum-Saunders distribution, we denote this by
$\rv \sim \BISA(\theta, \beta)$. The $\BISA$ cdf and pdf are
\begin{eqnarray*}  
 F(\realrv;\theta, \beta)&=&
\Phi_{\rm nor}
\left (z
\right)
\\
 f(\realrv;\theta, \beta)&=&
\frac{
\sqrt{
\frac{\realrv}{\theta}
     }
+
\sqrt{
\frac{\theta}{\realrv}
     }}
     {2 \beta \realrv}
\,
\phi_{\rm nor}
\left (z
\right), \quad t > 0
 \end{eqnarray*} 
where $\theta >0$ is a scale parameter,
$\beta >0$ is a shape parameter, and 
\begin{displaymath}
z=\frac{1}{\beta} \,
\left (
\sqrt{
\frac{\realrv}{\theta}
     }
-
\sqrt{
\frac{\theta}{\realrv}
     }
\right ).
\end{displaymath}
Figure~\ref{figure:distplot.bisa.ps} shows the $\BISA$ cdf, pdf, and
and hazard function.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.bisa.ps}
\caption{Birnbaum-Saunders
cdf, pdf, and hf for shape parameter $\beta=$ .5,.6,1
and $\theta=1$.}
\label{figure:distplot.bisa.ps}
\end{figure}
%-------------------------------------------------------------------
%----------------------------------------------------------------------
\subsection{Compact parameterization for Birnbaum-Saunders}

Sometimes is useful to write the $\BISA$ cdf and pdf as follows
\begin{eqnarray*}  
 F(\realrv;\theta, \beta)&=&
 \Phi_{\rm lbisa} \left [\log(\realrv/\theta);\beta \right ]
\\
 f(\realrv;\theta, \beta)&=&
\frac{1}{\realrv}
 \,\,
\phi_{\rm lbisa} \left [\log(\realrv/\theta);\beta \right ], \quad t > 0
\end{eqnarray*}
where 
\begin{eqnarray*}
\Phi_{\rm lbisa} \left (z;\beta \right )
&=&
\Phi_{\rm nor}
\left ( \nu
\right)
\\
 \phi_{\rm lbisa}(z; \beta)&=&
 \left [
\frac{
\exp(z/2)
+
\exp(-z/2)
      }
     {2 \beta }
 \right ]
\,
\phi_{\rm nor}
\left (\nu
\right)
\\
\mbox{and} \quad \nu&=&\frac{1}{\beta} \,
\left [
  \exp(z/2)
-
   \exp(-z/2)
\right ],\,\,-\infty< z< \infty.
 \end{eqnarray*} 

%-------------------------------------------------------------------

\subsection{Birnbaum-Saunders moments and quantiles}
The $m$th moment of the Birnbaum-Saunders distribution is
\begin{displaymath}
\E(\rv^{m})=
\theta^{m}
\sum_{i=0}^{m}
\beta^{2(m-i)}
\frac{[2(m-i)]!}{\left[ 2^{3(m-i)}\right ]\,(m-i)!} \,
\sum_{k=0}^{m-i}
\left (
\begin{array}{c}
2 m
\\
2 k
\end{array}
\right )
\left (
\begin{array}{c}
m-k
\\
i
\end{array}
\right ).
\end{displaymath}
It follows that
\begin{displaymath}
\E(\rv) =  \theta 
	\left (
	1+\frac{\beta^{2}}{2}
	\right )
\quad \mbox{and} \quad 
\var(\rv)= (\theta \beta)^{2}
	\left (
	1+\frac{5 \beta^{2}}{4}
	\right ). 
\end{displaymath}
The  $p$ quantile can be expressed as
\begin{equation}
\label{equation:bisa.quantile}
\rvquan_{p}=
\frac{\theta}{4} 
	\left \{
	 \beta \, \Phi_{\nor}^{-1}(p) + \sqrt{4+ \left [\beta \, 
\Phi_{\nor}^{-1}(p) \right ]^{2}}
	\right \}^{2}.
\end{equation}


%----------------------------------------------------------------------
\subsection{Properties of the Birnbaum-Saunders distribution}
\label{section:bias.properties}
The Birnbaum-Saunders distribution has the following properties

\begin{itemize} 
\item 
If $\rv \sim \BISA(\theta, \beta)$ and $c>0$ then $c \rv \sim \BISA(c
\theta, \beta)$.  \item If $\rv \sim \BISA(\theta, \beta)$ then $1/\rv
\sim
\BISA(\theta^{-1},\beta)$.  
\item 
The hazard function $\BISA$
$h(\realrv;\theta, \beta)$ is not always increasing. Also, it is easy
to show that
$h(0;\theta, \beta)=0$ and $\lim_{\realrv \to \infty}
h(\realrv;\theta, \beta)=1/(2 \theta
\beta^{2})$. Through numerical experiments with
a wide range of parameters it appears that $h(\realrv;\theta, \beta)$
is always unimodal.
\end{itemize}

There is a close relationship between the BISA and the IGAU
distributions. Simply stated, the BISA distribution was from
a discrete-time degradation process. The IGAU is based
on an underlying continuous-time stochastic process. Nevertheless,
comparison of Figures~\ref{figure:distplot.igau.ps} and 
\ref{figure:distplot.bisa.ps} shows that
the shapes of the two distributions are very similar.  Desmond~(1986)
describes the relationship in more detail.  The BISA and the IGAU
distributions are similar to the lognormal distribution
(Section~\ref{section:lognormal.distribution.def}) in shape and
behavior. This can be seen by comparing Figures~\ref{figure:distplot.lnor.ps},
\ref{figure:distplot.igau.ps}, and \ref{figure:distplot.bisa.ps}.
A direct comparison fitting the models to data is illustrated in
Figure~\ref{figure:bkfat10.cf.ln.bisa.igau.gmleprobplot.ps}.

%-------------------------------------------------------------------
\section{Gompertz-Makeham Distribution}
\label{section:gompertz.makeham}
%-------------------------------------------------------------------
The Gompertz-Makeham (or $\GOMA$) distribution has an 
increasing hazard function and is used to model human life in middle
age and beyond.  A common
parameterization for this distribution is 
\begin{eqnarray*}
\Pr(\rv \le \realrv;\gamma, \kappa, \lambda)&=& 
1-\exp \left [ -\, \frac{ \lambda \kappa \realrv +
\gamma \exp( \kappa \realrv) -\gamma } {\kappa
} \right ],
\quad \realrv > 0
\end{eqnarray*} 
where $\gamma >0, \kappa > 0, \lambda \ge 0$,
and all the parameters have units that are the reciprocal of the units of
$\realrv$.  

An alternative representation for the cdf is 
\begin{equation}
\label{equation:goma.sev.connection}
\Pr(\rv \le \realrv;\gamma, \kappa, \lambda) = 
1- 
 \left [
 \frac{
   1-\Phi_{\rm sev}
  \left  (
   \frac{\realrv-\mu}{\sigma}
  \right )
      }
      {
   1-\Phi_{\rm sev}
  \left  (
   \frac{-\mu}{\sigma}
  \right )
      }
\right ]\, \exp ( -\lambda \realrv), \quad \realrv > 0
\end{equation}
where $\mu=-(1/\kappa) \log(\gamma/\kappa)$, and $\sigma=1/\kappa$.  When
$\lambda=0$, (\ref{equation:goma.sev.connection}) reduces to the
Gompertz--distribution, corresponding to an $\SEV$ distribution
truncated at $t=0$ (i.e., an SEV random variable, conditional on
being positive). The $\GOMA$ distribution satisfies a requirement for
a positive random variable and has a hazard function similar to
that of the $\SEV$. In fact, as indicated in the following section,
except for an additive constant, the hazard function of the
Gompertz-Makeham distribution agrees with the hazard function of an
SEV truncated at the origin. See Exercise~\ref{exercise:goma.haz}
for another interpretation.

\subsection{Gompertz-Makeham scale/shape parameterization and
cdf, pdf, and hazard functions} 

In order to separate out the scale parameter from the shape parameters,
we parameterize in terms of $[\theta,\zeta,
\eta]=[1/\kappa,\log(\kappa/\gamma),\lambda/\kappa]$ and say that
$\rv \sim \GOMA(\theta,\zeta, \eta)$ if
\begin{eqnarray*}  
 F(\realrv;\theta,\zeta, \eta)&=&
\Phi_{\rm lgoma} \left [\log \left ( \frac{\realrv}{\theta} \right ) ; \zeta, \eta \right ]
\\[1ex]
 f(\realrv;\theta,\zeta, \eta)&=&
		\frac{1}{\realrv}
        \,\,
\phi_{\rm lgoma}   
\left [\log  \left ( \frac{\realrv}{\theta} \right ); \zeta, \eta \right ]
\\[1ex]
 h(\realrv;\theta,\zeta, \eta)&=&
 \frac{\eta}{\theta}+
 \frac{\exp(-\zeta)}{\theta}
 \,
 \exp 
	\left (
	\frac{\realrv}{\theta}
	\right ) , \quad t > 0.
 \end{eqnarray*}
Here
$\theta$ is a scale parameter, $\zeta$ and $\eta$ are
unitless shape parameters (not depending on the time scale), and
\begin{eqnarray*}  
 \Phi_{\rm lgoma}(z; \zeta, \eta)&=& 1- \exp \left \{ \exp \left
(-\zeta \right ) - \exp \left [ \exp(z) 	-\zeta \right ] -\eta
\,
\exp(z)
\right \}
\\[1ex]
 \phi_{\rm lgoma}(z;\zeta, \eta)&=& \exp(z) \left \{ \eta + \exp \left
[\exp(z) 		-\zeta \right] \right \} \left [
1-\Phi_{\rm lgoma}(z;\zeta, \eta)\right]  
\end{eqnarray*}  
are, respectively, the standardized cdf and pdf of
$z=\log(\realrv/\theta)$.  These functions are graphed in
Figure~\ref{figure:distplot.goma.ps}.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.goma.ps}
\caption{Gompertz-Makeham cdf, pdf, and hf 
for $\theta=1$ and $\zeta=$ .2, 2. and $\eta=$ .5, 3.}
\label{figure:distplot.goma.ps}
\end{figure}
%-------------------------------------------------------------------
The $p$ quantile is $\rvquan_{p}=\theta \, \exp[\Phi^{-1}_{\rm
lgoma}(p;\zeta, \eta)]$.  There is no simple closed form equation for
$\Phi^{-1}_{\rm lgoma}(p;\zeta, \eta)$, so it must be computed by inverting
$p=\Phi_{\rm lgoma}(z;\zeta, \eta)$ numerically.


\subsection{Gompertz-Makeham distribution properties}
Some properties of the Gompertz-Makeham distribution are
\begin{itemize}
 \item $h(0;\theta,\zeta, \eta)=(1/\theta)[\eta+\exp(-\zeta)]$
and  $h(\realrv;\theta,\zeta, \eta)$
increases with $t$ at an exponential rate.

 \item If $\rv \sim \GOMA(\theta,\zeta, \eta)$
and $c>0$ then
$c \rv \sim \GOMA(c \theta,\zeta, \eta)$.

\end{itemize}


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Comparison of Spread and Skewness
Parameters} 
\label{section:comp.shape}
Figure~\ref{figure:momentplot.ps} is similar to the figure
on page 27 of Cox and Oakes~(1984).  It plots $\gamma_{3}$, the
standardized third central moment (a unitless indication of skewness,
defined in Section~\ref{section:functions.of.parameters}) against
$\gamma_{2}$, the coefficient of variation (the standardized second
central moment, a unitless indication of spread, also defined in
Section~\ref{section:functions.of.parameters}). The curves in this
graph indicate the wide range of shapes that the corresponding
distribution can take across values of its shape parameter. Any
distribution lying above the $\gamma_{3}=0$ line will tend to be
skewed to the right.  Distributions lying below this line tend to be
skewed to the left (e.g., the Weibull distribution with large
$\beta$). We can also see the similarities and differences among the
different distributions that we have described.  For example, the
lines for the Birnbaum-Saunders and the inverse Gaussian distributions
are not too far apart and the Weibull and gamma distributions cross at
the exponential distribution point.  Note that the Generalized F ($\GENF$)
distribution spans the shapes of the specific distributions (but not
all of the specific distributions are special cases of the $\GENF$).
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/momentplot.ps}
\caption{Standardized third moment versus coefficient of
variation for different values of the shape parameters for
failure-time distributions. The Burr type XII distribution with
$r=2$ is equivalent to the $\GENF(\mu,\sigma,1,2)$. The $\bullet$
marks the exponential distribution point.}
\label{figure:momentplot.ps}
\end{figure}
%-------------------------------------------------------------------


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Distributions With a Threshold Parameter} 
\subsection{Background}
\label{section:intro.to.threshold.distributions}
The exponential, lognormal, Weibull, gamma, and generalized gamma
distributions, as defined previously, all are defined on the
positive real line $(0,\infty)$. That is, the pdf $f(t) > 0$ for all
$t>0$ and $f(t)=0$ for $t < 0$.  Correspondingly, the cdf begins
increasing at $t=0$.  All of these and other similar distributions
can be generalized by the addition of a threshold parameter,
which we denote by $\threshold$, to shift the beginning of the
distribution away from 0. These distributions are particularly
useful for fitting skewed distributions that are shifted far to the
right of 0.
\subsection{The cdf and pdf for distributions with a threshold}
\label{section:threshold.cdf}
The cdf and pdf of a log-location-scale distribution with
a threshold can be expressed as 
\begin{eqnarray}
 F(\realrv;\mu, \sigma,\threshold)&=&
\Phi\left [\frac{\log(\realrv-\threshold)-\mu}{\sigma}
\right ]
\\
f(\realrv;\mu, \sigma,\threshold)&=&
\frac{1}
     {\sigma (\realrv-\mu)}
\phi\left [\frac{\log(\realrv-\threshold)-\mu}{\sigma}
\right ], \quad t > \threshold 
\end{eqnarray}
where $\Phi$ is a completely specified cdf
and $\phi$ is the pdf corresponding to $\Phi$.
For a particular log-location-scale distribution, substitute 
the appropriate $\Phi$ and $\phi$. For example,
to obtain the three-parameter lognormal
distribution, substitute $\Phi_{\nor}$ and
$\phi_{\nor}$.
Figure~\ref{figure:threshold.shift.ps} shows the
three-parameter lognormal pdfs for $\mu=0$ and
$\sigma=.5$ with $\threshold=$ 1, 2, and 3.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/threshold.shift.ps}
\caption{Three-parameter lognormal
pdfs with $\sigma=.5$, $\mu=0$,
and $\threshold=$ 1, 2, and 3.}
\label{figure:threshold.shift.ps}
\end{figure}
%-------------------------------------------------------------------
Similarly, for the three-parameter Weibull 
distribution, substitute $\Phi_{\sev}$ and
$\phi_{\sev}$ giving
\begin{displaymath}
 \Pr(\rv \le t)=F(\realrv;\alpha, \beta,\threshold)=\Phi_{\sev}\left [\frac{\log(\realrv-\gamma)-\mu}{\sigma}
\right ]=1-
\exp \left [-\left (\frac{\realrv-\threshold}{\alpha} \right )^{\beta}
\right ], \quad t > \threshold
\end{displaymath}
where $\sigma=1/\beta$ and $\mu=\log(\alpha)$.  Sometimes
$\threshold$ is called a ``guarantee parameter'' because with
$\threshold > 0$, failure is impossible before time $\gamma$. More
generally, however, there is no mathematical reason to restrict
$\gamma$ to be positive.


The properties of the distributions with nonzero $\gamma$ are
closely related to the properties of the distributions with
$\gamma=0$ given in the earlier section in this chapter. In general,
$\gamma$ is added to the expectation and quantiles of the
distribution with threshold equal to $0$ to obtain the corresponding
expectation and quantiles of the distribution with a given $\gamma$.
Because changing $\gamma$ simply shifts the distribution on the time
axis, there is no effect on the distribution's spread or shape. Thus
$\var(\rv)$ does not change with changes in $\gamma$ and it can be
obtained directly from the distribution with $\gamma=0$.
Section~\ref{section:threshold.dist.ml} will describe methods of
fitting threshold distributions to data.

\subsection{Embedded distributions}
\label{section:embedded.models}
For some values of the parameters $(\mu, \sigma,\threshold)$, the
threshold distribution is very similar to a two-parameter
location-scale distribution, as described below. To facilitate the
description of these embedded distributions, we use the
reparameterization
\begin{displaymath}
\alpha =\gamma+\eta, \quad \varsigma=\sigma \eta
\end{displaymath}
where $\eta=\exp(\mu)$. Then the cdf for the threshold 
log-location-scale distribution becomes
\begin{displaymath}
F(\realrv;\alpha, \sigma,\varsigma)=
\Phi
\left [
\log \left ( 1+ \sigma z
           \right )^{1/\sigma}
\right ], \quad \mbox{for $z> -1/\sigma$}	
\end{displaymath}
where $z=(\realrv-\alpha)/\varsigma$. As $\sigma \to 0$ from above,
then
$
\left ( 1+ \sigma z
           \right )^{1/\sigma}
\to 
\exp(z),
$
and the {\em limiting} distribution
or embedded distribution is
\begin{displaymath}
F(\realrv;\alpha, 0,\varsigma)=
\Phi
\left (
 z \right ), \quad \mbox{for $-\infty<\realrv<\infty$}.
\end{displaymath} For example, when $\Phi=\Phi_{\sev}$ the embedded
distribution is the SEV distribution and when $\Phi=\Phi_{\nor}$ the
embedded distribution is the normal distribution. It is important to
observe that the embedded distributions are not members of the
original family (i.e., the embedded SEV distribution is not a Weibull
distribution and the embedded normal distribution is not a lognormal
distribution, except in the limit as $\sigma \to 0$ from above).  In
terms of the original parameters, embedded distributions arise when
$\sigma^{-1}$, $\exp(\mu)$, and $-\threshold$ are all approaching
$+\infty$ at rates such that $\sigma \times \exp(\mu)$ and
$\threshold+\exp(\mu)$ approach finite values.


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Generalized Threshold-Scale Distribution}
\label{section:gets.dist}
\subsection{Background} 
After reparameterization of a threshold-scale distribution to
$(\alpha,
\sigma,\varsigma)$, the parameter space can be enlarged to include the
embedded distributions.  This is accomplished by including the
limiting case $\sigma=0$.  It also useful to enlarge the parameter
space such that the limiting distributions are interior points of the
parameter space.  This is achieved by allowing $\sigma$ to take any
value in $(-\infty, \infty)$.  We call such a distribution a
generalized threshold-scale (or $\GETS$) distribution.

\subsection{Generalized threshold-scale cdf and pdf}

The cdf of the $\GETS$ distribution is
\begin{equation} 
\label{equation:gets.cdf}
F(\realrv;\alpha, \sigma,\varsigma)= \left \{
\begin{array}{ll} \Phi \left [ \log \left ( 1+ \sigma z
           \right )^{1/\sigma}
\right ], & \mbox{for $\sigma>0$, $z>-1/\sigma$}\\
\Phi\left ( z\right ), & \mbox{for $\sigma=0$, $-\infty<  z <\infty$}\\
1-\Phi\left [ \log \left ( 1+ \sigma z \right )^{1/|\sigma|}
\right ], & \mbox{for $\sigma<0$, $z<-1/\sigma$}
\end{array}	
\right.
\end{equation}
where $z=(\realrv-\alpha)/\varsigma$,
$-\infty< \sigma <\infty$,
$-\infty<\alpha<\infty$, and $\varsigma>0$.
The corresponding pdf  is
\begin{displaymath}
\displaystyle
f(\realrv;\alpha, \sigma,\varsigma)=
\left \{
\begin{array}{ll}
\phi
\left [
\log \left ( 1+ \sigma z
           \right )^{1/|\sigma|}
\right ]
\times
\frac{1
     }
     {
 \varsigma (1+\sigma z)
     }, 
& \mbox{for $\sigma \ne 0$   }
\\
\phi
\left (
 z
\right )\times 
\frac{
1
     }
     {
\varsigma
     }, & \mbox{for $\sigma=0,$}.
\end{array}	
\right.
\end{displaymath}
Note that the restrictions on $z$ in (\ref{equation:gets.cdf})
define the range of values of $t$ having positive density, as a
function of $\alpha$, $\sigma$, and $\varsigma$.
In particular, for $\sigma>0$, $t> \alpha-\varsigma/\sigma$
and for $\sigma < 0$, $t <  \alpha-\varsigma/\sigma$.
\subsection{Generalized threshold-scale quantiles}
Inverting the cdf in (\ref{equation:gets.cdf})
gives the $\GETS$ $p$ quantile
\begin{eqnarray*}
\rvquan_{p}&=& \alpha+\varsigma \times w(\sigma,p)
\end{eqnarray*}
where
\begin{eqnarray*}
w(\sigma,p)&=&
\left \{
\begin{array}{ll}
 \frac{\exp[\sigma \Phi^{-1}(p)]-1}
			   {\sigma},
 & \mbox{for $\sigma>0$}
\\[1ex]
\Phi^{-1}(p),  & 
\mbox{for $\sigma=0$}
\\[1ex]
\frac{\exp \left \{| \sigma  | \Phi^{-1}(1-p)
			         \right \}-1}
			   {\sigma},  & \mbox{for $\sigma<0$}.
\end{array}	
\right.
\end{eqnarray*}

\subsection{Special cases of the generalized threshold-scale 
distribution}
\label{section:gets.special.cases}
In this section we show the relationship between the 
$\GETS(\alpha, \sigma,\varsigma)$
and some well-known distributions that are special cases.
\begin{itemize}
\item
The location-scale
distributions, including the normal, logistic,
$\SEV$ and $\LEV$, are obtained by using the
appropriate definition of $\Phi$ along with $\sigma=0$, giving
\begin{displaymath}
F(\realrv; \alpha, 0,\varsigma)=\Phi[(\realrv-\alpha)/\varsigma].
\end{displaymath}
\item
The threshold log-location-scale
distributions are obtained with
$\sigma>0$ giving
\begin{displaymath}
F(\realrv;\alpha, \sigma,\varsigma)=
\Phi\{[\log(\realrv-\threshold)-\mu]/\sigma\},
\quad \realrv>\threshold
\end{displaymath}
where $\threshold=\alpha-\varsigma/\sigma$ and
$\mu=\log(\varsigma/\sigma)$.
Using $\Phi=\Phi_{\nor}$ gives the three-parameter lognormal distribution.
Using $\Phi=\Phi_{\sev}$ gives the three-parameter Weibull 
distribution (also known
as Weibull-type distribution for {\em minima}).
With $\Phi=\Phi_{\lev}$ one obtains the Fr\'echet for 
{\em maxima} with a threshold parameter.
\item
The reflection (negative) of the threshold
log-location-scale distributions are obtained with $\sigma<0$,
giving
\begin{displaymath}
F(\realrv;\alpha, \sigma,\varsigma)=
1 - \Phi\{[\log(-\realrv-\threshold)-\mu]/ |\sigma| \},
\quad \realrv<-\threshold
\end{displaymath}
where $\threshold=-(\alpha-\varsigma/\sigma)$ and
$\mu=\log(-\varsigma/\sigma)$.  With $\Phi=\Phi_{\nor}$ this gives the
negative of a lognormal with a threshold parameter.  With
$\Phi=\Phi_{\sev}$ this gives the negative of a Weibull (also known as
a Weibull-type distribution for {\em maxima}) with a threshold.
\end{itemize}
Figure~\ref{figure:gets.pddf.ps} shows pdfs for the SEV-GETS,
NOR-GETS, and LEV-GETS distributions with $\alpha=0$,
$\sigma=-.75$, 0, .75, and $\varsigma = .5$ (least
disperse), 1, and 2 (most disperse).
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/gets.pddf.ps}
\caption{SEV-GETS, NOR-GETS, and LEV-GETS pdfs with $\alpha=0$,
$\sigma= -.75$, 0, .75, and $\varsigma = .5$ (least disperse), 1, and
2 (most disperse).}
\label{figure:gets.pddf.ps}
\end{figure}
%-------------------------------------------------------------------

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Other Methods of 
Deriving Failure-Time Distributions} 
\label{section:other.deriv}
There are a number of general
methods for deriving other failure-time distributions. The basic idea
behind these methods is to model the physical system
or the physical/chemical processes leading to failure.

%-------------------------------------------------------------------
\subsection{Finite (discrete) mixture distributions}
\label{section:discrete.mix.intro}
Mixtures of distributions often arise in practice.  For example,
components may be manufactured over a period of time, using two
different machines. Physical characteristics and thus
reliability of the components from the two
different machines may be different, but it may be impossible
otherwise to distinguish between the components made with the
different machines. For example, 40\% of all units in a population
were manufactured at plant A and have a lognormal life distribution
with $\exp(\mu_{A})=60$ thousand hours and $\sigma_{A}=.7$. The other
60\% of the units are manufactured at plant B and have a lognormal
distribution with $\exp(\mu_{B})=70$ and $\sigma_{B}=.7$. Then the
population cdf is
\begin{displaymath}
F(t)=.4\times \Phi_{\nor}\left [\frac{\log(t)-\log(60)}{.7}\right ] 
+.6\times\Phi_{\nor}
\left [ \frac{\log(t)-\log(70)}{.7} \right ]
\end{displaymath}
Product mixtures also result from different environments.  For some
products, such as toasters, the environment may be assumed to be
homogeneous.  Other products might be subject to widely different
operating environments (e.g., automobile batteries used in Florida
versus those used in Alaska).  

An extreme product mixture situation arises when a failure type can
occur only in a subset of the population, e.g., on those units for
which an operator skips a step in an assembly operation or units that
are made from a particular batch of raw materials or that include a
particular optional accessory, such as an air-conditioning unit for an
automobile.  This situation is closely related to the concept of
immunity from a failure mode discussed in
Section~\ref{section:using.the.lfp.model}.

More generally, the cdf and pdf of units in a population
consisting of a mixture of units from $k$ different populations can be
expressed as $F(\realrv;\thetavec)=
\sum_{i} \xi_{i} F_{i}(\realrv;\thetavec_{i})$
and $f(\realrv;\thetavec)=
\sum_{i} \xi_{i} f_{i}(\realrv;\thetavec_{i})$, respectively,
where $\thetavec=(\thetavec_{1},\thetavec_{2},\ldots,\xi_{1}, \xi_{2},
\ldots)$, $0 \leq \xi_{i} \leq 1$, and $\sum_{i} \xi_{i}=1$. There
may be some components of the $\thetavec_{i}$ that are common across
the components (or ``subpopulations'') of the mixture. Others will
differ from component to component. In general, however, such
mixtures tend to have a large number of parameters and this usually
makes estimation difficult.

In some situations it may be possible to estimate the parameters of
the individual components of a mixture distribution. This task is
facilitated when it is possible to identify the individual population
from which sample units originated. When identification is not
possible it may be difficult to separate out the different components
from the available data unless there is considerable ``separation'' in
the components and/or enormous amounts of data.  In other situations,
to answer certain questions of interest, it is sufficient to fit a
simple single distribution to describe the overall mixture. Indeed, it
can be said that all populations and processes are mixtures. However,
because individual components may not be identifiable and/or may not
be of interest, we use a single distribution to describe the mixture.
There are, however, some potential pitfalls of fitting a simple
distribution to what is really data from a mixture of different
subpopulations. If data collection methods or censoring tends to over
represent certain subpopulations of the mixture, seriously misleading
conclusions are possible.  Hahn and Meeker~(1982b) describe and
illustrate these pitfalls with an example.  Also see the case study in
Section~\ref{section:cen.mixed.pop}.
%-------------------------------------------------------------------
\subsection{Compound (continuous mixture) distributions}
\label{section:continuous.mixtures}
An important class of probability models arises from distributions in
which one or more of the parameters is modeled by a continuous
random variable. These distributions are called compound distributions
and can also be viewed as a continuous mixture of a family of
distributions.  

Suppose that $\rv$, for a fixed value of a scalar parameter
$\theta_{1}$, has a distribution with density
$f_{\rv|\theta_{1}}(t;\thetavec)$ but that $\theta_{1}$, an element
from the vector $\thetavec=(\theta_{1},\thetavec_{2})$, is itself a
random from unit to unit, according to a distribution with density
$f_{\theta_{1}}(\vartheta;\thetavec_{3})$ where $\thetavec_{3}$ is a
parameter vector that has no elements in common with $\thetavec$. Then
the cdf of the unconditional distribution of $\rv$, a compound
distribution, is
\begin{eqnarray*}
F(t;\thetavec_{2},\thetavec_{3}) =
\Pr(\rv  \leq \realrv) 
	&=& \int_{-\infty}^{\infty} \Pr(\rv  \leq \realrv
|\theta_{1}=\vartheta ) f_{\theta_{1}}(\vartheta;\thetavec_{3})
d\vartheta\\ 
&=&\int_{-\infty}^{\infty}F_{\rv|\theta_{1}=\vartheta}(t;\thetavec)
f_{\theta_{1}}(\vartheta;\thetavec_{3}) d\vartheta
\end{eqnarray*}
and the corresponding pdf is
\begin{displaymath}
f(t;\thetavec_{2},\thetavec_{3}) =
\int_{-\infty}^{\infty}f_{\rv|\theta_{1}=\vartheta}(t;\thetavec)
f_{\theta_{1}}(\vartheta;\thetavec_{3}) d\vartheta.
\end{displaymath}
Extension to a vector of parameters $\thetavec_{1}$ is
straightforward.  Many important applications, however, use only a
scalar $\theta_{1}$.
\begin{example}
\label{example:pareto.dis}
{\bf Pareto distribution.} If the units in a population have an
exponential distribution, but with a failure rate that varies from unit to
unit according to a $\GAM(\theta,\kappa)$
distribution, then the
unconditional time to failure of a unit selected at random from the
population has a Pareto distribution of the form
\begin{equation}
\label{equation:gamma.mix.of.exp}
F(t;\theta,\kappa)=
1 - \frac{1}{(1+\theta t)^{\kappa}},
\quad t > 0.
\end{equation}
The proof of this result is left as an exercise.
\end{example}
%For the derivation, see page~19 of Cox and Oakes~(1984).  
Of course, it is possible to define and use compound distributions
that do not have simple closed-form expressions.  We will illustrate
the use of such distributions in
Section~\ref{section:fatigue.limit.model}.  For other examples of
compound distributions, see page 163 of Johnson, Kotz, and
Balakrishnan~(1994).

%-------------------------------------------------------------------
\subsection{Power distributions}
\label{section:power.dist}
%-------------------------------------------------------------------
Distributions of minima and maxima of iid random variables provide a
useful method for generating distributions of random variables that
have a number of important applications.

{\bf Minimum-type distributions.}
If ${\kappa}$ is a positive constant and $W$ is a random
variable with cdf $F_{W}$,
we say that $\grv$ is a minimum-type
distribution generated from $W$ if for all $\grealrv$
\begin{displaymath}
\Pr(\grv \le \grealrv)=F_{\grv}(\grealrv)=1-[1 - F_{W}(\grealrv)]^{\kappa}
\end{displaymath}
or equivalently, $S_{\grv}(\grealrv)=[S_{W}(\grealrv)]^{\kappa}$
where $S_{W}=1 - F_{W}$ is the survival or reliability function.  An
important special case is when $\kappa$ is an integer. In this situation
$F_{\grv}$ is the cdf of the minimum of ${\kappa}$ independent
observations from $F_{W}$.


It can be shown that $\grv$ is a minimum-type distribution generated
from $W$ if and only if the hazard functions of $\grv$ and $W$ are
proportional, i.e., $h_{\grv}(\grealrv)=\kappa \times h_{W}(\grealrv)$.
Also if $W \sim \WEIB(\mu, \sigma)$ then $\grv \sim \WEIB[\mu-\sigma
\log(\kappa), \sigma]$. But in general (i.e., 
when $W$ is not Weibull distributed) $\grv$ is not a member of
the same family as $W$.

%-------------------------------------------------------------------
{\bf Maximum-type distributions} For $\kappa$ and $W$ defined as in
Minimum-type distributions, we say that $\grv$ is a maximum-type
distribution generated from $W$ if for all $\grealrv$,
\begin{displaymath}
\Pr(\grv \le \grealrv)=F_{\grv}(\grealrv)=[F_{W}(\grealrv)]^{\kappa}.
\end{displaymath}
An important case is when $\kappa$ is an integer. Then $F_{\grv}$ is
the cdf of the maximum of $\kappa$ independent observations from 
$F_{W}$.

%-------------------------------------------------------------------
\subsection{Distributions based on stochastic components of
physical/chemical degradation models} We saw in
Sections~\ref{section:igau.dist} and \ref{section:bisa.dist} how distributions
of time to failure can be derived from details of randomness in
particular physical phenomena.
Chapters~\ref{chapter:degradation.data} and
\ref{chapter:accelerated.test.models} provide other
examples.



%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Bibliographic Notes}
Johnson, Kotz, and Balakrishnan~(1994, 1995) provide detailed
information on a wide range of different continuous probability
distribution functions.  Evans, Hastings, and Peacock~(1993) provide
a brief description and summary of properties of a large number of
parametric distributions including most, but not all, of the
distributions outlined in this chapter.  Chhikara and Folks~(1989)
give detailed information on the inverse Gaussian distribution.

Although the generalized gamma distribution had appeared in the
literature earlier, Prentice~(1974) was the first to provide a
parameterization and operational method for estimation of the distribution
parameters that works well with moderate sample sizes, as long as
censoring is not too heavy. Liu, Meeker, and Escobar~(1998) suggest
and illustrate the use of a parameterization that is stable even for
heavy censoring.  Farewell and Prentice~(1977) showed how to use the
generalized gamma distribution in problems of parametric distribution
discrimination.  Prentice~(1975) extends the generalized gamma
distribution to the generalized F distribution.  

Cheng and Iles~(1990)
describe some statistical problems created by embedded
distributions that arise as limiting cases of the threshold (3 parameter)
gamma, inverse Gaussian, loglogistic, lognormal, and Weibull
distributions, providing motivation for the GETS family of
distributions. See also Nakamura~(1991) for a different view of
embedded distributions within threshold families.

Titterington, Smith, and Makov~(1985) and Everitt and Hand~(1981)
provide detailed information on finite mixture distributions. Nelson
and Doganaksoy~(1995) describe the power lognormal distribution,
including methods for estimation. Barlow and Proschan~(1975) provide a
detailed and extensive discussion of classes of distributions that are
based on different hazard function behaviors like IHR and DHR.



%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section*{Exercises}

\begin{exercise}
\label{exercise:simple.mix.exp}
If the times to failure in a population are adequately described by a
distribution with a decreasing hazard function, one might think that the
surviving
units in the population are getting better with time.  In fact,
decreasing hazard functions are common for certain
solid-state electronic components and electronic systems. 
Weaker units fail early, after which the hazard decreases.
For a mixture
of two exponential distributions with $\gamma=0$ but different values
of 
$\theta$ (say $\theta_{1}=1$ and $\theta_{2}=5$), and equal proportions
from the two populations, do the following:
\begin{enumerate}
\item
Obtain an expression for the cdf of the mixture.
\item
Obtain an expression for the pdf of the mixture.
\item
Use the previous 2 parts to derive an expression for the hazard
function  of the mixture.
\item
Graph the mixture hazard from $t=0$  to $t=10$. 
\item
What is the shape of
the mixture hazard? What is the intuition for this result?
\item
In what sense is the mixed exponential population ``improving'' with time (as
suggested by the
decreasing hazard function)?
\end{enumerate}
\end{exercise}

\begin{exercise}
In some applications a sample of failure times comes from
a mixture of subpopulations.
\begin{enumerate}
\item
Write down the expression for the cdf $F(t)$ for a mixture of two
exponential distributions with means $\theta_{1}=1$ and
$\theta_{2}=10$ (subpopulations $1$ and $2,$ respectively) with $\xi$
being the proportion from subpopulation 1.
\item
For $\xi=0, .1, .5, .9$ and $1,$ compute the mixture $F(t)$ for a
number of values of $t$ ranging between $0$ and $30.$ Plot these
distributions on one graph.
\item
Plot  $\log(t)$ versus $\log\{-\log[1-F(t)]\}$  for each $F(t)$ computed
in part (b). Comment on the shapes of the mixtures of exponential
distributions, relative to a pure exponential distribution or a
Weibull distribution.
\item
Plot the hazard function $h(t)$ of the mixture distributions in part (b).
\item
Qualitatively, what do the Weibull plots in part (c) suggest about the
hazard function of a mixture of two exponential distributions?
\end{enumerate}
\end{exercise}

\begin{exercise}
Show that the exponential distribution is a special case of the
gamma distribution given in Section~\ref{section:gamma.distribution}.
\end{exercise}

\begin{exercise1}
\label{exercise:mix.exp.haz.dec}
Refer to Exercise~\ref{exercise:simple.mix.exp}.
Show that a mixture of two exponential distributions with different
$\theta$ values will always have a decreasing hazard function. 
\end{exercise1}

\begin{exercise1}
Conduct a numerical/graphical comparison of the shapes of
the hazard function for a populations consisting of the mixture of
two Weibull distributions. Investigate all combinations of the
parameters $\alpha_{1} > \alpha_{2}=1,2$;
$\beta_{1}, \beta_{2}=.5,2$; $\xi_{1}=.01,.4.$
What do you conclude?
\end{exercise1}


\begin{exercise1}
Let $\rv_{i}, i=1,\ldots,\gammashape$ be $\gammashape$ independent
random variables from the $ \EXP(\expmean)$ distribution. Show that
the random variable $\sum_{i=1}^{\gammashape} \rv_{i}$ has a
$\GAM(\theta,\gammashape)$ distribution.
\end{exercise1}


\begin{exercise1}
Section \ref{section:gng.special.cases} gives important special cases
of the generalized gamma distribution. Use direct substitution to show
the relationships to the WEIB, EXP and GAM distributions. Use a
limiting agreement to show the relationship to the LOGNOR
distribution.
\end{exercise1}



\begin{exercise1}
As described in Example~\ref{example:pareto.dis},
the Pareto distribution can be derived as a gamma mixture of exponential
distributions.
\begin{enumerate}
\item
Show this by deriving (\ref{equation:gamma.mix.of.exp}).
\item
Take the first derivative of (\ref{equation:gamma.mix.of.exp})
with respect to $t$ to obtain the Pareto pdf.
\item
Plot the Pareto hazard function for several different
combinations of $\theta$ and $\kappa$.
\end{enumerate}
\end{exercise1}

\begin{exercise1}
Derive the expression for the Birnbaum-Saunders $p$ quantile
given in (\ref{equation:bisa.quantile}).
\end{exercise1}

\begin{exercise1}
\label{exercise:goma.haz}
As an interpretation of the Gompertz-Makeham distribution, suppose
that the failure time of a device is determined by which of the
following two events happens first:
\begin{itemize}
\item
Wear out at time $W$,  which can be
modeled by an $\SEV(\mu, \sigma)$, left truncated at time zero.
\item
An accident at time $R$, which can be modeled by
an $\EXP(\lambda)$.
\end{itemize}
In other words, the device
failure time is $\rv=\min\{W,R\}$.
For this exercise, also suppose that $W$ and $R$ are independent.
\begin{enumerate}
\item 
Show that the cdf for $\rv$ is the same as the Gompertz-Makeham cdf,
having the form
\begin{eqnarray*}
\Pr(\rv \le \realrv; \mu, \sigma, \lambda)&=&
1- 
 \left [
 \frac{
   1-\Phi_{\rm sev}
  \left  (
   \frac{\realrv-\mu}{\sigma}
  \right )
      }
      {
   1-\Phi_{\rm sev}
  \left  (
   \frac{-\mu}{\sigma}
  \right )
      }
\right ]\, \exp \left ( -\frac{\realrv}{\lambda}
\right ), \quad \realrv > 0
\\
&=&
\Phi_{\rm lgoma} \left [\log \left ( \frac{\realrv}{\theta} \right ) ; \zeta, \eta \right ]
\end{eqnarray*}
where $\theta=\sigma$, $\zeta=\mu/\sigma$, and
$\eta=\sigma/\lambda$.  (Hint: one can write
$F_{\rv}(\realrv)=1-[1-F_{W}(t)][1-F_{R}(t)]$. Explain why.  )
\item 
\label{part:haz.sum}
Show that the hazard function for $\rv$ is $h_{\rv}(\realrv; \mu, \sigma,
\lambda)=h_{R}(\realrv; \lambda)+ h_{W}(\realrv; \mu,\sigma)$. Also
show that this coincides with the hazard function of a
$\GOMA(\theta,\zeta,
\eta)$.
\end{enumerate}
\end{exercise1}

\begin{exercise1}
Show that the hazard relationship in part~\ref{part:haz.sum} of
Exercise~ \ref{exercise:goma.haz} holds in general when $\rv=\min
\{W, R\}$ where $W$ and $R$ are any two independent continuous
random variables.
\end{exercise1}

\begin{exercise}
Let $T_{(1)}$ denote the minimum of $m$ independent Weibull random
variables with parameters $\mu_{i}, i=1, \ldots, m$ and constant
$\sigma$.  Show that $T_{(1)}$ has a Weibull distribution.
\end{exercise}


\begin{exercise1}
Use the lognormal base (i.e.,
use $\Phi=\Phi_{\nor}$) $\GETS(\eta, \sigma, \zeta)$ 
distribution given in Section~\ref{section:intro.to.threshold.distributions}
to do the following
\begin{enumerate}
\item
Plot the cdfs and pdfs for all possible combinations of 
the parameters $\eta=-.5, .5$, $\sigma=-1,1$,
and $\zeta=1$.
\item
For fixed $\eta=0$ and $\zeta=1$, draw the cdfs for several small
values of $\sigma$ say $\sigma=\pm .1, \pm .01, \pm .001$
and compare with a $\NOR(0,1)$ cdf. How well do the
$\GETS$ cdfs approach the normal cdf as $\sigma \to 0$?
\end{enumerate}
\end{exercise1}
%----------------------------------------------------------------------
%----------------------------------------------------------------------

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{exercise1}
Consider the $\GETS(\eta, \sigma, \zeta)$ 
distribution as given 
in~Section~\ref{section:intro.to.threshold.distributions}. 
\begin{enumerate}
\item
Show that when $\sigma \to 0$ then
the $\GETS$ cdf approaches 
$\Phi(z)$. (Hint: when $\sigma \to 0$, $(1/\sigma) \log(1+ \sigma z) \to z$).
\item
Show that, in terms of the
parameters $(\threshold, \alpha, \eta)$
of Section~\ref{section:intro.to.threshold.distributions},
when $\sigma$ approaches
$0$ from above (below) then
the $\threshold$ is approaching $+\infty$ ($-\infty$) and
$(\threshold+\alpha) \to \eta$.
\end{enumerate}
\end{exercise1}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
