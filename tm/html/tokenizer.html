<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Tokenizers</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for tokenizer {tm}"><tr><td>tokenizer {tm}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Tokenizers</h2>

<h3>Description</h3>

<p>Tokenize a document or character vector.</p>


<h3>Usage</h3>

<pre>
Boost_tokenizer(x)
MC_tokenizer(x)
scan_tokenizer(x)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>A character vector, or an object that can be coerced to character by
<code>as.character</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The quality and correctness of a tokenization algorithm highly depends
on the context and application scenario. Relevant factors are the
language of the underlying text and the notions of whitespace (which
can vary with the used encoding and the language) and punctuation
marks. Consequently, for superior results you probably need a custom
tokenization function.
</p>

<dl>
<dt>Boost_tokenizer</dt><dd><p>Uses the Boost (<a href="http://www.boost.org">http://www.boost.org</a>)
Tokenizer (via <span class="pkg">Rcpp</span>).</p>
</dd>
<dt>MC_tokenizer</dt><dd><p>Implements the functionality of the tokenizer in the
MC toolkit (<a href="http://www.cs.utexas.edu/users/dml/software/mc/">http://www.cs.utexas.edu/users/dml/software/mc/</a>).</p>
</dd>
<dt>scan_tokenizer</dt><dd><p>Simulates <code>scan(..., what = "character")</code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A character vector consisting of tokens obtained by tokenization of <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="getTokenizers.html">getTokenizers</a></code> to list tokenizers provided by package <span class="pkg">tm</span>.
</p>
<p><code><a href="../../NLP/html/Regexp_Tokenizer.html">Regexp_Tokenizer</a></code> for tokenizers using regular expressions
provided by package <span class="pkg">NLP</span>.
</p>
<p><code><a href="../../tau/html/tokenize.html">tokenize</a></code> for a simple regular expression based tokenizer
provided by package <span class="pkg">tau</span>.
</p>
<p><code><a href="../../tokenizers/html/tokenizers.html">tokenizers</a></code> for a collection of tokenizers provided
by package <span class="pkg">tokenizers</span>.
</p>


<h3>Examples</h3>

<pre>
data("crude")
Boost_tokenizer(crude[[1]])
MC_tokenizer(crude[[1]])
scan_tokenizer(crude[[1]])
strsplit_space_tokenizer &lt;- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))
strsplit_space_tokenizer(crude[[1]])
</pre>

<hr /><div style="text-align: center;">[Package <em>tm</em> version 0.7-5 <a href="00Index.html">Index</a>]</div>
</body></html>
