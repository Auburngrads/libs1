%chapter 12
%\batchmode
%original by wqmeeker  12 Jan 94
%edited by wqmeeker 26 Apr 94
%edited by wqmeeker  9 aug 94
%edited by driker 25 july 97
%edited by wqmeeker 24 aug 97 
%edited by wqm 24/26 oct 1997 notation and cond prob changes

\setcounter{chapter}{11}

\chapter{Prediction of Future Random Quantities}

\label{chapter:prediction}

\input{\chapterhome/common_heading.tex}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Objectives}
This chapter explains
\begin{itemize} 
\item 
Important reliability-related
applications of prediction.
\item 
The difference between probability prediction and statistical
prediction.
\item 
Methods for computing predictions
and prediction bounds for future failure times.
\item 
Methods for computing predictions and prediction bounds for the
number of failures in a future time interval.
\end{itemize}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Overview}
This chapter describes methods to construct prediction bounds or
intervals for future random quantities. Both new-sample prediction
(using data from a previous sample to make predictions on a future
unit or sample of units) and within-sample prediction problems
(predicting future events in a sample based on early data from the
sample) are considered.  To illustrate new-sample prediction we show
how to construct a prediction interval for a single future
observation from a previously sampled population/process (motivated
by a customer's request for an interval to contain the life of a
purchased product).  To illustrate within-sample prediction, we show
how to compute a prediction interval for the number of future
failures in a specified period beyond the observation period
(motivated by a warranty prediction problem).  A third example
requires more general methods to deal with complicated censoring
arising because units enter service at different points in time
(staggered entry).  Section~\ref{section:prob.pred} describes
``probability prediction intervals'' for a completely specified
distribution. Probability prediction intervals, when computed on the
basis of estimates from limited data, are sometimes called ``naive
prediction intervals,'' and they can serve as a basis for developing
more commonly needed statistical prediction intervals.
Section~\ref{section:stat.pred} describes coverage probability
concepts and other basic ideas pertaining to statistical prediction
intervals.  Section~\ref{section:pivot.pred.method} introduces the basic
ideas of pivotal methods for complete and Type~II censored data from
log-location-scale distributions and shows how they can be
extended to obtain approximate prediction intervals for the more
commonly used Type~I censoring. Section~\ref{section:pred.simple}
describes some simple special case prediction interval procedures
that can be implemented with simple hand computations.
Section~\ref{section:calibrate.pred} presents a more general
approach of calibrating naive statistical prediction intervals and
shows how these methods are related to the pivotal-like methods.
Section~\ref{section:pred.single.group} shows how to apply the
calibration method to a commonly occurring problem of predicting
future field failures on the basis of early field failures.
Section~\ref{section:pred.multiple.groups} extends the field
prediction problem to situations where units enter the field over a
longer period of time.

%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
\section{Introduction}
%--------------------------------------------------------------------------
\subsection{Motivation and prediction problems}
%--------------------------------------------------------------------------
Practical problems often require the computation of predictions
and prediction bounds for
future values of random quantities. For example,
\begin{itemize}
 \item 
A consumer purchasing a refrigerator would like to
have a lower bound for the failure time of the unit to be purchased (with
less interest in distribution of the population of units purchased by
other consumers).
\item 
Financial managers in manufacturing companies need
upper prediction bounds on future warranty costs. 	
\item 
When planning life tests, engineers may need to predict the number of
failures that will occur by the end of the test or the amount of time
that it will take for a specified number of units to fail.
\end{itemize}

Some applications require a two-sided prediction interval
$[\Tlower, \quad \Tupper]$ that will, with a specified high degree of
confidence, contain the future random variable of interest, say
$\frv$. In many applications, however, interest is focused on either
an upper prediction bound or a lower prediction bound (e.g., the
maximum warranty cost is more important than the minimum and the time
of the early failures in a product population is more important
than the last ones).

Conceptually, it is useful to distinguish between ``new-sample''
prediction (see Figure~\ref{figure:newsamplefig.ps}) and
``within-sample'' prediction (see
Figure~\ref{figure:withinsamplefig.ps}).
%-------------------------------------------------------------------
\begin{figure}
\xfigbookfiguresize{\figurehome/newsamplefig.ps}{3in}
\caption{New-sample prediction.}
\label{figure:newsamplefig.ps}
\end{figure}
%-------------------------------------------------------------------
For new-sample
prediction, data from a past sample is used to make predictions on a
future unit or sample of units from the same process or population.
For example, based on previous (possibly censored) life test data,
one could be interested in predicting the	
\begin{itemize} 
\item 
Time to failure of a new item.
\item 
Time until $k$ failures in a future sample of $m$ units.
\item 
Number of failures by time $\realrv_{w}$ in a future sample of
$m$ units.
\end{itemize} 

For within-sample prediction, the problem is to predict future events
in a sample or process based on early data from that sample or
process.
For example if $n$ units are followed until $\censortime$
and there are $r$ observed failures, $t_{(1)}, \ldots, t_{(r)}$, one could 
be interested in predicting the
\begin{itemize} 
\item 
Time of the next failure. 
\item 
Time until $k$ additional failures.
\item 
Number of additional failures in a future interval $(\censortime,
\predicttime)$.
\end{itemize} 
%-------------------------------------------------------------------
\begin{figure}
\xfigbookfiguresize{\figurehome/withinsamplefig.ps}{3in}
\caption{Within-sample prediction.}
\label{figure:withinsamplefig.ps}
\end{figure}
%-------------------------------------------------------------------

%--------------------------------------------------------------------------
\subsection{Model}
In general to predict a future realization of a random quantity one
needs:
\begin{itemize} 	 
\item
A statistical model to describe the population or process of interest.
This model usually consists of a distribution depending on a vector 
of parameters $\thetavec$.
Nonparametric new-sample prediction is also possible (Chapter 5 of Hahn
and Meeker 1991 gives examples and references).
\item
Information on the values of the parameters $\thetavec$.  This
information could come from either a laboratory life test or field
data.
\end{itemize} 
We will assume that the failure times have a continuous distribution
with cdf $F(t)=F(t; \thetavec)$ and pdf $f(t)=f(t; \thetavec)$, where
$\thetavec$ is a vector of parameters. Generally, $\thetavec$ is
unknown and will be estimated from available sample data.  In such
cases we will make the standard assumptions of statistical
independence of failure times and that censoring times are independent
of any future failure time, that would be observed if a unit were not
to be censored (as described in Section~\ref{section:import.cen.assump}).

%--------------------------------------------------------------------------
\subsection{Data}
%--------------------------------------------------------------------------
The beginning of this chapter considers situations in which
$n$ units begin operation at time 0 and are observed
until a time $\censortime$ where the available data are to be analyzed. 
Failure times are recorded for the $r$ units
that fail in the interval $(0, \censortime)$. Then the data consist
of the $r$ smallest order statistics $\realrv_{(1)}< \cdots <
\realrv_{(r)} \le \censortime$ and the information that 
the other $n-r$ units will have failed after $\censortime$.
Section~\ref{section:pred.multiple.groups} shows how to compute
prediction bounds for more complicated kinds of censored data that are
frequently encountered in the analysis of field reliability data.

%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
\section{Probability Prediction Intervals ($\boldsymbol{\theta} $ Given)}
\label{section:prob.pred}
With a completely specified continuous probability distribution, an exact
$100(1-\alpha)\%$ ``probability prediction interval'' for a future 
observation from $F(\rvquan;\thetavec)$ is (ignoring any
data)
\begin{equation}
\label{equation:probability.prediction}
	\pinterval(1-\alpha)=[ \Tlower,\quad \Tupper ] =
[\rvquan_{\alpha/2}, \quad \rvquan_{1-\alpha/2}]
\end{equation}
where $\rvquan_{p}$ is the $p$ quantile of
$F(\rvquan;\thetavec)$. The probability of coverage of the interval in
(\ref{equation:probability.prediction}) is
\begin{displaymath}
\Pr[T \in \pinterval(1-\alpha)] = 
	\Pr(\Tlower \leq \rv \leq \Tupper)=
	\Pr(\rvquan_{\alpha/2} \leq \rv \leq \rvquan_{1-\alpha/2})=
	1-\alpha
\end{displaymath}
by the definition of quantiles of continuous distributions.

\begin{example}
\label{example:spec.dist.pi}
{\bf Prediction interval for a completely-specified probability
distribution.} A potential customer plans to purchase a system. The
system contains a bearing known to be a life-limiting component that
has failed in some existing systems.  The potential customer needs a
lower prediction bound on $T$, the number of use-cycles to failure
for one of these systems that is to be placed into service.  Based
on previous experience, the manufacturer believes that the number of
cycles to failure for the bearing can be described by a lognormal
cdf
\begin{displaymath}
	\Pr(T \leq t) = F(t;\mu,\sigma) = \Phi_{\nor}
\left[\frac{\log(t) - \mu}{\sigma}\right]
\end{displaymath}
with specified parameters $\mu=4.0$ and $\sigma=.5$.  From
(\ref{equation:probability.prediction}), a two-sided $90\%$
probability prediction interval is
\begin{eqnarray*}
	[ \Tlower, \quad \Tupper ] &=& [ \exp(\mu + \Phi_{\nor}^{-1}(.05)
\times \sigma), \quad
\exp(\mu + \Phi_{\nor}^{-1}(.95) \times \sigma)] \\
&=& \left [ \exp(4.0 + (-1.645) \times .5), \quad \exp(4.0 + 1.645 \times .5)
\right ]= \left [23.93, \quad 124.59  \right].
\end{eqnarray*}
Then $\Pr(\Tlower \leq \rv \leq \Tupper)= \Pr( 23.93 \leq \rv
\leq 124.59 ) = .90.$ With misspecified parameters, coverage
probability may not be .90.
\end{example}

%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
\section{Statistical Prediction Interval ($\boldsymbol{\theta} $ Estimated)}
\label{section:stat.pred}
%--------------------------------------------------------------------------
\subsection{Coverage probability concepts}
\label{section:pi.coverage.prob}
Before describing methods for constructing $\thetavec$-estimated
prediction intervals, let us first consider methods for evaluating
the properties of prediction intervals. In particular, ``coverage
probability'' is an important property.  We describe these concepts
in terms of a ``new-sample'' prediction interval for a
future observation but the ideas also hold for other new-sample
prediction problems and within-sample prediction problems.
 
In statistical prediction, the objective is to predict the random
quantity $\frv$ based on ``learning'' sample information (denoted by
$\DATA)$).  Generally, with only sample data, there is uncertainty
in the distribution parameters. The random $\DATA~$ leads to a
parameter estimate $\thetavechat$ and then to a nominal
100$(1-\alpha)$\% prediction interval
$\pinterval(1-\alpha)=[\Tlower,\quad \Tupper ]$.  Thus $[\Tlower,
\quad \Tupper ]$ and the future random variable $\frv$ have a joint
distribution that depends on a parameter vector $\thetavec$.

There are two kinds of coverage probabilities:
\begin{itemize}
\item 
For fixed $\DATA~$   (and thus fixed $\thetavechat$ and $[
\Tlower, \quad \Tupper ]$) the conditional coverage probability 
of a particular interval $[\Tlower, \quad \Tupper ]$ is 
\begin{equation}
\label{equation:cond.coverage}
\cp[\pinterval(1-\alpha) \mid \thetavechat ; \thetavec]
	= \Pr(\Tlower \leq \rv \leq \Tupper \mid \thetavechat ; \thetavec )
	= F(\Tupper;\thetavec)-F(\Tlower;\thetavec).
\end{equation}
This conditional probability is {\em unknown} because $F(t;\thetavec)$
depends on the unknown $\thetavec$.  
\item 
From sample to sample, the conditional coverage probability is
{\em random} because $[ \Tlower, \quad \Tupper ]$ depends on $\thetavechat$.
The unconditional coverage probability for the prediction interval
{\em procedure} is
\begin{equation}
\label{equation:uncond.coverage}
	\cp[\pinterval(1-\alpha) ;\thetavec] =
\Pr(\Tlower \leq \rv \leq\Tupper ; \thetavec) =
		 \E_{\thetavechat} \left \{\cp[\pinterval(1-\alpha) 
	\mid \thetavechat ;\thetavec]
			   \right \}
\end{equation}
where the expectation is with respect to the random $\thetavechat$.
Because it can be computed (at least approximately) and can be
controlled, it is this unconditional probability that is generally used to
describe a prediction interval procedure. When $\cp[\pinterval(1-\alpha) ;
\thetavec]=1-\alpha$ does not depend on $\thetavec$, the procedure is said 
to be ``exact.'' In general $\cp[\pinterval(1-\alpha) ;\thetavec]
\approx 1-\alphanom$ because of dependency on the unknown $\thetavec$.
In such cases only an approximate prediction interval procedure
is available.  
\end{itemize}

%--------------------------------------------------------------------------
\subsection{Relationship between one-sided and
two-sided prediction intervals}

Combining a one-sided lower $100(1-\alpha/2)\%$ prediction
bound and a one-sided upper 
$100(1-\alpha/2)\%$ prediction
bound gives an equal-tail two-sided $100(1-\alpha)\%$ probability 
prediction interval.  In particular, if $\Pr(\Tlower \leq \rv <
\infty) = 1-\alpha/2$ and $\Pr( 0  < \rv \leq \Tupper) =
1-\alpha/2,$ then $\Pr(\Tlower \leq \rv \leq 	\Tupper) = 1-\alpha.$
It may be possible to find a narrower interval with
unequal probabilities in the upper and lower tails, still summing to
$\alpha$. Use of equal-probability intervals, however, has the
important advantage of providing an interval that has endpoints that
can be correctly interpreted as one-sided prediction bounds (with the
appropriate adjustment in the confidence level).  This is important
because in most applications the cost of predicting too high is
different from the cost of predicting too low and two-sided
prediction intervals
are often reported even though primary interest is on one side or the
other.

\subsection{Naive method for computing a statistical prediction interval}
A ``naive'' prediction interval for continuous $\rv$ is obtained by
substituting the maximum likelihood (ML) estimate for $\thetavec$
into (\ref{equation:probability.prediction}), giving
\begin{displaymath}
\label{equation:naive.coverage}
	\pinterval(1-\alpha)=[ \Tlower,\quad \Tupper ] =  
[\rvquanhat_{\alpha/2},\quad\rvquanhat_{1-\alpha/2}]
\end{displaymath}
where $\rvquanhat_{p}=\rvquan_{p}(\thetavechat)$ is the ML estimate
of the $p$ quantile of $T$.  To predict a future independent
observation from a log-location-scale distribution, the naive
prediction interval is
\begin{eqnarray}
\nonumber
	\pinterval(1-\alpha)&=&[ \Tlower,\quad \Tupper ] = 
[\rvquanhat_{\alpha/2},\quad\rvquanhat_{1-\alpha/2}]\\
\label{equation:lls.naive.pi}
	&=& [\exp(\muhat + \Phi^{-1}(\alpha/2) \times \sigmahat),\quad
	\exp(\muhat + \Phi^{-1}(1-\alpha/2) \times \sigmahat)].
\end{eqnarray}
The unconditional coverage probability for this naive procedure is
approximately equal to the nominal $1-\alpha$ with large samples
sizes. For small to moderate number of units failing, however, the
coverage probability may be {\em far} from $1-\alpha$.


\begin{example}
\label{example:naive.ln.pi}
{\bf Naive prediction interval for predicting the life of a ball
bearing (lognormal distribution).}  Refer to the problem of
predicting bearing life in Example~\ref{example:spec.dist.pi}, but
suppose that only limited data are available to make the prediction.
Figure~\ref{figure:lzb5.mleprobpolt.lognor.ps} is a lognormal
probability plot of the first 15 of 23 failures in a bearing life
test described in Lawless~(1982, page 228) when the data are
right-censored at 80 million cycles.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/lzb5.mleprobpolt.lognor.ps}
\caption{Lognormal probability plot
of bearing life test data censored after 80 million cycles (with 15 of
23 units failed) with lognormal ML estimates and pointwise 95\%
confidence intervals.}
\label{figure:lzb5.mleprobpolt.lognor.ps}
\end{figure}
%-------------------------------------------------------------------
Failures occurred at 17.88, 28.92, 33.00,
41.52, 42.12, 45.60, 48.40, 51.84, 51.96, 54.12, 55.56, 67.80, 68.64,
68.64, and 68.88 million revolutions. The other eight bearings were treated as if 
they had been censored at 80 million cycles.
The lognormal ML estimates are $\muhat=4.160$ and $\sigmahat=.5451$.
%splusML Estimates
%splus            MLE        se   t.ratio 95% lower 95% upper 
%splus   mu 4.160028 0.1240819 33.529495 3.9172068 4.4035988
%splussigma 0.5451135 0.1078112  5.056186 0.3699461 0.8032218
%splus   exp(4.160-1.645*(0.5451))
%splus [1] 26.13592
%splus   exp(4.160+1.645*(0.5451))
%splus  [1] 157.0697
%splus
From~(\ref{equation:lls.naive.pi}), the naive two-sided $90\%$
prediction interval is
\begin{eqnarray}
        [ \Tlower, \quad \Tupper ] &=&
\label{equation:naive.pi}
\left [ \exp(\muhat + \Phi_{\nor}^{-1}(.05) \times \sigmahat), \, 
\quad\exp( \muhat + \Phi_{\nor}^{-1}(.95)  \times \sigmahat)
\right ] \\
 &=&
\left [ \exp(4.160 + (-1.645) \times .5451), \,
	\quad \exp(4.160 + 1.645 \times .5451)
	\right ] =  \left [26.1, \quad 157.1  \right]. \nonumber
\end{eqnarray}
Intervals constructed in this manner are generally too narrow and their
coverage probability is below the nominal value of $1-\alpha$ because
they ignore the uncertainty in $\muhat$ and $\sigmahat$ relative to
$\mu$ and $\sigma$.
\end{example}


For the Weibull distribution, the prediction interval can be computed
analogously by recognizing that $\log(T)$ has a smallest extreme
value (SEV) distribution. In particular,
\begin{displaymath}
\Pr(\rv \leq \realrv;\weibscale,\beta ) = 1-
\exp \left [-\left (\frac{\realrv}{\weibscale} \right )^{\beta}
\right ] =  \Phi_{\sev}\left [\frac{\log(\realrv)-\mu}{\sigma}
\right ],    \quad \realrv > 0
\end{displaymath}
where $\mu=\log(\eta)$ and $\beta=1/\sigma$.

\begin{example}
\label{example:naive.weibull.pi}
{\bf Naive prediction interval for predicting the life of a ball
bearing
(Weibull distribution).}
%splus
%splus   stuff for the weibull naive to be added
%splus
%splusParameter Estimation Results 
%splus ML Estimates
%splus             MLE        se   t.ratio 95% lower 95% upper 
%splus    mu 4.3343549 0.1053891 41.127147 4.1277959 4.5409138
%splus sigma 0.4012563 0.0914223  4.389042 0.2567343 0.6271331
%splus
%splus Weibull beta= 2.827134 eta= 1.424349 
%splus
%splus
%splus     exp(4.334 + qsev(.05)*.4013) = 23.15152
%splus     exp(4.334 + qsev(.95)*.4013) =  118.4276
%splus
%splus
%splus
The Weibull distribution also provides an adequate description of
the censored ball bearing data.
Following the approach in Example~\ref{example:naive.ln.pi}, the
Weibull ML estimates are $\muhat=4.334$ and $\sigmahat=.4013$.
From~(\ref{equation:lls.naive.pi}), the naive two-sided $90\%$
prediction interval is
\begin{eqnarray}
          [ \Tlower, \quad \Tupper ] &=&
\label{equation:naive.weibull.pi}
\left [ \exp(\muhat + \Phi_{\sev}^{-1}(.05) \times \sigmahat), \, 
\quad\exp( \muhat + \Phi_{\sev}^{-1}(.95)  \times \sigmahat)
\right ] \\
 &=&
\left [ \exp(4.334 + (-2.970) \times .4013), \,
	\quad \exp(4.334 + 1.097 \times .4013)
	\right ] =  \left [23.2, \quad  118.4 \right] \nonumber
\end{eqnarray}
Note that, in comparison with the prediction interval for the
lognormal distribution in Example~\ref{example:naive.ln.pi}, the
Weibull prediction interval has a much smaller upper endpoint. It is
typical that the lognormal distribution, when compared with the
Weibull, will provide a more optimistic extrapolation into the upper
tail of a fitted distribution. This is because the lognormal
distribution has a much ``longer'' upper tail.
\end{example}

%--------------------------------------------------------------------------
\section{The (Approximate) Pivotal Method for Prediction Intervals}
%--------------------------------------------------------------------------
\label{section:pivot.pred.method}
\subsection{Type~II (failure) censoring}
\label{section:tyep2.pivot.pred.method}
With Type~II (failure) censoring, a life test is run until a
specified number of $r$ failures where $1 \leq r \leq n$.  When $T$
has a log-location-scale distribution and the data are complete or
Type~II (failure) censored, the random variable $Z_{\log(T)}=\left [
\log(T)-\muhat \right ]/\sigmahat$ is pivotal. That is, the
distribution of $Z_{\log(T)}$ depends only on $n$ and $r$
but it does not depend on $\mu$ or
$\sigma$. Then
\begin{displaymath}
\Pr \left[ \norquan_{{\log(T)}_{(\alpha/2)}} < 
\frac{\log(T)-\muhat}{\sigmahat}
\leq \norquan_{{\log(T)}_{(1-\alpha/2)}} \right] 
= 1-\alpha
\end{displaymath}
where $\norquan_{{\log(T)}_{(p)}}$ is the $p$ quantile of
$Z_{\log(T)}$. Thus
\begin{displaymath}
\Pr \left[ \muhat + \norquan_{{\log(T)}_{(\alpha/2)}} \times \sigmahat < 
\log(T)
\leq \muhat + \norquan_{{\log(T)}_{(1-\alpha/2)}} \times \sigmahat \right] = 1-\alpha
\end{displaymath}
which suggests the prediction interval
\begin{displaymath}
[ \Tlower, \quad \Tupper ] = 
\left 	[ \exp(\muhat + \norquan_{{\log(T)}_{(\alpha/2)}} \times \sigmahat),
\quad \exp(\muhat + \norquan_{{\log(T)}_{(1-\alpha/2)}} \times \sigmahat)
\right  ].
\end{displaymath}
The quantiles $\norquan_{{\log(T)}_{(\alpha/2)}}$ and
$\norquan_{{\log(T)}_{(1-\alpha/2)}}$ can be obtained from the
distribution of $Z_{\log(T)}$ which can be obtained approximately
(the approximation due only to Monte Carlo error) by simulating $B$
realizations of
\begin{equation}
\label{equation:z.pivot}
Z_{\log(\Tboot)}= \frac{\log(\Tboot)-\muhatboot}{\sigmahatboot}.
\end{equation}
The procedure is
\begin{enumerate}
\item
Draw a sample of size $n$ from a log-location-scale distribution
with parameters $(\muhat,\sigmahat)$, censored at the $r$th failure.
\item
Use the censored sample to compute ML estimates $\muhatboot$ and
$\sigmahatboot$.
\item
Draw an additional single observation $\Tboot$ from
the log-location-scale distribution with parameters $(\muhat,\sigmahat)$.
\item
Compute $Z_{\log(\Tboot)}=[\log(\Tboot)-\muhatboot]/\sigmahatboot$.
\item
Repeat steps 1 to 4 $B$ times. Obtain the approximations for the
quantiles $\norquan_{{\log(T)}_{(\alpha/2)}}$ and
$\norquan_{{\log(T)}_{(1-\alpha/2)}}$ from the empirical
distribution of $Z_{\log(\Tboot)}$.
\end{enumerate}
Because the quantiles of $Z_{\log(T)}$ depend only on $n$ and $r$
the procedure to predict~$T$ will, except for Monte Carlo error, have
exactly the nominal coverage probability.

\subsection{Type~I (time) censoring}
For single time censoring (test run until a specified censoring time
$\censortime$), the simulation procedure is modified by censoring
the samples at the specified censoring time $\censortime$. In this
case, the $Z_{\log(T)}$ is only approximately pivotal and quantiles
of $Z_{\log(T)}$ depend on $F(\censortime,\thetavec)$ (the unknown
expected proportion failing by time $\censortime$) and the sample
size $n$. Thus, with Type~I censoring, the prediction interval to
predict~$T$ is approximate.

\begin{example}
\label{example:pivot.ln.pi}
{\bf Approximate prediction interval for predicting the life
of a ball bearing based on an approximate pivotal (lognormal distribution).}
Figure~\ref{figure:lzb5.lognor.pivot.hist.ps} shows the simulated
distribution of the lognormal predictive pivotal-like statistic in
(\ref{equation:z.pivot}).
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/lzb5.lognor.pivot.hist.ps}
\caption{Histogram of 100,000 simulated $Z_{\log(\rvhat^{*})}$ 
values, based on the bearing life test data censored after 80 million cycles.}
\label{figure:lzb5.lognor.pivot.hist.ps}
\end{figure}
%-------------------------------------------------------------------
The simulated values were obtained by doing 100,000 simulations of the
censored life test, using the lognormal distribution and the ML
estimates for bearing life from Example~\ref{example:naive.ln.pi}.  The
needed quantiles of the simulated distribution are
$\norquan_{{\log(T^{*})}_{(.05)}}\approx -1.802$ and
$\norquan_{{\log(T^{*})}_{(.95)}} \approx 1.837$. Thus a two-sided
approximate 90\% prediction interval for lognormal $T$ is
%splus
%splus> lzb80.gethenout.twosamp.p1.lnor.out$pred.quant
%splus       .01      .025       .05        .1 
%splus -2.682609 -2.194771 -1.801863 -1.375864
%splus      .01     .025      .05       .1 
%splus 2.716377 2.230874 1.836867 1.410396
%splus
%splus   exp(4.160+(-1.802)*(.5451))=24.0
%splus
%splus   exp(4.160+(1.837)*(.5451))=174.4
%splus
\begin{eqnarray*}
[ \Tlower, \quad \Tupper ]& = &
	[ \exp(\muhat + \norquan_{{\log(T^{*})}_{(.05)}} \times \sigmahat), 
	\quad \exp(\muhat +\norquan_{{\log(T^{*})}_{(.95)}} \times \sigmahat) ]\\& = &
	\left[ \exp(4.160 +(-1.802) \times .5451), \quad 
\exp(4.160 + 1.837  \times .5451) \right ] \\ & = &
	\left[ 24.0, \quad 174.4 \right ].
\end{eqnarray*}
It is important to note that the upper prediction bound requires
some extrapolation given that there were only
15 failures in the sample of 23 of the bearings.  This upper bound
does not account for possible model error in the unobserved upper
tail of the failure-time distribution.
\end{example}

\begin{example}
\label{example:pivot.weib.pi}
{\bf Approximate prediction interval for predicting the life of a
ball bearing based on an approximate pivotal (Weibull
distribution).} Following the approach used in
Example~\ref{example:pivot.ln.pi}, a simulation for the
Weibull distribution to estimate the distribution of
$Z_{\log(\Tboot)}$ resulted in $\norquan_{{\log(T^{*})}_{(.05)}}
\approx -3.263$ and $\norquan_{{\log(T^{*})}_{(.95)}} \approx
1.260$. Thus a two-sided approximate 90\% prediction interval for
Weibull $T$ is
% splus>
% splus> ML Estimates
% splus>             MLE        se   t.ratio 95% lower 95% upper 
% splus>    mu 4.3343549 0.3422589 12.629955 3.6518992 4.9935294
% splus> sigma 0.4013151 0.1723784  2.051969 0.1360917 0.9193384
% splus> 
% splus> Weibull beta= 2.492173 eta= 1.4937 
% splus>  
%splus
%splus>+ > lzb80.gethenout.twosamp.p1.weib.out$pred.quant
%splus>       .01     .025       .05        .1 
%splus> -5.333163 -4.10383 -3.262781 -2.417168
%splus>      .01     .025      .05       .1 
%splus> 1.886323 1.537034 1.260342 0.932985
%splus  
%splus  exp(4.334 +(-3.263)* .4013) = 20.58491
%splus  exp(4.334 +(1.260)* .4013) = 126.4236
%splus  
\begin{eqnarray*}
[ \Tlower, \quad \Tupper ]& = &
	[ \exp(\muhat +\norquan_{{\log(T^{*})}_{(.05)}} \times \sigmahat), 
	\quad \exp(\muhat +\norquan_{{\log(T^{*})}_{(.95)}} \times \sigmahat) ]\\& = &
	[ \exp(4.334 +(-3.263) \times .4013), \quad 
\exp(4.334 + 1.260 \times .4013) ]\\ & = &
	[ 20.6, \quad 126.4].
\end{eqnarray*}
Note that, as we observed with the naive prediction intervals
(Examples~\ref{example:naive.ln.pi} and
\ref{example:naive.weibull.pi}), in comparison with the prediction
interval for the lognormal distribution in
Example~\ref{example:pivot.ln.pi}, the Weibull prediction interval has
a much smaller upper endpoint.
\end{example}

The (approximate) pivotal method can be extended directly to compute
prediction intervals for random variables from other
log-location-scale distributions.  It can also be extended to other
non-location-scale distributions by using a location estimate, such
as a central quantile of the distribution of $\log(\rv)$ in place of
$\muhat$ and an estimate of the standard deviation of
$[\log(\rv)-\muhat]$ in place of $\sigmahat$.

%--------------------------------------------------------------------------
\section{Prediction in Simple Cases}
%--------------------------------------------------------------------------
\label{section:pred.simple}
This section describes some simple special case prediction interval
procedures. The methods are based on pivotal quantities
that are related to the pivotal quantity methods
described in Section~\ref{section:tyep2.pivot.pred.method}. For the
simple cases presented here, the distribution of the pivotal
quantities have well-known distributions (Student's $t$ and
the Snedecor F distributions) for which tables of quantiles are
readily available.

\subsection{Complete samples from a lognormal distribution}
Suppose that $\realrv_{1}, \dots, \realrv_{n}$ is a complete sample
from a $\LOGNOR(\mu, \sigma)$ distribution and $\rv$ is an
independent new observation from the same distribution. Then it can
be shown that $\sqrt{(n-1)/(n+1)} \times Z_{\log(\rv)}$ has a
Student's $t$ distribution with $n-1$ degrees of freedom.  From this
is follows that
\begin{eqnarray*}
\Pr \left [\Tquant_{(\alpha/2;n-1)} <
\sqrt{\frac{n-1} {n+1}}
\times \frac{	 \log(\rv) -\muhat } 
{ \sigmahat }\le\Tquant_{(1-\alpha/2;n-1)} 
\right ] &=& \\
\Pr \left [\Tquant_{(\alpha/2;n-1)} <
\sqrt{\frac{n} {n+1}}
\times \frac{	 \log(\rv) -\muhat } 
{s }\le\Tquant_{(1-\alpha/2;n-1)} 
\right ]&=&1-\alpha\\
\end{eqnarray*}
where $s=\sqrt{n/(n-1)} \times \sigmahat$ is the sample 
standard deviation of the logarithms of the 
observed failure times and $\Tquant_{(p;\nu)}$ is the $p$
quantile of the Student's $t$ distribution with $\nu$ degrees of freedom.
This leads to an exact $100(1-\alpha)\%$ prediction interval for a
new independent observation given by
\begin{displaymath}
[\Tlower, \quad \Tupper]
=\left [
\exp \left(
\muhat-
\omega
\times s
\right)
,
\quad
\exp \left(
\muhat +
\omega \times s \right) \right ]
\end{displaymath}
where $\omega = \Tquant_{(1-\alpha/2;n-1)}
     \times 
\sqrt{1+1/ n }$\,\,.
This prediction interval is equivalent to the prediction interval
for a new independent observation for complete data
given in other books (e.g., Hahn and Meeker~1991, page~61).

The exact prediction interval $[\Tlower, \quad \Tupper]$ is wider than the 
naive prediction interval obtained from using the ML estimates
of the quantiles of $T$
\begin{displaymath}
[\rvquanhat_{\alpha/2},\quad\rvquanhat_{1-\alpha/2}]
=
\left [
\exp \left \{
\muhat+
\Phi^{-1}_{\nor}(\alpha/2)
\times 
\sigmahat
     \right \},
\quad
\exp \left \{
\muhat+
\Phi^{-1}_{\nor}(1-\alpha/2)
\times 
\sigmahat
     \right \}
\right ].
\end{displaymath}
When $n$ is large (say $n>30$), however, the differences are
negligible because for $0<p<1$,
\begin{displaymath}
\Tquant_{(p;n-1)}
 \times
\sqrt{1+ \frac{ 1 } { n } } \times s
\approx 
\Phi^{-1}_{\nor}(p) \times \sigmahat.
\end{displaymath}
\begin{example}
\label{example:prediction.complete.data.lognormal}
{\bf Prediction interval for a new observation from a lognormal
distribution.}  Refer to Examples ~\ref{example:bkfat10.bisa.igau}.
Suppose that the analysts wanted a 95\% prediction interval to
contain the time to fracture of a specimen of the same type to be
tested in the future. Based on a sample of size $n=63$ specimens,
the ML estimate of the parameters for the lognormal distribution are
$\muhat=4.722$, $\sigmahat=1.0255$ and $s= \sqrt{63/62} \times
1.0255=1.034$.
%splus>  
%splus>  sqrt(63/(62))*1.0255= 1.033737
%splus>  
Then, an exact $90\%$ prediction interval for a 
new observations is
\begin{eqnarray*}
[\Tlower, \quad \Tupper]
&=& [\exp(\muhat - \omega \times  s ), 
\quad
\exp(\muhat + \omega  \times s) ]\\
&=& [ \exp(4.722  -1.683 \times  1.034 ), 
\quad
\exp( 4.722 + 1.683  \times 1.034) ]\\
&=&[19.72, \quad 640.0].
\end{eqnarray*}
where $\omega=\sqrt{1+(1/63 )} \times 1.669804 = 1.683 $.
%splus>>sqrt(1+(1/63 ))*( -1.669804)
%splus>>> [1] -1.683004
%splus>>>nexp(4.721612  +1.683004 *( 1.033737 ))
%splus>>>[1] 639.9535
%splus
%splus>>  exp(4.721612 +  -1.683004 *( 1.033737 ))
%splus
%splus>[1] [1] 19.72385
%> n<-63
%> muhat<-4.721612
%> sigmahat<-1.025504
%> alpha<-.10
%> lower<-muhat+sqrt((n+1)/(n-1))*sigmahat*qt(alpha/2,df=n-1)
%> upper<-muhat+sqrt((n+1)/(n-1))*sigmahat*qt(1-alpha/2,df=n-1)
%> lower
%[1] 2.981821
%> upper
%[1] 6.461403
%> Upper<-exp(upper)
%> Lower<-exp(lower)
%> Upper
%[1] 639.9582
%> Lower
%[1] 19.7237
Thus we are $90\%$ confident that this interval
will contain the fatigue life of the specimen.
\end{example}
\subsection{Complete or Type~II censored samples from an
exponential distribution} If $\realrv_{1}, \dots, \realrv_{r}$ is a
Type~II ($r<n$) censored sample or a complete sample ($r=n$) from an
$\EXP(\theta)$ distribution, $\thetahat$ is the ML estimator from
these data, and if $\rv$ is another, future independent observation
from the same distribution, then $\rv/\thetahat$ has an F
distribution with $(2,2r)$ degrees of freedom.  From this, it follows
that
\begin{displaymath}
\Pr
\left [
\Fquant_{(\alpha/2; 2, 2r)} <
\rv / \thetahat \le 
\Fquant_{(1-\alpha/2; 2, 2r)}
\right ]=1-\alpha
\end{displaymath}
where $\Fquant_{(p; \nu_{1} , \nu_{2})}$ is the $p$ quantile of the
F distribution with $(\nu_{1},\nu_{2})$
degrees of freedom.  This leads to an exact $100(1-\alpha)\%$
prediction interval for a new observation given by
\begin{displaymath}
[\Tlower, \quad \Tupper]
=
\left [
\Fquant_{(\alpha/2; 2, 2r)} \times \thetahat, 
\quad
\Fquant_{(1-\alpha/2; 2, 2r)} \times \thetahat 
\right ]
\end{displaymath}
where the F distribution $p$ quantile with 2 numerator degrees of
freedom can be obtained from $\Fquant_{(p;2;2r)}=r \times \left \{
\exp[ -(1/r) \log(1-p)]-1
                       \right \}$.
This prediction interval is wider than the
naive prediction interval computed as ML estimates
of quantiles of the distribution of $T$
\begin{displaymath}
[\rvquanhat_{\alpha/2},\quad\rvquanhat_{1-\alpha/2}]
=
\left [
\{-\log(1-\alpha/2)\} \times \thetahat, \quad
 \{-\log(\alpha/2)\} \times \thetahat
\right ]
\end{displaymath}
but when $r$
is large 
the differences between the two intervals are negligible because  
for $0<p<1$, and large $r$, $\Fquant_{(p;2,2r)} \approx -\log(1-p)$.
\begin{example}
\label{example:prediction.typeiidata.exponential}
{\bf Prediction interval for the lifetime of an insulation
specimen.}  Refer to Example~\ref{example:insul.type2} where $n=25$
insulation specimens were tested until $r=15$ failures had been
observed. The ML estimate of the exponential mean $\expmean$ is
$\thetahat=63.392$ hours.  Another specimen is to be tested. An
exact $90\%$ prediction interval for the new observations is
\begin{displaymath}
[\Tlower, \quad \Tupper]=[\Fquant_{(.05;2,30)} \times 63.392, \quad
\Fquant_{(.95;2,30)}\times63.392]=[3.257, \quad 210.197]. 
\end{displaymath}
Thus we are $90\%$ confident that this interval will contain the
lifetime of the specimen to be tested.
\end{example}
%> thetahat<-63.392
%> alpha<-.1
%> r<-15
%> Lower<-thetahat*qf(alpha/2,df1=2,df2=2*r)
%> Upper<-thetahat*qf(1-alpha/2,df1=2,df2=2*r)
%> Lower
%[1] 3.25715
%> Upper
%[1] 210.1971

%--------------------------------------------------------------------------
\section{Calibrating Naive Statistical Prediction Bounds}
%--------------------------------------------------------------------------
\label{section:calibrate.pred}
Cox~(1975) suggested a large-sample approximate method, based on
maximum likelihood estimates, that can be used to calibrate or
correct a naive prediction interval.  The basic idea of this
approach is to calibrate the naive one-sided prediction bound by
evaluating $\cp[\pinterval(1-\alpha) ;\thetavec]$ at $\thetavechat$
and finding a calibration value $1-\alphacal$ such that for a
one-sided {\em lower} prediction bound for $T$,
\begin{equation}
\label{equation:calibration.lower.def}
\cp[\pinterval(1-\alphacal) ;\thetavechat] =
\Pr \left(\Tlower \leq \rv \leq \infty ; \thetavechat \right)
= \Pr \left(\rvquanhat_{\alphacal} \leq \rv \leq \infty ;
\thetavechat \right) = 1-\alphanom.
\end{equation}
Calibration for a one-sided {\em upper} prediction bound on $T$
(described at the end of
Section~\ref{section:calibration.from.simulation}) is similar.  For
a two-sided prediction interval, the calibration is done separately such that
the probability is $\alpha/2$ in each tail.  In problems where
$\cp[\pinterval(1-\alpha);\thetavec]$ does not depend on
$\thetavec$, the calibrated $\pinterval(1-\alphacal)$ provides an exact
prediction interval.

Although it is sometimes possible to do analytical calibration,
operationally, the analytical approach is intractable except in the
simplest of situations, where alternative, simpler methods exist
(e.g., the methods in Section~\ref{section:pred.simple}).

%--------------------------------------------------------------------------
\subsection{Calibration by simulation of the sampling/prediction process}
\label{section:calibration.from.simulation}
Modern computing capabilities make it easy to use Monte Carlo
methods to evaluate, numerically, quantities like
(\ref{equation:calibration.lower.def}), even for complicated
statistical models.  In particular, under the assumed model we can
use ML estimates $\thetavechat$ to simulate both the sampling {\em
and} prediction process a large number $B$ (e.g., $B=$50,000 or
$B=$100,000) of times.  Although $B=2000$ or so is often suggested
for simulation-based confidence intervals, larger values of $B$ are
required for prediction problems due to the added variability of the
single future observation.

Conceptually, (\ref{equation:calibration.lower.def}) can be evaluated as
follows:
\begin{enumerate}
\item
Choose a value of $1-\alpha$, say $1-\alpha_{0}$.
\item
Simulate $\DATA^{*}_{j}$ from the assumed model with
parameter values equal to the ML estimates $\thetavechat$ [i.e., from
$F(t;\thetavechat )$]. Use the 
sampling procedures and censoring that mimics the original experiment.
\item
Compute simulation ML estimates $\thetavechatstar_{j}$ from $\DATA^{*}_{j}$.
\item
Compute the naive 100$(1-\alpha_{0})\%$ lower prediction bound
$\Tlower_{j}^{*}$ from the simulated $\DATA^{*}_{j}$. Compare
$\Tlower_{j}^{*}$ with
an independent $T_{j}^{*}$ simulated from
$F(t;\thetavechat )$ to see if $T_{j}^{*}>\Tlower_{j}^{*}$.
\item 
Repeat steps 2 to 4 for $j=1,2,\ldots,B$.  The proportion of the
$B$ trials having $T_{j}^{*} > \Tlower_{j}^{*}$ gives the Monte
Carlo evaluation of $\cp\left [ \pinterval(1-\alpha_{0}); \thetavec
\right ]$ at $\thetavechat$, which we denote by
$\cp^{*}[\pinterval(1-\alpha_{0});\thetavechat]$.
\item
Repeat steps 2 to 5 for different values of $1-\alpha_{0}$.
\item
Find $1-\alphacal$ such that $\cp^{*}[ \pinterval(1-\alphacal)
;\thetavechat]= 1-\alphanom$.
\end{enumerate}
The difference between $\cp
\left [ \pinterval(1-\alpha_{0}); \thetavechat \right ]$
and $\cp^{*}\left [\pinterval(1-\alpha_{0}) ;\thetavechat\right ]$
is due to Monte Carlo error and can be made arbitrarily small by
choosing a sufficiently large value of $B$. To avoid
cumbersome notation we will use $\cp
\left [ \pinterval(1-\alpha_{0}); \thetavechat \right ]$ even when the
evaluation is done with simulation.

Operationally, for a log-location-scale distribution where $\thetavec=
(\mu,\sigma)$, the $\cp[\pinterval(1-\alpha) ;\thetavechat]$ function
can be evaluated more directly by using the following procedure:
\begin{enumerate}
\item
Use simulation to compute $B$ realizations of the pivotal-like statistic
$Z_{\log(T^{*})}$, as described in
Section~\ref{section:pivot.pred.method}.
\item
The empirical distribution of the observed values of the random
variable $P=1-\Phi[Z_{\log(T^{*})}]$ provides a Monte Carlo
evaluation of lower prediction bound $\cp[\pinterval(1-\alpha)
;\thetavechat]$ in (\ref{equation:calibration.lower.def}). In particular,
for a lower prediction bound, $1-\alphacal$ is the $1-\alpha$
quantile of the distribution of the random variable $P= 1 -
\Phi(Z_{\log(T^{*})})$.
\end{enumerate}  

The naive one-sided upper prediction bound for $T$ is calibrated by
finding $1-\alphacal$ such that
\begin{equation}
\label{equation:calibration.upper.def}
\cp[\pinterval(1-\alphacal) ;\thetavechat] =
\Pr \left(0 \leq \rv \leq \Tupper ; \thetavechat \right)
= \Pr \left(0 \leq \rv \leq \rvquanhat_{1 - \alphacal}  ; \thetavechat \right) 
=  1-\alphanom.
\end{equation}
Then a Monte Carlo evaluation of the upper prediction bound
$\cp[\pinterval(1-\alpha) ;\thetavechat]$ can be obtained from the
empirical distribution of the observed values of the random variable
$P=\Phi[Z_{\log(T^{*})}]$. In particular, for an upper prediction
bound, $1-\alphacal$ is the $1-\alpha$ quantile of the distribution
of the random variable $P=\Phi(Z_{\log(T^{*})})$.

Escobar and Meeker~(1998a)
provide justification for this procedure and demonstrate the
equivalence of the calibration method and the approximate pivotal method
from Section~\ref{section:pivot.pred.method}.   For 
predicting random variables with distributions that are
not log-location-scale, the approach is similar, as will be
illustrated in Sections~\ref{section:pred.single.group} and
\ref{section:pred.multiple.groups}.

\subsection{Calibration by averaging conditional coverage probabilities}
\label{section:calibration.from.prob.aver}
As suggested by Mee and Kushary (1994), it can be much more
efficient, computationally, to obtain the needed calibration curves
for (\ref{equation:calibration.lower.def}) and
(\ref{equation:calibration.upper.def}) by simulating conditional
coverage probabilities like those in (\ref{equation:cond.coverage})
and averaging these to estimate the expectation in
(\ref{equation:uncond.coverage}).  The procedure is similar to the
one in Section~\ref{section:calibration.from.simulation}, replacing
steps 4 and 5 with
\begin{enumerate}
\setcounter{enumi}{3}
\item
For each simulated sample, compute the {\em naive} $100(1-\alpha)$\%
upper and lower prediction bounds $\Tlower$ and $\Tupper$, respectively. 
For a log-location-scale distribution, $\Tlower=\exp(\muhatboot +
\Phi^{-1}(\alpha) \times \sigmahatboot)$ and $\Tupper=\exp(\muhatboot +
\Phi^{-1}(1-\alpha) \times \sigmahatboot)$.
\item
A Monte Carlo evaluation of the unconditional coverage probability
is obtained from the average of the simulated conditional coverage
probabilities $\cp[\pinterval(1-\alpha_{0})
;\thetavechat]=\sum_{j=1}^{B}P_{j}/B$ where
\begin{enumerate}
\item
For the upper prediction bound calibration
$P_{j}= \Pr(T \leq \Tupper)$.  For a
log-location-scale distribution,
$P_{j}=\Phi[(\log(\Tupper)-\muhat)/\sigmahat]$.
\item
For the lower prediction bound calibration, compute the conditional
coverage probability $P_{j}= \Pr(T \geq \Tlower)$.  For a
log-location-scale distribution,
$P_{j}=1 - \Phi[(\log(\Tlower)-\muhat)/\sigmahat]$.
\end{enumerate}
\end{enumerate}

To obtain the entire calibration curves, one would need to compute
$\cp[\pinterval(1-\alpha_{0}) ;\thetavechat]$ for a large number of
different values of $1-\alpha_{0}$ between 0 and 1. Operationally,
to compute a one-sided prediction bound one needs only to find the
appropriate $1-\alphacal$ value.  The
$\cp[\pinterval(1-\alphacal) ;\thetavechat]$ function is
continuous and monotone increasing in $1-\alphacal$, so the
appropriate calibration value can be found by using a simple
root-finding method.

The procedure for Monte Carlo evaluation of the coverage probability
in Section~\ref{section:calibration.from.simulation} utilized the
observed proportion of correct prediction intervals. The advantage
of the probability-averaging procedure is that it does not include a
simulation of the future random variable in the evaluation.  Thus
the procedure requires fewer Monte Carlo samples to get the same
level of accuracy.

For either evaluation method, it is a simple matter to use standard
sampling methods to quantify Monte Carlo error. For example, the
standard error of the Monte Carlo evaluation of
$\cp[\pinterval(1-\alpha_{0}) ;\thetavechat]$ for any particular
$1-\alpha_{0}$ is
\begin{displaymath}
 \sqrt{ \sum_{j=1}^{B} \frac{ (P_{j} -
\cp[\pinterval(1-\alpha_{0});\thetavechat])^{2} }{B(B-1)}}.
\end{displaymath}
For the probability-averaging procedure, the variability in the
$P_{j}$ values is related to the variability in $\thetavechatboot$. The
probability-averaging procedure can provide substantial savings in
computing time.

\begin{example}
{\bf Calibration of the naive prediction interval for a future
lognormal bearing life.}
Returning to Example~\ref{example:pivot.ln.pi},
Figure~\ref{figure:lzb5.lognor.pivot.hist.ps} is a histogram
of the 100,000 simulated values of $Z_{\log(T^{*})}$.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/lzb5.lognor.phi.hn.hist.ps}
\caption{Histogram of 100,000 simulated $\Phi_{\nor}[Z_{\log(\rvhat^{*})}]$
values, based on the bearing life test data censored after 80
million cycles.}
\label{figure:lzb5.lognor.phi.hn.hist.ps}
\end{figure}
%-------------------------------------------------------------------
Figure~\ref{figure:lzb5.lognor.phi.hn.hist.ps} is a corresponding
histogram of the $B$=100,000 simulated values of
$\Phi_{\nor}[Z_{\log(T^{*})}]$.  Although the lower and upper
$\cp[\pinterval(1-\alpha) ;\thetavechat]$ calibration functions in
Figure~\ref{figure:lzb5.predict.calibration.ps} could have been
computed from the empirical cdfs of the simulated
$1-\Phi_{\nor}[Z_{\log(T^{*})}]$ and $\Phi_{\nor}[Z_{\log(T^{*})}]$
values, respectively, the conditional probability averaging methods
with $B$=100,000 was used instead. The simulation sample size of
$B$=100,000 was chosen to be large enough to assure that the printed
calibration values are correct to the number of digits shown.
Because $B$ is so large, the differences between the calibration
methods were small. With $B$=10,000, the differences were more
pronounced, but $B$=10,000 would, for practical purposes, be large
enough for the conditional probability averaging method.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/lzb5.predict.calibration.ps}
\caption{Calibration functions for predicting 
the failure time of a future bearing based on a lognormal
distribution and life test data censored after 80 million cycles.}
\label{figure:lzb5.predict.calibration.ps}
\end{figure}
%-------------------------------------------------------------------
%splus  > pnorm(1.825462)
%splus  [1] 0.9618396
%splus  > pnorm(1.825462)
%splus  [1] 0.9660343
Using the calibration method, a naive 96.4\% lower prediction
bound for $T$ provides a calibrated approximate 95\% lower
prediction bound for $T$.  Also, a naive 96.7\% upper prediction
bound for $T$ provides a 95\% calibrated upper prediction bound for
$T$. Comparing to the results in Example~\ref{example:pivot.ln.pi},
numerically $1-\alphacal=.964 \approx
1-\Phi_{\nor}(-1.802)$ for an approximate 95\% lower
prediction bound and $1-\alphacal= .967 \approx
\Phi_{\nor}(1.837)$ for an approximate 95\% upper prediction
bound. Differences are due to Monte Carlo error in the pivotal
method. Thus substituting $\Phi^{-1}_{\nor}(1-.964)$ for
$\Phi^{-1}_{\nor}(.05)$ and $\Phi^{-1}_{\nor}(.967)$ for
$\Phi^{-1}_{\nor}(.95)$ in the naive interval formula
(\ref{equation:naive.pi}) will result in a calibrated interval from
a procedure that is equivalent to the pivotal method, but with
somewhat less Monte Carlo error for the same $B$.
\end{example}

%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
\section{Prediction of Future Failures from a Single Group
of Units in the Field}
\label{section:pred.single.group}
Consider the situation where $n$ units are placed into service
at approximately one point in time.  Failures are reported until
$\censortime$, another point in time where the available data are to
be analyzed. Suppose that $F(t;\thetavec)$ is used to describe the
failure time distribution and that $r>0$ units have failed in the
interval $(0,\censortime)$. Thus there are $n-r$ unfailed units at
$\censortime$.

A common problem is the need to predict the number of additional
failures $K$ that will be reported between $\censortime$ and
$\predicttime$, where $\predicttime > \censortime$.  In addition, it
is sometimes necessary, to quantify the uncertainty in such a
prediction. The upper prediction bound for $K$ is usually of
particular interest.

Conditional on the number of failures $r$, $K$ follows a
$\BINOMIAL(n-r, \rho)$ distribution where
\begin{equation}
\label{equation:rhohat}
	\rho=\frac{\Pr(\censortime < \rv \leq \realrv_{w})}
		  {\Pr( \rv > \censortime )}
	    =\frac{F(\realrv_{w};\thetavec)-F(\censortime;\thetavec)}
		  {1 - F(\censortime;\thetavec)}
\end{equation}
is the conditional probability of failing in the interval
$(\censortime, \predicttime)$, given that a unit survived until
$\censortime$. The corresponding binomial cdf is $\Pr(K \leq
k)=\BINCDF(k,n-r,\rho)$.

The naive $100(1-\alpha)\%$ upper prediction bound for $K$ is
$\kupper(1-\alpha)=\Khat_{1-\alpha}$. This upper prediction bound is
computed as the smallest integer $k$ such that $\BINCDF(k, n-r,
\rhohat) \ge 1-\alpha$.  The ML estimate $\rhohat$ is obtained by
evaluating (\ref{equation:rhohat}) at ML estimate $\thetavechat$.
This upper prediction bound can be calibrated by finding
$1-\alphauppercal$ such that
\begin{equation}
\label{equation:bin.upper.calibration}
\cp[\pinterval(1-\alphauppercal); \thetavechat] =
\Pr\left [K \le \kupper(1 - \alphauppercal) \right] =  1- \alpha.
\end{equation}
Then the $100(1-\alpha)\%$ calibrated upper prediction bound would be
$\kupper(1-\alphacal)=\Khat_{1-\alphauppercal}$.

The naive $100(1-\alpha)\%$ lower prediction bound for $K$ is
$\klower(1-\alpha)=\Khat_{\alpha}$.  This lower prediction bound is
computed as the largest integer $k$ such that
$\BINCDF(k,n-r,\rhohat) < \alpha$.  This lower prediction bound can
be calibrated by finding $1-\alphacal$ such that
\begin{equation}
\label{equation:bin.lower.calibration}
\cp[\pinterval(1-\alphalowercal); \thetavechat]=
\Pr\left [K \geq \klower(1-\alphalowercal)\right] =  1 - \alpha
\end{equation}
and the calibrated lower prediction bound would be 
$\klower(1-\alphalowercal)=\Khat_{\alphalowercal}$.

The needed calibration curves for
(\ref{equation:bin.upper.calibration}) and
(\ref{equation:bin.lower.calibration}) can be found by averaging
conditional coverage probabilities obtained from Monte Carlo
simulation by using the following procedure that is similar to the
one in Section~\ref{section:calibration.from.prob.aver}.
\begin{enumerate}
\item
Choose a value of $1-\alpha$, say $1-\alpha_{0}$.
\item
Generate simulated samples of size $n$, say $\DATA^{*}_{j}$ for $j=1,
\dots, B$ from the assumed model with parameter values equal to
$\thetavechat$ and the same censoring scheme as in the original sample
(leading to the same censoring pattern, except for the variability in
$n-r$).
\item
The $j$th simulated sample $\DATA^{*}_{j}$ provides $n-r^{*}_{j}$,
$\thetavechatboot_{j}$, and $\rhohat^{*}_{j}$.
\item
Use the cdf $\BINCDF(k;n-r_{j}^{*},\rhohat_{j}^{*})$ to compute the
upper and lower {\em naive} prediction bounds
$\kupper(1-\alpha_{0})^{*}_{j}$ and $\klower(1-\alpha_{0})^{*}_{j}$.
\item
For the upper prediction bound calibration, compute the conditional
coverage probability $P_{j}=\BINCDF\left
[\kupper(1-\alpha_{0})^{*}_{j}; n-r_{j}^{*},\rhohat \right ]$. A Monte
Carlo evaluation of the unconditional coverage probability is
$\cp[\pinterval(1-\alpha_{0})
;\thetavechat]=\sum_{j=1}^{B}P_{j}/B$.  
\item
For the lower prediction bound calibration, compute the conditional
coverage probability $P_{j}=1-\BINCDF \left
[\klower(1-\alpha_{0})^{*}_{j}-1; n-r_{j}^{*},\rhohat \right ]$. A
Monte Carlo evaluation of the unconditional coverage probability is
$\cp[\pinterval(1-\alpha_{0})
;\thetavechat]=\sum_{j=1}^{B}P_{j}/B$.
\end{enumerate}
The justification for this procedure is given in 
the Appendix of Escobar and Meeker~(1998a).


\begin{example}
{\bf Prediction interval to contain the number of future Product-A
failures.} During one month, $n=$10,000 units of Product-A (the actual
name of the product is not being used to protect proprietary
information) were put into service.  After 48 months, 80 failures had
been reported.  Management requested a point prediction and an upper
prediction bound on the number of the remaining $ n-r=10000-80=9920$
units that will fail during the next 12 months (i.e., between
48 and 60 months of age).  The available data suggested a Weibull
failure-time distribution and the ML estimates are $\alphahat=1152$
and $\betahat=1.518$. From these,
\begin{displaymath} 	
\rhohat= \frac{\Fhat(60)-\Fhat(48)} {1 -\Fhat(48)}=.003233.  	
\end{displaymath} 
Figure~\ref{figure:wund.pred.exfig.ps} shows the point prediction,
the naive 95\% upper prediction bound, and the calibrated approximate
95\% upper prediction bound.
%-------------------------------------------------------------------
\begin{figure}
\xfigbookfiguresize{\figurehome/wund.pred.exfig.ps}{3in}
\caption{Prediction of the future number failing in the Product-A population.}
\label{figure:wund.pred.exfig.ps}
\end{figure}
%-------------------------------------------------------------------
The point prediction for the number failing between 48 and 60 months is 
$\Khat = (n-r) \times \rhohat = 9920 \times .003233=32.07$.
%splus 
%splus Example 3
%splus 
%splus   > 10000-80
%splus   [1] 9920
%splus 
%splus   (pweibull(60/1152,1.518)-pweibull(48/1152,1.518))/
%splus		(1-pweibull(48/1152,1.518)) 
%splus 	[1] 0.003233117
%splus 	qbinom(.95,9920,0.003233117)
%splus 	pbinom(42,9920,0.003233117)
%splus 	[1] 0.9627936                     <----this is the one
%splus 	
%splus 	pbinom(41,9920,0.003233117)
%splus 	[1] 0.9476667        
%splusFigure~\ref{figure:wund.pred.exfig.ps}
The naive  $95\%$  upper prediction bound on $K$ is 
$\kupper(.95)=\Khat_{.95}=42$, the 
smallest integer $k$ such that $\BINCDF(k,9920,.003233) \geq .95$.
The calibration curve shown in 
Figure~\ref{figure:wund1.predict.calibration.ps}
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/wund1.predict.calibration.ps}
\caption{Calibration functions
for upper and lower prediction bounds on 
the number of field failures in the next year for the Product-A population.}
\label{figure:wund1.predict.calibration.ps}
\end{figure}
%-------------------------------------------------------------------
gives, for the upper prediction bound,
$\cp[\pinterval(.986);\thetavechat]=.95$.
Thus the calibrated approximate $95\%$ upper prediction bound on $K$ is 
$\kupper(.986)=\Khat_{.986}=45$, the  smallest integer $k$ such that
$\BINCDF(k,9920,.003233) \geq .986$.
%splus
%splus pbinom(41:46,9920,.003233)
%splus [1] 0.9476897 0.9628111 0.9740780 0.9822814 0.9881208 0.9921867
%splus
%splus
%splus pbinom(20:24,9920,.003233)
%splus [1] 0.01535067 0.02511125 0.03935605 0.05923933 0.08583386
%splus
%splus
The naive  $95\%$  lower prediction bound on $K$ is 
$\klower(.95)=\Khat_{.05}=22$, the 
largest integer $k$ such that $\BINCDF(k,9920,.003233) < .05$.
The calibration curve shown in
Figure~\ref{figure:wund1.predict.calibration.ps}
gives, for the lower prediction bound,
$\cp[\pinterval(.981);\thetavechat]=.95$.
Thus the calibrated approximate $95\%$ lower prediction bound on $K$ is 
$\klower(.981)=\Khat_{.019}=20$, the  largest integer $k$ such that
$\BINCDF(k,9920,.003233) < 1-.981 = .019$.
\end{example}

%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
\section{Prediction of Future Failures from Multiple Groups
of Units with Staggered Entry into the Field }
\label{section:pred.multiple.groups}
This section describes a generalization of the prediction problem in
Section~\ref{section:pred.single.group}. In many applications the
units in the population of interest entered service over a period of
time. This is called staggered entry.  As in
Section~\ref{section:pred.single.group}, the need is to use early
field-failure data to construct a prediction interval for the
number of future failures in some interval of calendar time, where the
amount of previous operating time differs from group to group. This prediction
problem is illustrated in Figure~\ref{figure:staggeredentryfig.ps}.
%-------------------------------------------------------------------
\begin{figure}
\xfigbookfigure{\figurehome/staggeredentryfig.ps}
\caption{Illustration of staggered entry prediction.}
\label{figure:staggeredentryfig.ps}
\end{figure}
%-------------------------------------------------------------------
Staggered entry failure-time data are multiply censored because of
the differences in operating time.  The prediction problem can be
viewed as predicting the number of the additional failures across
the $s$ groups during a specified period of real time. The problem
is more complicated than the prediction procedure given in
Section~\ref{section:pred.single.group} because the age of the
units,
the failure
probabilities, and number of units at risk to failure differ from group
to group.  For group $i$, $n_{i}$ units are followed for a period of
length $\realrv_{ci}$ and the first $r_{i}$ failures were observed
at times $\realrv_{(i1)}< \dots <
\realrv_{(ir_{i})}$, $i=1,\dots, s.$

Conditional on $n_{i}-r_{i}$, the number of additional failures $K_{i}$
from group $i$ during interval $(\realrv_{ci}, \realrv_{wi})$ (where
$\realrv_{wi}=\realrv_{ci}+\Delta \realrv$) is distributed
$\BINOMIAL(n_{i}-r_{i}, \rho_{i})$ with
\begin{equation}
\label{equation:rho.stagger}
	\rho_{i} =\frac{\Pr(\realrv_{ci} < \rv \leq \realrv_{wi})}
{\Pr( \rv >\realrv_{ci} )}
=\frac{F(\realrv_{wi};\thetavec)-F(\realrv_{ci};\thetavec)}
{1 - F(\realrv_{ci};\thetavec)}.
\end{equation}


Let $K=\sum_{i=1}^{s} K_{i}$ be the total number of additional
failures over $\Delta\realrv$.  Conditional on the $\DATA$ (and the
fixed censoring times) $K$ has a distribution that can be described by the
sum of $s$ independent but nonidentically distributed binomial random
variables with cdf denoted by $\Pr(K \leq k)=\SBINCDF(k;\nmrvec,\rhovec)$
where $\nmrvec =(n_{1}-r_{1},\dots,n_{s}-r_{s})$ and
$\rhovec=(\rho_{1},\dots,\rho_{s})$.  The appendix of Escobar and
Meeker~(1998a) describes methods for evaluating
$\SBINCDF(k;\nmrvec,\rhovec)$.

A naive $100(1-\alpha)\%$ upper prediction bound 
$\kupper(1-\alpha)=\Khat_{1-\alpha}$
is computed as the smallest integer $k$ such that 
$\SBINCDF(k,\nmrvecboot,\rhovechatboot) \ge 1-\alpha$. This
upper prediction bound can be calibrated by finding
$1-\alphauppercal$ such that
\begin{displaymath}
\cp[\pinterval(1-\alphauppercal);\thetavechat] =
\Pr[K \leq \kupper(1-\alphauppercal)] = 1-\alpha.
\end{displaymath}
A naive $100(1-\alpha)\%$ lower prediction bound 
$\klower(1-\alpha)=\Khat_{\alpha}$ is computed as the 
largest integer $k$ such that 
$\SBINCDF(k,\nmrvecboot,\rhovechatboot) < \alpha$.
This lower prediction bound can be calibrated by finding
$1-\alphalowercal$ such that
\begin{displaymath}
\cp[\pinterval(1-\alphalowercal); \thetavechat] =
\Pr\left [K \geq \klower(1-\alphalowercal)
    \right ] = 1- \alpha.
\end{displaymath}

To calibrate these one-sided prediction bounds, one can use the same
procedure outlined in Section~\ref{section:pred.single.group},
replacing $\BINCDF(k;n-r,\rhohat)$ with 
$\SBINCDF(k,\nmrvec,\rhovechat)$.

\begin{example}
{\bf Prediction interval to contain the number of future bearing cage
failures.} 
Abernethy, Breneman, Medlin, and Reinman~(1983, pages 43-47) describe
the analysis of bearing cage failure data.  Groups of bearing cages,
installed in a larger system, were introduced into service at
different points in time (staggered entry).  Failures had occurred at
230, 334, 423, 990, 1009, and 1510 hours of service. There were 1697
other units that had accumulated various service times without
failing. Figure~\ref{figure:bcage.weib.mleprobplot.ps}
is a Weibull probability plot for the data.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/bcage.weib.mleprobplot.ps}
\caption{Weibull probability plot of the bearing cage data
showing the ML estimate of $F(t)$ (solid line) and pointwise 95\%
confidence intervals (dotted line).}
\label{figure:bcage.weib.mleprobplot.ps}
\end{figure}
%-------------------------------------------------------------------
Because of an unexpectedly large
number failures in early life, the
bearing cage was to be redesigned. It would, however, be some time
before the design could be completed, manufacturing started, and the
existing units replaced. The analysts wanted to use the initial data
to predict the number of additional failures that could be expected
from the population of units currently in service, during the next
year, assuming that each unit will see $\Delta  = 300$ hours of service during
the year.  Abernethy et al.~(1983) computed point predictions.  We will
extend their results to compute a prediction interval to quantify
uncertainty.


Table~\ref{table:bcage.risk.table} is a future-failure risk
analysis.  This table gives, for each of the groups of units that
had been put into service, the number of units installed,
accumulated service times, number of observed failures, estimated
conditional probability of failure, and the estimated expected
number failing in the 300-hour period. The sum of the estimated
expected numbers failing is 5.057, providing a point prediction for
the number of failures in the 300-hour period.
\begin{table}
\caption{Bearing Cage Data and Future-Failure
Risk Analysis for the Next Year (300 Hours of Service per Unit).}
\centering\small
\begin{tabular} {rrrrrrrr} 
\\ 
\hline 
 & Group & \multicolumn{1} {c} {Hours in} &  & \multicolumn{1} {c}
{Failed} & \multicolumn{1} {c} {At Risk} &   &  \\ 
 & $i$ & \multicolumn{1} {c} {Service} & \multicolumn{1} {c} {$n_{i}$} & $r_{i}$ & \multicolumn{1} {c} {$n_{i} - r_{i}$}  & \multicolumn{1} {c} {$\rhohat_{i}$}
& $(n_{i} - r_{i}) \times \rhohat_{i} $  \\  \hline 
 & 1 & 50  & 288 & 0 & 288 & .000763 & .2196 \\  
 & 2 & 150 & 148 & 0 & 148 & .001158 & .1714 \\  
 & 3 & 250 & 125 & 1 & 124 & .001558 & .1932 \\  
 & 4 & 350 & 112 & 1 & 111 & .001962 & .2178 \\  
 & 5 & 450 & 107 & 1 & 106 & .002369 & .2511 \\  
 & 6 & 550 & 99 & 0 & 99 & .002778 & .2750 \\  
 & 7 & 650 & 110 & 0 & 110 & .003189 & .3508 \\  
 & 8 & 750 & 114 & 0 & 114 & .003602 & .4106 \\  
 & 9 & 850 & 119 & 0 & 119 & .004016 & .4779 \\  
 & 10 & 950 & 128 & 0 & 128 & .004432 & .5673 \\  
 & 11 & 1050 & 124 & 2 & 122 & .004848 & .5915 \\  
 & 12 & 1150 & 93 & 0 & 93 & .005266 & .4898 \\  
 & 13 & 1250 & 47 & 0 & 47 & .005685 & .2672 \\  
 & 14 & 1350 & 41 & 0 & 41 & .006105 & .2503 \\  
 & 15 & 1450 & 27 & 0 & 27 & .006525 & .1762 \\  
 & 16 & 1550 & 12 & 1 & 11 & .006946 & .0764 \\  
 & 17 & 1650 & 6 & 0 & 6 & .007368 & .0442 \\  
 & 18 & 1750 & 0 & 0 & 0 & .007791 & 0 \\  
 & 19 & 1850 & 1 & 0 & 1 & .008214 & .0082 \\  
 & 20 & 1950 & 0 & 0 & 0 & .008638 & 0 \\  
 & 21 & 2050 & 2 & 0 & 2 & .009062 & .0181 \\ \hline
%\cline{2-2} \cline{8-8} 
&Total&1703&&6&&& 5.057\\ \hline 
\end{tabular}  
\begin{minipage}[t]{4in}
Data from Abernethy, Breneman, Medlin, and Reinman~(1983, pages 43-47).
\end{minipage}
\label{table:bcage.risk.table}
\end{table}
%splus>
%splus>
%splus> ML Estimates
%splus>         MLE     se t.ratio 95% lower 95% upper 
%splus>   mu 9.3752 0.8351  11.226    7.7383   11.0120
%splus>sigma 0.4913 0.1607   3.058    0.2588    0.9327
%splus>
%splus>Weibull beta= 2.035 eta= 1.634 
%splus>
%-------------------------------------------------------------------
The Poisson distribution will, in this example,
provide a good approximation for the SBIN
distribution of $K$.
Figure~\ref{figure:bcage.pred.exfig.ps} shows
the point prediction, naive upper prediction
bound, and the calibrated upper prediction
bound for the bearing-cage population.
%-------------------------------------------------------------------
\begin{figure}
\xfigbookfiguresize{\figurehome/bcage.pred.exfig.ps}{3in}
\caption{Prediction of the future number
failing in the bearing-cage population.}
\label{figure:bcage.pred.exfig.ps}
\end{figure}
%-------------------------------------------------------------------
The naive $95\%$ upper prediction bound on $K$ is
$\kupper(.95)=\Khat_{.95}=9$, the smallest integer $k$ such that
$\SBINCDF(k,\nmrvec,\rhovec)\geq .95$.  The upper calibration curve
shown in Figure~\ref{figure:bcage.future.hn.ps}
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/bcage.future.hn.ps}
\caption{Calibration curve for a prediction interval for
the number of bearing cage failures in the next 12 months.}
\label{figure:bcage.future.hn.ps}
\end{figure}
%-------------------------------------------------------------------
gives, for the upper prediction bound,
$\cp[\pinterval(.991);\thetavechat]=.95$.  Thus the calibrated
$95\%$ upper prediction bound on $K$ is $\kupper(.991)=\Khat_{.991}=12$, the
smallest integer $k$ such that $\SBINCDF(k,\nmrvec,\rhovec) \geq
.991$. 
%splus
%splus  get.cal.numbers(bcage.weib100000.k1.up.300.gethn.out,conlev=.95)
%splus
%splus $lower:
%splus [1] 0.958512 0.959366 0.960220
%splus 
%splus $upper:
%splus [1] 0.990023 0.991122 0.992221
%splus 
%splus
%splus 
%splus [1] .006366651 .038560778 .119958506 .257159305 .430604498 .606015925
%splus [7] .753849217 .860641488 .928143304 .966069437 .985247476 .994063588
%splus[13] .997778611 .999223664 .999745604 .999921557 .999977166 .999993706
%splus[19] .999998353 .999999590 .999999903 .999999978 .999999995 .999999999
%splus[25] 1.000000000 1.000000000
%splus
%splus
%splus pbinom(20:24,9920,.003233)
%splus [1] .01535067 .02511125 .03935605 .05923933 .08583386
%splus
%splus
The naive $95\%$ lower prediction bound on $K$
is $\klower(.95)=\Khat_{.05}=1$, the largest integer $k$
such that $\SBINCDF(k,\nmrvec,\rhovec) < .05$.
The lower calibration curve shown in
Figure~\ref{figure:bcage.future.hn.ps}
gives
$\cp[\pinterval(.959);\thetavechat]=.95$.
Thus the calibrated $95\%$ lower prediction
bound on $K$ is $\klower(.959)=\Khat_{.041}=1$, the largest
integer $k$ such that
$\SBINCDF(k,\nmrvec,\rhovec) < 1 - .959=.041$. Note
that, in this particular case, the naive and
the calibrated prediction bounds are the same.
\end{example}

\section*{Bibliographic Notes}
This chapter is based largely on Escobar and Meeker~(1998a).
There is a considerable amount of literature on statistical
prediction.  Hahn and Nelson~(1973), Patel~(1989), and Chapter 5 of
Hahn and Meeker~(1991) provide surveys of methods for statistical
prediction for a variety situations.  Lawless~(1973), Nelson and
Schmee~(1981), Engelhardt and Bain~(1979), and Mee and Kushary~(1994)
describe exact simulation-based methods to obtain prediction intervals for
Type~II censored data.  These methods, for log-location-scale
distributions.
are based on the distribution of
pivotal statistics.  Type~II censoring, however, is rare in practical
application. Nelson~(1995c) gives a simple procedure for computing
prediction limits for the number of failures that will be observed in
a future inspection, based on the number of failures in a previous
inspection when the units have a Weibull failure-time distribution
with a given shape parameter.

Faulkenberry~(1973) suggests a method that can be applied when there
is a sufficient statistic that can be used as a predictor. Cox~(1975)
present a general approximate analytical approach to prediction based
on the asymptotic distribution of ML estimators.  Atwood~(1984) used a
similar approach. Beran~(1990) presents the bootstrap calibration method
for obtaining prediction intervals and gives theoretical results on the
properties of prediction statements obtained with such calibration methods.
An approximate pivotal-based approach is described
in Efron and Tibshirani~(1993, page 390-391).  Kalbfleisch~(1971)
describes a likelihood-based method, Thatcher~(1964) describes the
relationship between Bayesian and frequentist prediction for the binomial
distribution, while Geisser~(1993) presents a more general overview of the 
Bayesian approach (see also Chapter~\ref{chapter:singledist.bayes}).

\section*{Exercises}

\begin{exercise}
A sample of 20 aluminum specimens was tested until fatigue failure. A
probability plot showed that the lognormal distribution provides an
adequate description of the spread in the data.  The sample mean and
standard deviation of the logarithms of cycles-to-failure were 5.13 and
.161, respectively.
\begin{enumerate}
\item
\label{exer.part.ln.pred.a}
Compute a 95\% confidence interval for the median 
of the cycles to failure distribution.
\item
\label{exer.part.ln.pred.b}
Compute a 95\% prediction interval for the number of cycles to failure
for a future specimen tested in the same way. Compare this with the
``naive'' prediction interval computed as if the estimates are the
parameters.
\item
Redo parts~\ref{exer.part.ln.pred.a} and
\ref{exer.part.ln.pred.b}
supposing, instead, that the sample size had been 100 units.
Comment on the results.
\item
Explain why there is so much difference between 
the confidence interval in part~\ref{exer.part.ln.pred.a} and the
prediction interval in part~\ref{exer.part.ln.pred.b}.
\end{enumerate}
\end{exercise}

\begin{exercise}
Show that putting together a one-sided lower and a one-sided upper
100$(1-\alpha/2)$\% prediction bounds for a future observation results
in a two-sided 100$(1 - \alpha)$\% prediction interval for that
observation.
\end{exercise}

\begin{exercise1}
Let $\realrv_{1}, \dots, \realrv_{n}$ denote the $r$ failure times and
the $n-r$ censored times of a failure-censored test with
$\EXP(\expmean)$ data.  As indicated in
Chapter~\ref{chapter:parametric.ml.one.par}, the ML estimator of
$\theta$ is $\thetahat=\ttt/r$ and $2(r \thetahat/ \expmean) \sim
\chisquare_{(2r)}$.  If $\rv$ is a new independent observation then
$2\rv/\expmean \sim \chisquare_{(2)}$. Thus it follows that
$T/\thetahat \sim \Fquant_{(2,2r)}$.
\begin{enumerate}
\item
Show that $\pinterval(1-\alpha)=
(0, \Tupper)$ where 
$\Tupper= \Fquant_{(1-\alpha;2,2r)} \times \thetahat =
r \left [\alpha^{(-1/r)}-1 \right ] \times \thetahat $ 
is an exact $100(1-\alpha)\%$ prediction interval for $\rv$.
\item
Show that the conditional coverage probability, conditional on
$\thetahat$, is $$\cp[\pinterval(1-\alpha) \mid \thetahat ; \theta]
=1-\exp \left \{ -\thetahat \times r \times \left [\alpha^{(-1/r)}-1
\right ]/\expmean 	\right \}.  $$
\item
Show that $\lim_{r \to \infty}\cp[\pinterval(1-\alpha) \mid \thetahat ;
\theta]= 1-\alpha$. Comment on the practical interpretation of this
result.
\item
Compute the unconditional coverage probability using
\begin{eqnarray*}
\cp[\pinterval(1-\alpha)]&=&\E_{\thetahat}\left \{
	   \cp[\pinterval(1-\alpha) \mid \thetahat ; \theta]
\right \}
\\
&=&
\E_{\thetahat}
\left (
1-\exp \left \{
-\thetahat \times r \times \left [\alpha^{(-1/r)}-1 \right ]/\expmean
	\right \}
\right )
\end{eqnarray*}
and verify that it is equal to $100(1-\alpha)\%$ for any $n$.
\end{enumerate}
\end{exercise1}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{exercise1}
\label{exercise:pred.exp.complete}
Suppose $\realrv_{1}, \dots, \realrv_{n}$ is a random sample of size
$n$ from an $\EXP(\expmean)$.  Denote the ML of $\expmean$ by
$\thetahat_{n}$ ( $\thetahat_{n}=\bar{\realrv}$).  Consider the naive
prediction interval $\pinterval(1-\alpha)= (0, \Tupper)$ where
$\Tupper= [-\log(\alpha)] \times \thetahat_{n}$ and $[-\log(\alpha)]$
is the $100(1-\alpha)\%$ quantile of the $\EXP(1)$ distribution.
\begin{enumerate}
\item
Show that the conditional coverage probability is
$$\cp[\pinterval(1-\alpha) \mid \thetahat ; \theta]
=1-\alpha^{(\expmeanhat/\expmean)}.  $$
\item
Show that $\lim_{n \to \infty}
\cp[\pinterval(1-\alpha) \mid \thetahat ; \theta]=1-\alpha$. (Here
you might want to use the fact that $\thetahat_{n} \to \theta$ in
probability when $n \to \infty$.)  Explain the practical implications
of this result.
\item
Show that $\cp[\pinterval(1-\alpha)]=
\E_{\thetahat} \left \{
 \cp[\pinterval(1-\alpha) \mid \thetahat ; \theta] \right \}
=1-[1-(1/n)\log(\alpha)]^{-n}.  $
\item
Show that 
$\cp[\pinterval(1-\alpha)] < 1-\alpha$, for  all $1-\alpha$ and $n$.
\item
For $n=2$ draw a graph of $\cp[\pinterval(1-\alpha)]$ for values of
$1-\alpha$ over the interval $[.9,\,\, 1]$ and compare the
$\cp[\pinterval(1-\alpha)]$ with the nominal coverage of
$100(1-\alpha)\%$.

Repeat this for $n=4, 10, 100$.  Comment on the behavior of the
coverage probability and the length of the naive prediction interval
in large samples.
\item
Show that $\lim_{n\to \infty}\cp[\pinterval(1-\alpha)]=1-\alpha$ and
comment on the practical implications of this result.
\end{enumerate}
\end{exercise1}
%----------------------------------------------------------------------
%----------------------------------------------------------------------

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{exercise1}
Refer to Exercise~\ref{exercise:pred.exp.complete} but now suppose
that the data are failure-censored with $r$ observed failures.
Generalize all the results in that exercise to this new situation.
\end{exercise1}
%----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{exercise}
\label{exercise:pi.normal.sigma.known}
Suppose that $\realrv_{1}, \dots, \realrv_{n}$ is a random sample from
a $\LOGNOR(\mu, \sigma)$ distribution.  Suppose that $\sigma$ is known and let
$\muhat_{n}=\bar{\grealrv}=\sum_{i=1}^{n}\grealrv_{i}/n$ where,
$y_{i}=\log(t_{i})$.  Consider the prediction of a new time to failure
observation $\rv$.  In this case, $\left [\log(\rv)
-\muhat_{n} \right ]/\sqrt{\var(\rv -\muhat_{n})} = \left [\log(\rv)
-\muhat_{n}]/[\sigma \sqrt{1+1/n} \right ]
\sim \NOR(0,1)$, which suggests the prediction interval
\begin{displaymath}
\pinterval(1-\alpha)=
\left [0, \quad
\exp \left (\muhat_{n}+\sigma \sqrt{\frac{n+1}{n}}\, \, 
	\Phi^{-1}_{\nor}(1-\alpha)
\right )
\right ].
\end{displaymath}
\begin{enumerate}
\item
\label{exer.part:item.coverage.probability}
Show that $\cp[\pinterval(1-\alpha)]=1-\alpha$ for any $n$.
\item
Show that 
\begin{displaymath}
\cp[\pinterval(1-\alpha) \mid \muhat_{n} ; \mu]
=
\Phi_{\nor} \left [
\frac{\muhat_{n}-\mu}{\sigma}
+ \sqrt{\frac{n+1}{n}}\,\, \Phi^{-1}_{\nor}(1-\alpha) 	 \right ].
\end{displaymath}
\item
Show that $\lim_{n \to \infty}\cp[\pinterval(1-\alpha) \mid \muhat_{n} ;
\mu]=1-\alpha$.
\item
Show that $\cp[\pinterval(1-\alpha)] =\E_{\muhat_{n}}
\left \{
\cp[\pinterval(1-\alpha) \mid \muhat_{n} ; \mu]
\right \}=1-\alpha$. Notice the complexity of this computation
when compared with the computation on
part~\ref{exer.part:item.coverage.probability}.
\item
Derive an expression for a two-sided $100(1-\alpha)\%$ prediction
interval for $\rv$.
\item
A naive prediction interval is $\pinterval(1-\alpha)=[0,
\exp(\muhat+ \Phi^{-1}_{\nor}(1-\alpha) \times \sigma)]$.  Show that
$\cp[\pinterval(1-\alpha)]=
\Phi_{\nor}[\sqrt{n/(n+1)} \, \, \Phi^{-1}_{\nor}(1-\alpha)].$
Draw a graph of this coverage probability for values of $n=2,4,10$
over values of $1-\alpha$ in the interval $[.9,\,\, 1]$.  Comment on the
coverage probabilities and their behavior as a function of the sample
size.
\end{enumerate}
\end{exercise}
%----------------------------------------------------------------------
%----------------------------------------------------------------------

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{exercise1}
Refer to Exercise~\ref{exercise:pi.normal.sigma.known} but suppose
that $\sigma$ is unknown.  Define
$\sigmahat_{n}=\sqrt{(1/n)\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}}$.  In this case,
$[\log(\rv) -\muhat_{n}]/[\sigmahat_{n} \sqrt{(n+1)/(n-1)}]$ has a
Student's $t$ distribution with $(n-1)$ degrees of freedom, which
suggests the prediction interval
\begin{displaymath}
\pinterval(1-\alpha)=
\left [0, \quad
\exp \left (\muhat_{n}+\frac{\sigmahat_{n}}
		 {\sigma} \sqrt{\frac{n-1}{n+1}}\, \Tquant_{(1-\alpha;n-1)}
     \right )
\right ]
\end{displaymath}
where $\Tquant_{(1-\alpha;n-1)}$ is the $100(1-\alpha)\%$ quantile of
the Student's $t$ distribution with $(n-1)$ degrees of freedom.
\begin{enumerate}
\item
Show that $\cp[\pinterval(1-\alpha)] =1-\alpha$ for any $n$.
\item
Show that 
\begin{displaymath}
\cp[\pinterval(1-\alpha) \mid \muhat_{n} ; \mu]
=
\Phi_{\nor} \left [
\frac{\muhat_{n}-\mu}{\sigma}
+ \frac{\sigmahat_{n}} {\sigma} \sqrt{\frac{n-1}{n+1}}\,
\Tquant_{(1-\alpha;n-1)} 	 \right ].
\end{displaymath}
\item
Show that $\lim_{n \to \infty}\cp[\pinterval(1-\alpha) \mid \muhat_{n} ;
\mu]=1-\alpha$.
\item
Derive the expression for a two-sided $100(1-\alpha)\%$ prediction
interval for $\rv$.
\item
A naive upper prediction interval is 
$\pinterval(1-\alpha)=[0, \exp(\muhat+ \Phi^{-1}_{\nor}(1-\alpha)
\times \sigmahat)]$.
Show that $\cp[\pinterval(1-\alpha)]=
\Pr[ X \le \sqrt{n/(n+1)} \, \Phi^{-1}_{\nor}(1-\alpha)],$
where $X$ has a  Student's $t$ distribution
with $(n-1)$ degrees of
freedom.
Draw a graph of this coverage probability for values of
$n=2,4,10$ over values of $1-\alpha$ in the interval $[.9,\,\, 1]$.
\end{enumerate}

\end{exercise1}
%----------------------------------------------------------------------
%----------------------------------------------------------------------

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{exercise1}
Consider the prediction of a new observation from a log-location-scale
family with cdf $\Phi[(\log(\realrv)-\mu)/\sigma]$.  Let $[
\Tlower, \quad \Tupper ]=
[\rvquanhat_{\alpha/2},\quad \rvquanhat_{1-\alpha/2}]$ be the naive
prediction interval computed based on a set of available data.
\begin{enumerate}
\item
Show that the coverage probability of the prediction interval,
conditional the ML estimates $\thetavechat=(\muhat,\sigmahat)$, is
given by
\begin{displaymath}
\cp \left [ \pinterval(1-\alpha) \mid \thetavechat ; \thetavec \right ]=
\Phi\left [
 \frac{ \muhat-\mu } { \sigma } +\frac{ \sigmahat } { \sigma } \times
\Phi^{-1}(1-\alpha/2)   
     \right ] -
\Phi\left [
 \frac{ \muhat-\mu } { \sigma } +\frac{ \sigmahat } { \sigma } \times
\Phi^{-1}(\alpha/2)  
     \right ]
\end{displaymath}
where $\thetavechat=(\muhat, \sigmahat)$.
\item
Show that when the sample size increases to $+\infty$, the
conditional coverage probability converges in probability to
$1-\alpha$.
\end{enumerate}
\end{exercise1}

