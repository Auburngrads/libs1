%chapter 14
%\batchmode
%original by wqmeeker  12 Jan 94
%edited by wqmeeker 26 Apr 94
%edited by wqmeeker  1 June 94
%edited by wqmeeker  15 June 96 bringing in slides
%edited by driker 24 june 96
%edited by driker 22 july 97

\setcounter{chapter}{13}

\chapter{Introduction to the Use of
Bayesian Methods for Reliability Data}

\label{chapter:singledist.bayes}

\input{\chapterhome/common_heading.tex}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Objectives}
This chapter explains
\begin{itemize} 
\item 
The use of Bayesian statistical methods to
combine ``prior'' information with data to make inferences.
\item
The relationship between Bayesian methods and the likelihood methods
used in earlier chapters.
\item 
Sources of prior information.
\item 
Useful statistical and numerical methods for Bayesian analysis.
\item
Bayesian methods for estimating reliability.
\item
Bayesian methods for prediction.
\item 
The dangers of using ``wishful thinking'' or
expectations as prior information.
\end{itemize}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Overview}
This chapter explains basic Bayesian methods and illustrates
them with some simple applications in reliability analysis. This
chapter builds on material from
Chapter~\ref{chapter:parametric.ml.ls}.
Section~\ref{section:bayes.theorem} shows how Bayes' rule can be
used to combine prior information with data.
Section~\ref{section:prior.dist} explains the different kinds of
prior distributions and how they are obtained.
Section~\ref{section:numerical bayes} describes numerical methods
for combining prior information with a likelihood and for computing
marginal posterior distributions.
Section~\ref{section:bayes.estimation} describes and gives methods
for using the posterior distribution to obtain point estimates and
compute Bayesian confidence and prediction intervals.
Section~\ref{section:cautions.on.prior} describes some of the
dangers involved in using prior information in a statistical
analysis.

%----------------------------------------------------------------------
%----------------------------------------------------------------------

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Introduction}

Bayesian methods are closely related to likelihood methods. Bayesian
methods, however, allow data to be combined with ``prior'' information
to produce a posterior distribution for a parameter or parameters. This
posterior is used to quantify uncertainty about the parameters
and functions of parameters, much as the likelihood was used in earlier
chapters.

Combinations of extensive past experience and physical/chemical theory
can provide prior information to form a framework for inference and
decision making. In many applications
it is necessary to combine prior information with limited additional
observational or experimental data.  For example, reliability
engineers may know, with a high degree of certainty, that products
made out of a certain alloy will eventually fail from fracture caused
by repeated fatigue loading. The lognormal (base $e$) distribution with shape
parameter $\sigma$ in the interval of .5 to .7 has always provided an
adequate model. To estimate the cycles-to-failure distribution of a
new product made from the same alloy with needed precision might
require hundreds of sample units.  By incorporating the prior
information about $\sigma$ into the analysis, an adequate estimate of
reliability might be obtained with 20 or 30 units.  There are, of
course, dangers involved in making strong assumptions about knowledge
of model parameters. These will be described in more detail in
Section~\ref{section:cautions.on.prior}.

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Using Bayes' Rule to Update Prior Information}
\label{section:bayes.theorem}
\subsection{Notation}
To keep the presentation of the basic ideas simple, in this chapter we
follow the usual convention in the Bayesian statistics literature and
let the argument of pdfs and cdfs indicate the parameter for which
uncertainty is being described.  For example, $f(\thetavec)$ denotes
the prior pdf of $\thetavec$ and $f[\log(\thetavec)]$ denotes the
prior pdf of $\log(\thetavec)$.  Also $f(\thetavec | \DATA)$ and
$f[\log(\thetavec)|\DATA]$ will represent the posterior
pdfs of $\thetavec$ (distribution of $\thetavec$ given the available
data) and $\log(\thetavec)$, respectively.

In some cases it will be necessary to start with the prior
pdf of one parameter and then use it to obtain the prior pdf for
a function of that parameter. For example, for a scalar parameter
$\theta$, $f(\theta) = f[\log(\theta)]/\theta$. See
Appendix~\ref{asection:transformation.of.rv} for details on
deriving the pdf of a function of a parameter vector $\thetavec$.

\subsection{Bayes' rule}
Bayes' rule provides a mechanism for combining {\em prior} information
with sample data to make inferences on model parameters.
This is illustrated in Figure~\ref{figure:bayesfig.ps}.
%-------------------------------------------------------------------
\begin{figure}
\xfigbookfigure{\figurehome/bayesfig.ps}
\caption{Bayesian method for making inferences or predictions}
\label{figure:bayesfig.ps}
\end{figure}
%-------------------------------------------------------------------
Analytically, for a vector parameter $\thetavec$ the procedure is as
follows.  Prior knowledge about $\thetavec$ is expressed in terms of a
pdf denoted by $f(\thetavec)$.  The likelihood for the available data
and specified model is given by $\like(\DATA | \thetavec ) =
\like(\thetavec;
\DATA)$.  Then, using Bayes' rule, the conditional distribution of
$\thetavec$ given the data provides the {\em posterior} pdf
of $\thetavec$, representing the updated state of knowledge about
$\thetavec$. This posterior distribution can be expressed as
\begin{equation}
\label{equation:bayes.theorem}
f(\thetavec | \DATA)=
\frac{\like(\DATA | \thetavec ) f(\thetavec) }
     { \int \like(\DATA | \thetadummyvec ) f(\thetadummyvec) d \thetadummyvec}
=
\frac{R(\thetavec) f(\thetavec) }
     { \int R(\thetadummyvec) f(\thetadummyvec) d \thetadummyvec}.
\end{equation}
Here $R(\thetavec)=\like(\thetavec)/\like(\thetavechat)$ is the
relative likelihood (introduced in
Chapters~\ref{chapter:parametric.ml.one.par} and
\ref{chapter:parametric.ml.ls}) and
the integral is computed over the region $f(\thetadummyvec) > 0$.

In general, it is impossible to compute the integral in
(\ref{equation:bayes.theorem}) in closed form.  Numerical methods are
needed and these methods can be computationally intensive when the
length of $\thetavec$ is more than two or three. In the past this has
been an impediment to the use of Bayesian methods. Today, however, new
statistical and numerical methods that take advantage of modern
computing power are making it feasible to apply Bayesian methods to a
much wider range of real problems.

%----------------------------------------------------------------------
\section{Prior Information and Distributions}
\label{section:prior.dist}

It is convenient to divide available prior information 
about a parameter into
three
different categories:
\begin{enumerate}
\item
Parameters that are given as known, leading to a degenerate prior distribution.
\item
Parameters with a diffuse or approximately noninformative prior distribution.
\item
An informative, nondegenerate prior distribution.
\end{enumerate}

In general, there are two possible sources of prior information:
a) expert or other subjective opinion or b) past data.  The prior
pdf $f(\theta)$ may be either informative or not.  Loosely
speaking, a noninformative\footnote{There is a particular technical
definition for a noninformative prior distribution, but we use the
term loosely to indicate a prior distribution that carries little or
no weight in estimation relative to the information in the available
data} prior distribution is one that provides little or no information
about any of the parameters in $\thetavec$.  Such a prior distribution
is useful when it is desired to let the data speak for themselves
without being influenced by previous data, expert opinion, or other
available prior information.

The most important motivation for using prior information is to
combine it with data to provide more and better information about
model parameters of interest.  An informative prior distribution is
expressed in the form of a (joint) pdf for the parameter(s) for
which information is available.  A ``proper'' prior pdf $f(\theta)$
is a nonnegative function that is defined for all values of the
parameters and that integrates to one.  Some examples of proper
prior distributions for scalar parameters include:

\begin{itemize}
\item
Normal prior distribution with mean $a$ and a standard deviation $b$
so that $f(\theta)=(1/b)\phi_{\nor}[(\theta-a)/b]$ 
for $-\infty < \theta < \infty$.
\item
Uniform prior distribution between $a$ and $b$ [denoted by $\UNIF(a,b)$] so
that $f(\theta)=1/(b-a)$ for $a \le \theta \le b$.  This prior 
distribution does
not express strong preference for specific values of $\theta$ in the
interval.
\item
Beta prior distribution between specified $a$ and $b$ with specified shape
parameters (allows for a more general shape).
\item
Isosceles triangle prior distribution with base (range) between $a$ and $b$.
\end{itemize}
For a prior pdf with a finite endpoint on one or both sides of its
range, $f(\theta)=0$ outside the specified range.  For a positive
parameter $\theta$ it is often more natural or convenient to specify
the prior pdf in terms of $\log(\theta)$.

%----------------------------------------------------------------------
\subsection{Noninformative (diffuse) prior distributions}
\label{section:noninformative.prior}

Noninformative (or approximately noninformative) prior pdfs
are constant (or approximately constant) over the range(s) of the
model parameter(s).  Other names for noninformative prior
distributions are ``vague prior'' and ``diffuse prior''
distributions.

Some noninformative prior pdfs are ``improper'' because
they do not integrate to a finite quantity (i.e., $\int f(\thetavec)
d\thetavec= \infty$). For example, with an unrestricted $\theta$,
a uniform distribution
$f(\theta)=c$ for all $ -\infty < \theta < \infty $ does not have a
finite integral.  For a positive parameter $\theta$ the
corresponding choice is $f[\log(\theta)] = c$ or $f(\theta) =
c/\theta$, $0 < \theta < \infty$, and this prior pdf is also
improper.  Improper prior pdfs cause no difficulties as long as
the resulting {\em posterior} pdf is ``proper'' (integrates
to one). Whether this is so or not depends on the form of the model
and the available data.

The effect of using a noninformative prior distribution for
$\thetavec$ can be seen as follows.  For a uniform prior pdf
$f(\thetavec)$ (possibly improper) across all possible values of
$\thetavec$
\begin{displaymath}
f(\thetavec | \DATA)
=
\frac{R(\thetavec) f(\thetavec) }
     { \int R(\thetadummyvec) f(\thetadummyvec) d \thetadummyvec} = \frac{R(\thetavec) }
     { \int R(\thetadummyvec)  d \thetadummyvec}.
\end{displaymath}
This indicates that the posterior pdf
$f(\thetavec | \DATA)$ is
proportional to the likelihood.

It is possible, for example, to replace an improper uniform prior
with a proper uniform prior by limiting the range of the pdf to a
finite interval. As long as the range of the uniform prior pdf
includes all values of the parameters with substantially large
relative likelihood, the prior distribution will remain
(approximately) uninformative. That is, with a finite-range uniform
prior pdf, the posterior pdf is approximately proportional to the
likelihood if the range of the uniform prior distribution is large
enough so that $R(\thetavec) \approx 0$ where $f(\thetavec)=0$.


%----------------------------------------------------------------------
\subsection{Using past data to specify a prior distribution}
Prior distributions can also be based on available data.  Combining
past data with a noninformative prior distribution gives a posterior
pdf that is proportional to the likelihood. This posterior
pdf can then serve as a prior pdf for further updating with new
data.

%----------------------------------------------------------------------
\subsection{Expert opinion and eliciting prior information}

The elicitation of a prior distribution for a single parameter may be
straightforward if there has been considerable experience in estimating or
observing estimates of that parameter in similar situations.  For
vector of parameters, however, the elicitation and specification of a
meaningful joint prior distribution is more difficult.  In general,
marginal distributions for individual parameters do not completely
determine the joint distribution.  Also, it is difficult to
elicit opinion on dependences among parameters and then
express these as a joint distribution.  For example, if previous
experience with integrated circuit devices is always obtained from
studies with just a few percent failing, then past estimates of the
lognormal parameters $\mu$ and $\sigma$ would be highly correlated,
implying that the prior distribution for these parameters should
reflect this dependency.  Also, it may not be reasonable to elicit
opinion about parameters from a standard parameterization when those
parameters have no physical or practical meaning. Again, for the
integrated circuit devices, if only a few percent fail in studies, it
might be better to elicit information about a quantile at which a few
percent fail, rather than about $\mu$, which, for the lognormal distribution,
corresponds to the logarithm of the time at
which 50\% of the units in a population will fail.

A general approach is to elicit information about particular
quantities (or parameters) that, from past experience (or data), can
be specified approximately independently. For example, for a high
reliability integrated circuit, a good choice would be a quantile in
the lower tail of the failure-time distribution and the lognormal
shape parameter $\sigma$.  Then the corresponding prior
distributions for these quantities can be described as being
approximately independent.

When there is useful informative prior information for a parameter, one
elicits a general shape or form of the distribution and the range (or
approximate range) of the distribution.  For example, the uncertainty
in the .01 quantile of a failure-time distribution (a positive
quantity) might be described by lognormal prior distribution with
99.7\% content between two specified time points (expressing the
approximate range).

When specifying the prior distribution for quantities for which there
is {\em no}  prior information, no detailed
elicitation is necessary, but one does have to specify the form and
range of the vague prior distribution
(e.g. a uniform distribution over a sufficiently wide
interval, as described in Section~\ref{section:noninformative.prior},
is generally satisfactory).

\begin{example}
\label{example:bearing.cage.prior}
{\bf Prior distributions for estimating the bearing-cage life
distribution}.  This example revisits the Bearing-cage field data
that were fit to a Weibull distribution in
Example~\ref{example:bearing.cage}.  Suppose that with appropriate
questioning, engineers provided the following information based on
experience with previous products of the same material and knowledge
of the failure mechanism.  Life can be described adequately with a
Weibull distribution and the Weibull shape parameter $\beta$ would
almost certainly be between 2 and 5 ($\sigma$ between .2 and .5).
Using a normal distribution to express the uncertainty in
$\log(\sigma)$ gives $\log(\sigma) \sim
\NOR(a_{0},\, b_{0}), $ where $a_{0}$ and $b_{0}$ are obtained from
the specification of two extreme quantiles $\sigma_{(\gamma/2)}$ and
$\sigma_{(1-\gamma/2)}$ of the prior distribution for $\sigma$. Then
\begin{displaymath}
a_{0}= \log \left  [\sqrt{\sigma_{(\gamma/2)}\times\sigma_{(1-\gamma/2)}}
           \, \right ],
\quad
b_{0}= \frac
     {\log \left
	[\sqrt{\sigma_{(1-\gamma/2)}/\sigma_{(\gamma/2)}}\,\right ]}
	{\norquan_{(1-\gamma/2)}}.
\end{displaymath}
The prior (normal) pdf for $\log(\sigma)$ is
\begin{displaymath}
f[\log(\sigma)]= \frac{1}{b_{0}} \, 
  \phi_{\nor} \left [
	 \frac{\log(\sigma)-a_{0}}
	      {b_{0}}
              \right ], \quad \sigma>0.
\end{displaymath}
The corresponding prior pdf for $\sigma$ is $f(\sigma)=(1/\sigma)
f[\log(\sigma)]$.  Figure~\ref{figure:sigma.prior2.ps} shows the
marginal prior pdfs for $\log(\sigma)$ and $\sigma$ when $\sigma_{.005}=.2,
\sigma_{.995}=.5$ and $\gamma=.01$ (corresponding to 1\% probability outside
the limits $\sigma_{.005}=.2, \sigma_{.995}=.5$ and giving
$a_{0}=-1.151$ and $b_{0}=.178$).
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/sigma.prior2.ps}
\caption{Prior pdfs for $\log(\sigma)$ and $\sigma$ when
the prior for $\sigma$ is 
a lognormal distribution specified by $\sigma_{.005}=.2, \sigma_{.995}=.5$}
\label{figure:sigma.prior2.ps}
\end{figure}
%-------------------------------------------------------------------

In previous studies for similar products, censoring was heavy, and
there was a much better sense of expected time for 1\% failing than
for 50\% or more failing.  Thus for small $p$ (near the proportion
failing in previous studies), $\rvquan_{p}$ and $\sigma$ are
approximately independent (which allows for specification of
independent priors).  Actually, however, little was
known about the Weibull .01 quantile for this particular product
and, in this application, it was decided to describe the uncertainty in
$\log(\realrv_{p})$ with a $\UNIF[\log(a_{1}),
\log(b_{1})]$ distribution where $a_{1}=100$ and $b_{1}=5000$.  This
is a wide range and thus this part of the prior distribution is not
very informative. Then the prior pdf for $\log(\rvquan_{p})$ is uniform
\begin{displaymath}
f[\log(\rvquan_{p})]=\frac{1}{\log(b_{1}/a_{1})}, \quad a_{1} \le
\rvquan_{p} \le b_{1}.
\end{displaymath}
The corresponding prior pdf for $\rvquan_{p}$ is
$f(\rvquan_{p})=(1/\rvquan_{p})f[\log(\rvquan_{p})]$.
Figure~\ref{figure:q01.prior2.ps} shows the marginal prior pdfs for
$\log(\rvquan_{p})$ and $\rvquan_{p}$.  This figure shows why a
prior that is noninformative (uniform) for a parameter is
informative (nonuniform) for a nonlinear function of that parameter.
The distribution for $t_{.01}$ is, however, approximately
noninformative over the range $1000 < t_{.01} < 5000$.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/q01.prior2.ps}
\caption{Prior pdfs for $\log(\rvquan_{.01})$ and $\rvquan_{.01}$ with
$a_{1}=100, b_{1}=5000$.}
\label{figure:q01.prior2.ps}
\end{figure}
%-------------------------------------------------------------------

Using the (approximate) independence of $\rvquan_{p}$ and $\sigma$,
the joint prior pdf for $(\rvquan_{p},\sigma)$ is
\begin{displaymath}
f(\rvquan_{p}, \sigma)=
\frac{f[\log(\rvquan_{p})]}{\rvquan_{p}} \times 
\frac{f[\log(\sigma)]}{\sigma},\quad 
a_{1} \le \rvquan_{p} \le b_{1}, \,\, \sigma > 0.
\end{displaymath}
The transformation 
$\mu=\log(\rvquan_{p})-\Phi^{-1}_{\sev}(p) \sigma, \sigma=\sigma$
yields 
\begin{eqnarray}
f(\mu, \sigma)&=&\frac{f\left [ \log (\rvquan_{p}) \right ]}
{\rvquan_{p}} \times \frac{f[\log(\sigma)]} {\sigma} \times
\rvquan_{p}=f\left [ \log (\rvquan_{p}) \right ] \times
\frac{f[\log(\sigma)]}
      {\sigma} \nonumber
\\
\label{equation:bearing.cage.prior.mu.sigma}
&=& \frac{ 1 	 } 	 { \log(b_{1}/a_{1}) 	 } 	\times \frac{
\phi_{\nor} \left \{ 		 \left [
\log(\sigma)-a_{0}	 \right ]/b_{0} \right \} } { \sigma b_{0} }
\end{eqnarray}
as the joint prior pdf for $(\mu, \sigma)$ where 
$
\log(a_{1})-\Phi^{-1}_{\sev}(p) \sigma \le   \mu   \le \log(b_{1})-\Phi^{-1}_{\sev}(p) \sigma,
\sigma > 0.$ 
\end{example}

%----------------------------------------------------------------------
\section{Numerical Methods for Combining Prior 
	Information with a Likelihood}
\label{section:numerical bayes}

Given a specified prior pdf and likelihood function,
it is easy to write an expression for the posterior pdf
using (\ref{equation:bayes.theorem}).

\begin{example}
\label{example:bearing.cage.posterior.form}
{\bf Posterior for the bearing-cage life distribution}. 
Following Section~\ref{section:exp.loc.scale.like},
the likelihood for the bearing-cage life distribution is 
\begin{equation}
\label{equation:bearing.likelihood}
\like(\DATA | \mu,\sigma) = \prod_{i=1}^{2003}
\left\{ \frac{1}{\sigma \realrv_{i}} \, \phi_{\sev}
\left[\frac{ \log(\realrv_{i}) -\mu}{\sigma}
\right]
\right\}^{\delta_{i}}
\times
 \left\{1-\Phi_{\sev} \left[\frac{ \log(\realrv_{i}) -\mu}{\sigma}
\right] \right\}^{1-\delta_{i}} 
\end{equation}
where $\delta_{i}=1$ ($\delta_{i}=0$) if observation $i$ is a
failure (right censored observation).  Substituting the prior pdf in
(\ref{equation:bearing.cage.prior.mu.sigma}) and the likelihood in
(\ref{equation:bearing.likelihood}) into
(\ref{equation:bayes.theorem}) provides an expression for the
posterior pdf for the bearing-cage life distribution.  In
general, this posterior pdf cannot be evaluated
analytically. Numerical methods must be used instead.
\end{example}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\subsection{Numerical integration methods for 
	computing the posterior pdf of $ \protect \boldsymbol{\thetavec}$} 

For problems with one or two parameters it is reasonably easy to
compute, numerically, the posterior distribution by using numerical
integration.  It is difficult, however, to provide general-purpose
software that will work on all problems, especially with two or more
parameters.  Although numerical integration is generally a reasonably
stable numerical procedure and there are many algorithms available
for one-dimensional integration, it is possible for numerical problems
to arise. To guarantee accurate results, it is generally necessary to
have an idea of the shape of the function being integrated and to make
sure that the algorithm performed satisfactorily over the entire
relevant range of integration. Such checking becomes difficult when
there is more than one variable of integration. It is relatively
difficult to find good algorithms for integration over two or more
variables.

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\subsection{Simulation-based methods for computing the 
	posterior distribution of $ \protect \boldsymbol{\thetavec}$}
\label{section:sim.bayes.comp}
Simulation can be used to generate a sample from the posterior
distribution of $\thetavec$.
Then this sample can be used to approximate the posterior
distribution.  Using a larger number of simulated points provides a
better approximation and the number of points used is limited only
by computing equipment and time constraints.  The procedure is
general and is easy to apply, requiring only computable expressions
of the relative likelihood and the inverse cdf of the independent
marginal prior distributions.  The procedure is as follows:
\begin{algorithm}{\bfseries Computation of a sample from the
prior distribution with Monte Carlo simulation.}
\label{algorithm:bayes.mc.sim}
\begin{enumerate}
\item
Generate a random sample $\thetavec_{i}$, $i=1, \ldots, M$ from the
prior $f(\thetavec)$ (as described in
Section~\ref{section:generating.random.numbers}).
\item
Retain the $i$th
sample value, $\thetavec_{i}$, with probability
$R(\thetavec_{i}).$ Do this by 
generating $U_{i}$, a random quantity from a uniform $(0,1)$,
and retain $\thetavec_{i}$ if
$U_{i} \le R(\thetavec_{i}).$
\end{enumerate}
\end{algorithm}
It can be shown that the 
retained sample values, say
$\thetavec_{1}^{\star}, \ldots
\thetavec_{M^{\star}}^{\star}$ ($M^{\star} \le M$),
are a random sample from
the posterior pdf $f(\thetavec | \DATA)$.

\begin{example}
\label{example:bearing.cage.prior.compute}
{\bf Computation of a sample from the prior distribution
of the parameters of the bearing-cage life distribution.} Continuing
with Example~\ref{example:bearing.cage.posterior.form}, the joint
prior for $\thetavec=(\mu, \sigma)$, is generated as follows. First
use the inverse cdf method (see
Section~\ref{section:generating.random.numbers}) to obtain a
random sample for $\rvquan_{p}$ from
\begin{displaymath}
(\rvquan_{p})_{i}=a_{1}\times (b_{1}/a_{1})^{U_{1i}},
\quad i=1, \ldots, M
\end{displaymath}
where $U_{11}, \ldots, U_{1M}$ is a random sample from a
$\UNIF(0,1)$ distribution.  For this example we use $p=.01$ because
it was thought that $\rvquan_{.01}$ and $\sigma$ could be described as
being approximately independent.  Similarly, obtain a random
sample for $\sigma$, say
\begin{displaymath}
\sigma_{i}=\exp \left [a_{0}+b_{0} \Phi^{-1}_{\nor}(U_{2i})
	        \right ],
\quad i=1, \ldots, M
\end{displaymath}
where $U_{21}, \ldots, U_{2M}$ is another
independent random sample
from a $\UNIF(0,1)$.
Figure~\ref{figure:q01.sigma.prior2.sim.ps} shows simulated
points from the joint prior distribution for $\rvquan_{.01}$ and $\sigma$.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/q01.sigma.prior2.sim.ps}
\caption{Simulated points from the 
joint prior for $\rvquan_{.01}$ and $\sigma$.}
\label{figure:q01.sigma.prior2.sim.ps}
\end{figure}
%-------------------------------------------------------------------
Then $\thetavec_{i}=(\mu_{i}, \sigma_{i})$ with $\mu_{i}=\log \left
[(\rvquan_{p})_{i}\right ]-\Phi^{-1}_{\sev}(p)\sigma_{i}$ is a
random sample from the $(\mu, \sigma)$ prior.
Figure~\ref{figure:mu.sigma.prior2.sim.ps} shows the simulated prior,
transformed from the points in
Figure~\ref{figure:q01.sigma.prior2.sim.ps}.  Histograms of the
individual (marginal) samples for $\mu$ and $\sigma$ are also shown.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/mu.sigma.prior2.sim.ps}
\caption{Simulated points from the joint prior and 
the corresponding marginal 
	prior distributions for $\mu$ and $\sigma$}
\label{figure:mu.sigma.prior2.sim.ps}
\end{figure}
%-------------------------------------------------------------------
Consider the sample from the joint distribution; the histogram of
the sample from the distribution of $\mu$ shows why the prior for
$\mu$ is not uniform.
\end{example}

The size of the random sample $M^{\star}$ from the posterior is random
with an expected value of
\begin{displaymath}
\E(M^{\star})=M \int f(\thetadummyvec)R(\thetadummyvec) d \thetadummyvec
\end{displaymath}
When the prior distribution and the data (i.e., relative likelihood
contours) do not agree well, $M^{\star}$ can be much less than
$M$. In such cases it may be necessary to use a very large sample
from the prior distribution.  Operationally, one can add to the
posterior by sequentially filtering groups of prior points until a
sufficient number of random values are available in the
posterior. Generally 2000 to 4000 points in the posterior provides
sufficient accuracy to get a smooth estimate of a marginal
distribution for a scalar quantity.

\begin{example}
\label{example:bearing.cage.post.compute}
{\bf Computation of a sample from the posterior distribution
of the parameters of the bearing-cage life distribution.} Continuing
with Example~\ref{example:bearing.cage.prior.compute}, the joint
posterior for $\thetavec=(\mu, \sigma)$, is generated 
by using Algorithm~\ref{algorithm:bayes.mc.sim}.
Figure~\ref{figure:bcage.weib.cont.ps} shows the same $(\mu, \sigma)$
prior given in Figure~\ref{figure:mu.sigma.prior2.sim.ps} with the
bearing-cage data relative likelihood contours superimposed.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/bcage.weib.cont.ps}
\caption{Simulated points from the joint 
prior distribution for $\mu$ and $\sigma$ with the bearing-cage data
Weibull relative likelihood contours.}
\label{figure:bcage.weib.cont.ps}
\end{figure}
%-------------------------------------------------------------------
Figure~\ref{figure:joint.postx.bcage.ps} shows a sample of 500 points
from the $(\mu, \sigma)$ posterior, showing the effect of the
filtering illustrated in Figure~\ref{figure:bcage.weib.cont.ps}.
Over 4000 points were actually computed to provide 
the Monte Carlo approximation to the posterior distribution.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/joint.postx.bcage.ps}
\caption{Simulated points from the joint posterior distribution 
	for $\mu$ and $\sigma$ for the bearing-cage data.}
\label{figure:joint.postx.bcage.ps}
\end{figure}
\end{example}

%----------------------------------------------------------------------
\subsection{Marginal posterior distributions}
Inferences on individual parameters are obtained by
using the marginal posterior distribution of the parameter of
interest.
Mathematically, the marginal posterior pdf of $\theta_{j}$ is
\begin{displaymath}
f[\thetavec_{j} | \DATA]=
\int f( \thetavec | \DATA) d\thetavec_{[j]}
\end{displaymath}
where $\thetavec_{[j]}$ is the vector $\thetavec$ with $\theta_{j}$
omitted.  Using the general resampling method described above, one
gets a sample from the posterior distribution for $\thetavec$, say
$\thetavec_{i}^{\star}= (\mu_{i}^{\star}, \sigma_{i}^{\star})$, $i=1,
\ldots, M^{\star}$ 
and use this to approximate $f[\thetavec_{j} | \DATA]$.  Then, for
example, inferences for $\mu$ or $\sigma$ alone are based on the
corresponding ``marginal'' distributions of $\mu_{i}^{\star}$ and
$\sigma_{i}^{\star}$, respectively.

\begin{example}
{\bf Marginal distribution for  $\mu$ and $\sigma$ for the bearing-cage life
distribution.}  Figure~\ref{figure:joint.marginal.postx.bcage.ps}
shows the same simulated points from the joint posterior
distribution given in Figure~\ref{figure:joint.postx.bcage.ps}, but
also provides histograms of the samples from the corresponding
marginal distributions for $\mu$ and $\sigma$.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/joint.marginal.postx.bcage.ps}
\caption{Simulated points from the joint posterior and 
corresponding marginal posterior distributions for $\mu$ and $\sigma$
for the bearing-cage life distribution.}
\label{figure:joint.marginal.postx.bcage.ps}
\end{figure}
%-------------------------------------------------------------------
\end{example}

Estimates and confidence intervals for a scalar function of the
parameters $g(\thetavec)$ are obtained by using the marginal
posterior pdf $f[g(\thetavec) | \DATA]$ and the corresponding
posterior cdf $F[g(\thetavec) | \DATA]$ of the functions.  Using the
simulation method, $f[g(\thetavec) | \DATA]$ and $F[g(\thetavec) |
\DATA]$ are approximated by using the empirical pdf and
cdf of $g(\thetavec^{\star})$, respectively.

\begin{example}
{\bf Joint posterior and marginal distributions for functions of
$\mu$ and $\sigma$ for the bearing-cage life distribution.}  The
marginal posterior distribution of $\rvquan_{p}$ is used to estimate
distribution quantiles. This marginal posterior is obtained from the
empirical distribution of $\mu_{i}^{\star}+
\Phi^{-1}_{\sev}(p) \sigma_{i}^{\star}$.
The marginal posterior distribution of $F(\estimtime)$ is used to
estimate the population fraction failing at $\estimtime$.  This
distribution is obtained from the empirical distribution of
$\Phi_{\sev} (\estimtimestd^{\star})$ where $\estimtimestd^{\star} =
[\log(\estimtime)-\mu_{i}^{\star}]/ \sigma_{i}^{\star}$.
\end{example}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Using the Posterior Distribution for Estimation}
%----------------------------------------------------------------------
\label{section:bayes.estimation}
\subsection{Bayesian point estimation}

Bayesian inference for $\thetavec$
and functions of the parameters $\gvec(\thetavec)$ 
are entirely based, respectively, on the
posterior pdfs $f(\thetavec | \DATA)$
and $f[\gvec(\thetavec)| \DATA]$.
If $g(\thetavec)$ is a
scalar, a common Bayesian estimate of
$g(\thetavec)$ is
the mean of the posterior distribution, which is given by
\begin{displaymath}
\ghat(\thetavec)=\E[g(\thetavec)| \DATA]=
\int g(\thetadummyvec) f( \thetadummyvec| \DATA) d\thetadummyvec.
\end{displaymath}
This estimate of $g(\thetavec)$ is the Bayesian estimate that
minimizes the square error loss.  Other possible choices to estimate
$g(\thetavec)$ include the mode of the posterior pdf (which is very
similar to the ML estimate) and the median of the posterior
distribution.  Such estimates are easy to compute from a simulated
sample from a posterior. In particular,
\begin{displaymath}
\ghat(\thetavec) \approx  \frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}} g(\thetavec^{\star})
\end{displaymath}
is the sample mean, the posterior median is the sample median of the
$g(\thetavec^{\star})$ values, and the mode can be obtained by finding the
maximum of a smooth density estimate of the distribution of
$g(\thetavec^{\star})$ values.

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\subsection{Bayesian interval estimation}

A $100(1-\alpha)\%$ 
Bayesian lower confidence bound (or credible bound)
for a scalar function $g(\thetavec)$ is 
value $\undertilde{g}$ satisfying $\int_{\undertilde{g}}^{\infty}
f[g(\thetadummyvec) | \DATA ] d g(\thetadummyvec)=1-\alpha$.
A $100(1-\alpha)\%$ Bayesian upper confidence bound (or credible bound)
for a scalar function $g(\thetavec)$ is value $\tilde{g}$ satisfying
$\int_{-\infty}^{\tilde{g}}
f[g(\thetadummyvec) | \DATA ] d g(\thetadummyvec)=1-\alpha$.
A $100(1-\alpha)\%$ Bayesian confidence interval (or credible interval)
for a scalar function $g(\thetavec)$ is 
any interval $[\undertilde{g}, \quad \tilde{g}]$ satisfying
\begin{equation}
\label{equation:tail.prob.const}
\int_{\undertilde{g}}^{\tilde{g}}
f[g(\thetadummyvec) | \DATA ] d g(\thetadummyvec)=1-\alpha.
\end{equation}
The two-sided interval $[\undertilde{g}, \quad \tilde{g}]$
can be chosen in different ways:
\begin{itemize} 
\item
Combining two $100(1-\alpha/2)\%$ intervals puts equal probability in
each tail (preferable when there is more concern for 
being incorrect in one direction than the other).
\item
A $100(1-\alpha)\%$ Highest Posterior Density (HPD) confidence
interval chooses $[\undertilde{g}, \quad \tilde{g}]$ to consist of
all values of $g$ with $f(g |\DATA) > c$ where $c$ is chosen such
that (\ref{equation:tail.prob.const}) holds.  HPD intervals are
similar to likelihood-based approximate confidence intervals,
calibrated with a $\chi^{2}$ quantile.
\end{itemize}

\begin{example}
\label{example:bcage.marg.post.quan}
{\bf Marginal posterior distributions for the $\rvquan_{.05}$ and
$\rvquan_{.10}$ quantiles of bearing-cage life.}
Figure~\ref{figure:quantile.marginal.postx.bcage.ps} shows the
marginal posterior distributions for the $\rvquan_{.05}$ and
$\rvquan_{.10}$ quantiles of the bearing-cage life
distribution. The figure also shows 95\% two-sided Bayesian
confidence intervals for the quantiles. These confidence intervals
were obtained by combining lower and upper one-sided 97.5\% confidence
bounds for each quantile. Numerically, the interval for
$\rvquan_{.05}$
is $[1613, \quad 3236]$ hours and for $\rvquan_{.10}$, 
the interval is $[2018, \quad 4400]$ hours.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/quantile.marginal.postx.bcage.ps}
\caption{Marginal posterior
distributions for $\rvquan_{.05}$ and $\rvquan_{.10}$
(quantiles) of bearing-cage life.}
\label{figure:quantile.marginal.postx.bcage.ps}
\end{figure}
%-------------------------------------------------------------------
\end{example}

\begin{example}
{\bf Marginal posterior distributions for $F(t)$ of
the bearing-cage life distribution.} Similar to
Example~\ref{example:bcage.marg.post.quan} and
Figure~\ref{figure:quantile.marginal.postx.bcage.ps},
Figure~\ref{figure:failure.prob.marginal.postx.bcage.ps} shows the
marginal posterior distributions for the $F(2000)$ and $F(5000)$
points on the bearing-cage life distribution, along with 95\% Bayesian
confidence intervals. Numerically, the interval for $F(2000)$ is
$[.015, \quad .097]$ and for $F(5000)$, the interval is
$[.132,  \quad .905]$.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/failure.prob.marginal.postx.bcage.ps}
\caption{Marginal posterior
distributions for $F(2000)$ and $F(5000)$
for bearing-cage life.}
\label{figure:failure.prob.marginal.postx.bcage.ps}
\end{figure}
%-------------------------------------------------------------------
\end{example}

The procedure for constructing a confidence interval
for a scalar generalizes to the computation of confidence
regions for vector functions $\gvec(\thetavec)$
of $\thetavec$.  
In particular, a
$100(1-\alpha)\%$
Bayesian confidence region (or credible region)
for a vector valued function $\gvec(\thetavec)$ is defined as
\begin{displaymath}
\mbox{CR}_{\rm B}=
\left \{\gvec(\thetavec) | f[ g |\DATA] \ge c
 \right \}
\end{displaymath}
where $c$ is chosen such that 
\begin{displaymath}
\int_{\mbox{CR}_{\rm B}} 
f[\gvec(\thetadummyvec) | \DATA] d \gvec(\thetadummyvec)=1-\alpha.
\end{displaymath}
The presentation of the confidence region
is difficult when $\thetavec$ has more than 2 components.

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Bayesian Prediction}

Bayesian methods are also useful for predicting a future event
like the failure of a unit from a specified population or a process.
Future events can be predicted by using the Bayesian posterior predictive
distribution.

%----------------------------------------------------------------------
\subsection{Bayesian posterior predictive distribution}
If $X$ [with pdf $f(x|\thetadummyvec)$]
represents a future random variable, then
the posterior predictive pdf of $X$ is
\begin{equation}
\label{equation:bayes.pred.pdf}
f(x | \DATA)=
\int f(x| \thetadummyvec) f(\thetadummyvec | \DATA)
d \thetadummyvec = \E_{\thetavec|\DATA} \left [f(x| \thetavec)\right ].
\end{equation}
The corresponding posterior predictive cdf of $X$ is
\begin{equation}
\label{equation:bayes.pred.cdf}
F(x | \DATA)=\int_{-\infty}^{x} f(u| \thetadummyvec) d u
=
\int F(x| \thetadummyvec) f(\thetadummyvec |\DATA)
d \thetadummyvec=
\E_{\thetavec|\DATA} \left [F(x|\thetavec) \right ].
\end{equation}
Both posterior predictive distributions
are expectations computed with respect to the
posterior distribution of $\thetavec$.

%----------------------------------------------------------------------
\subsection{Approximating posterior predictive distributions}

Using the simulation approach described in 
Section~\ref{section:sim.bayes.comp},
the Bayesian posterior predictive pdf can be  approximated
by the average of the posterior pdfs
$
f(x| \thetavec_{i}^{\star})
$ using
\begin{equation}
\label{equation:bayes.pred.pdf.sim}
f(x|\DATA) \approx \frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}}
f(x| \thetavec_{i}^{\star}).
\end{equation}
Similarly,
the Bayesian posterior predictive cdf can be  approximated
by the average of the posterior cdfs
$
F(x| \thetavec_{i}^{\star}).
$ 
In particular,
\begin{equation}
\label{equation:bayes.pred.cdf.sim}
F(x|\DATA) \approx 
\frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}}
F(x | \thetavec_{i}^{\star}).
\end{equation}
A two-sided $100(1-\alpha)\%$ Bayesian prediction interval for a new
observation is given by the $\alpha/2$ and $(1-\alpha/2)$ quantiles of
$F(x|\DATA)$.  A one-sided $100(1-\alpha)\%$ lower (upper)
Bayesian prediction bound
for a new observation is given by the
$\alpha$ quantile [or the $(1-\alpha)$ quantile] of $F(x|\DATA)$.

%----------------------------------------------------------------------
\subsection{Prediction of an observation from a log-location-scale
distribution}
This section describes prediction of a future observation
$T$ that has a log-location-scale
distribution (extension to other distributions with
shape parameters is straightforward, but the notation
becomes more complicated).
Using $X=\rv$ and $x=\realrv$,
the pdf and cdf of the observation to be predicted
(conditional on the parameters in $\thetavec$) are
\begin{equation}
\label{equation:basic.cdf}
f(\realrv|\thetavec)=\frac{1}{\sigma \realrv}\, \phi(\zeta)
\quad {\rm and} \quad
F(\realrv|\thetavec)=\Phi(\zeta)
\end{equation}
where $\zeta=[\log(\realrv)-\mu]/\sigma$.  These functions are then
averaged over the posterior distribution of $\thetavec$ using
(\ref{equation:bayes.pred.pdf}) and (\ref{equation:bayes.pred.cdf}), giving
the posterior predictive pdf and cdfs. To implement
with the simulation
method, substitute instead into
(\ref{equation:bayes.pred.pdf.sim}) and
(\ref{equation:bayes.pred.cdf.sim}).
\begin{example}
{\bf Predictive distributions for a bearing-cage failure.}
Continuing with the bearing-cage fracture example, the Weibull distribution
Monte Carlo approximations for the posterior predictive pdf and cdf are
\begin{eqnarray*}
f(\realrv |\DATA) & \approx & \frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}}
\frac{1}{\sigma_{i}^{\star} \realrv} \phi_{\sev}
\left [
\frac{\log(\realrv)-\mu_{i}^{\star}}
     {\sigma_{i}^{\star}}
\right ]
\\
F(\realrv|\DATA) & \approx & \frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}}\Phi_{\sev}
\left [
\frac{\log(\realrv)-\mu_{i}^{\star}}
     {\sigma_{i}^{\star}}
\right ].
\end{eqnarray*}
The posterior predictive pdf $f(\realrv | \DATA)$ for the failure time of a
bearing-cage is shown in Figure~\ref{figure:pred.postx.bcage.ps},
along with a 95\% prediction interval. Numerically, the interval is
[\text{1,618}, \quad  \text{13,500}] hours.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/pred.postx.bcage.ps}
\caption{Posterior predictive pdf and Bayesian 95\% 
prediction interval for a future observation from the bearing-cage
population.}
\label{figure:pred.postx.bcage.ps}
\end{figure}
%-------------------------------------------------------------------
\end{example}

%----------------------------------------------------------------------
%----------------------------------------------------------------------

\subsection{Posterior predictive distribution for the $k$th failure from 
a future sample of size $m$}
\label{section:pred.dist.order.stat}
It is often of interest to predict the $k$th failure (or more
generally the $k$th order statistic) in a future sample of size $m$.
We will illustrate how to do this when the distribution of time $\rv$
has a log-location-scale distribution. We let $\rv_{(k)}$
denote the $k$th largest observation for a sample of size $m$ from the
distribution of $T$.  In this case, $X=\rv_{(k)}$ and
$x=\realrv_{(k)}$. Then the pdf for $\rv_{(k)}$, conditional on
$\thetavec$, is
\begin{equation}
\label{equation:pred.pdf.order}
f[\realrv_{(k)}|\thetavec]=
\frac{m!}{(k-1)!\, (m-k)!}\times \left [\Phi\left (\zeta\right )\right ]^{k-1}
\times
\frac{1}{\sigma \realrv_{(k)}} \phi\left (\zeta\right )
\times
\left [1-\Phi\left (\zeta\right )\right ]^{m-k}
\end{equation}
where $\zeta=[\log(\realrv_{(k)})-\mu]/\sigma$.  
The corresponding cdf of $\rv_{(k)}$, conditional on
$\thetavec$, is
\begin{equation}
\label{equation:pred.cdf.order}
\Pr[ \rv_{(k)} \leq \realrv_{(k)}  | \thetavec  ] = F[\realrv_{(k)}|\thetavec]=
\sum_{j=k}^{m}
\frac{m!}{j!\,(m-j)!}\left [\Phi\left (\zeta\right )\right ]^{j}
\times
\left [1-\Phi\left (\zeta\right )\right ]^{m-j}.
\end{equation}
Substituting (\ref{equation:pred.pdf.order}) and 
(\ref{equation:pred.cdf.order} ) into
(\ref{equation:bayes.pred.pdf.sim}) 
and (\ref{equation:bayes.pred.cdf.sim}) 
provides the following  expressions for the
posterior predictive pdf and cdf of $\rv_{(k)}$, respectively,
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{eqnarray*}
f[\realrv_{(k)} |\DATA] & \approx & \frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}}
\left \{
\frac{m!}{(k-1)!\, (m-k)!}\times \left [\Phi\left (\zeta^{\star}_{i}\right )\right ]^{k-1}
\times
\frac{1}{\sigma_{i}^{\star} \realrv_{(k)}} \phi\left (\zeta^{\star}_{i}\right )
\times
\left [1-\Phi\left (\zeta^{\star}_{i}\right )\right ]^{m-k}
\right \}
\\
F[\realrv_{(k)}|\DATA]
 &\approx  & \frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}}
\left \{
\sum_{j=k}^{m}
\frac{m!}{j!\,(m-j)!}\left [\Phi\left (\zeta^{\star}_{i}\right )\right ]^{j}
\times
\left [1-\Phi\left (\zeta^{\star}_{i}\right )\right ]^{m-j}
\right \}.
\end{eqnarray*}
where
$F[\realrv_{(k)}|\DATA]=
\Pr[ \rv_{(k)} \leq \realrv_{(k)}  | \DATA  ]
$ and
$
\zeta^{\star}_{i}=[\log(\realrv_{(k)})-\mu_{i}^{\star}]/
      \sigma_{i}^{\star}.
$


%-------------------------------------------------------------------
\begin{example}
{\bf Posterior predictive distributions for the first failure from 
a group of 50 of bearing-cages.}
%-------------------------------------------------------------------
Fifty bearings have been put into service.  A prediction interval is
needed for the time (measured in hours of operation) of the first
failure from this group. Using $m=50$ and the simulated
$\mu_{i}^{\star}$ and $\sigma_{i}^{\star}$ values from
Example~\ref{example:bearing.cage.post.compute}, the posterior
predictive pdf for the first order statistic is obtained as
follows. 
First, with
$k=1$ the pdf of
$\rv_{(k)}$ in (\ref{equation:pred.pdf.order}) simplifies to
\begin{displaymath}
f[\realrv_{(1)}|\thetavec]=
	m \times \frac{1}{\sigma \realrv_{(1)}} 
	\phi\left (\zeta\right )\times
	\left [1-\Phi\left (\zeta\right )\right ]^{m-1}
\end{displaymath}
where $\zeta=[\log(\realrv_{(1)})-\mu]/\sigma$.
The corresponding cdf for $\rv_{(1)}$, the first order statistic, is
\begin{displaymath}
\Pr[\rv_{(1)} \le \realrv |\thetavec]=F[\realrv_{(1)}|\thetavec] =
1-\left [1-\Phi\left (\zeta\right )\right ]^{m}.
\end{displaymath}
Thus the  posterior predictive pdf for $\rv_{(1)}$ is
\begin{eqnarray*}
f[\realrv_{(1)} |\DATA] & \approx & \frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}}
\left \{m \times 
\frac{1}{\sigma_{i}^{\star} \realrv_{(1)}} \phi\left (\zeta^{\star}_{i}\right )
\times
\left [1-\Phi\left (\zeta^{\star}_{i}\right )\right ]^{m-1}\right \}
\end{eqnarray*}
where
$
\zeta^{\star}_{i}=[\log(\realrv_{(1)})-\mu_{i}^{\star}]/
      \sigma_{i}^{\star}.
$
The corresponding posterior predictive cdf for $\rv_{(1)}$ is
\begin{displaymath}
\Pr[\rv_{(1)} \le \realrv|\DATA]=F[\realrv_{(1)}|\DATA]  \approx  \frac{1}{M^{\star}} \sum_{i=1}^{M^{\star}}
\left \{1-\left [1-\Phi\left (\zeta^{\star}_{i}\right )\right ]^{m}\right \}.
\end{displaymath}
The posterior predictive pdf $f[\realrv_{(1)} |\DATA]$ for the first
failure time out of a sample of size $m=50$ bearing cages is shown
in Figure~\ref{figure:pred.first.fail.bcage.ps}, along with a 95\%
prediction interval. Numerically, the interval is [401, \quad 2771]
hours.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/pred.first.fail.bcage.ps}
\caption{Posterior predictive pdf and Bayesian 95\% 
prediction interval for the first failure from a future sample of 50
bearing cages.}
\label{figure:pred.first.fail.bcage.ps}
\end{figure}
%-------------------------------------------------------------------
\end{example}


%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Practical Issues in the Application of Bayesian Methods}
%----------------------------------------------------------------------
\subsection{Comparison between Bayesian and 
likelihood/frequentist statistical methods}

One of the most important differences between Bayesian methods and
the likelihood method of making inferences is the manner in which
nuisance parameters are handled.  Bayesian interval inference
methods are based on marginal distribution in which nuisance
parameters have been integrated out and parameter uncertainty can be
interpreted in terms of probabilities from the marginal posterior
distribution.  In the profile likelihood method, on the other hand,
nuisance parameters can be maximized out, as suggested by large-sample
theory.  Confidence intervals based on likelihood and profile
likelihood functions can be calibrated and interpreted in terms of
repeated-sampling coverage probabilities (as described in
Section~\ref{section:confidence.intervals}).  In large samples
(where the likelihood and posterior are approximately symmetric),
Bayesian and likelihood/frequentist confidence interval methods give
very similar answers when prior information is approximately
uninformative.

%----------------------------------------------------------------------
\subsection{Cautions on the use of prior information}
\label{section:cautions.on.prior}
In many applications, engineers really have useful, indisputable prior
information (e.g., information from physical theory or past experience
deemed relevant through engineering or scientific knowledge). In such
cases, the information should be integrated into the analysis.
Analysts and decision makers must, however, beware of and avoid the
the use of ``wishful thinking'' as prior information.  The potential
for generating seriously misleading conclusions is especially high
when experimental data will be limited and the prior distribution will
dominate in the final answers (common in engineering applications).
Evans~(1989) describes such concerns from an engineering point of
view.

As with other analytical methods, when using Bayesian statistics, it
is important to do sensitivity analyses with respect to uncertain
inputs to one's model.  For some model/data combinations, Bayes'
estimates and confidence bounds can depend entirely on prior
assumptions.  This possibility can be explored by changing prior
distribution assumptions and checking the effect that the changes
have on final answers of interest.

%----------------------------------------------------------------------
%----------------------------------------------------------------------

\section*{Bibliographic Notes}
Lindley~(1972) describes the basic ideas and philosophy of Bayesian
inference. Singpurwalla~(1988b) explains reasons for using Bayesian
methods in reliability and risk analysis applications. Box and
Tiao~(1973) present procedures for applying Bayesian methods to a
wide range of commonly used statistical models, including analysis
of variance and regression analysis. Martz and Waller~(1982) apply
Bayesian methods to numerous applications in reliability and risk
assessment.  Gelman, Carlin, Stern, and Rubin~(1995) provide a
comprehensive treatment of modern methods of Bayesian modeling and
computation and illustrate how to apply the methods in an impressive
array of applications.  Singpurwalla~(1988a) presents methods and a
computer program for eliciting prior information from experts.
Smith and Gelfand~(1992) describe simple Monte Carlo methods for
doing Bayesian computations, including the method used in this
chapter. Gelfand and Smith~(1990) describe and compare three more
sophisticated Monte Carlo methods (data augmentation, Gibbs
sampling, and importance sampling) that can be used to compute
marginal posterior distributions.  Gelfand and Smith~(1992) apply
the Gibbs sampling methods to constrained parameter and censored
data problems. Severini~(1991) describes the relationship between
Bayesian and Non-Bayesian confidence intervals. Smith and
Naylor~(1987) compare maximum likelihood and Bayesian methods for
estimating the parameters of a 3-parameter Weibull (see
Section~\ref{section:threshold.dist.ml}), providing a convincing
example of the potential difference between Bayesian and likelihood
methods.  Hamada and Wu~(1995) show how to use Bayesian methods to
analyze data from fractional factorials experiments when faced with
censored data.  Such experiments are sometimes used in reliability
improvement efforts. Geisser~(1993) describes Bayesian methods for
prediction.

\section*{Exercises}

%--------------------------------------------
\begin{exercise}
Starting with the  traditional form of Bayes' rule, show that it can
be expressed in terms of relative likelihoods.
\end{exercise}

%--------------------------------------------
\begin{exercise1}
The Monte Carlo approach to making Bayesian inferences is
convenient intuitive, and easy to explain.
\begin{enumerate}
\item
\label{item:expected.retention.rate}
Show that the expected number of retained units using the Monte Carlo
technique is given by
\begin{displaymath}
\E(M^{\star})=M \int f(\thetavec)R(\thetavec) d \thetavec.
\end{displaymath}
Hint: the number of retained units has a binomial distribution
with parameters $M$ and success probability equal to $\Pr(U \le
R(\theta))$, where $U$ and $R(\theta))$ are independent. Then the
proof consists of showing that $\Pr(U \le R(\theta))=\int
f(\thetavec)R(\thetavec) d \thetavec$ which is the expected relative
likelihood under the prior.
\item
Use the result in part \ref{item:expected.retention.rate} to argue
that if there is agreement between the prior pdf and the likelihood then
the ratio $M^{\star}/M$ tends to be large and the prior pdf and the
posterior pdf are similar.
\item
Discuss the case in which the prior pdf and the likelihood do not agree and
indicate when the prior and the posterior pdfs tend to agree (or disagree)
in this case.
\end{enumerate}
\end{exercise1}

%--------------------------------------------
\begin{exercise}
Explain why doing a Bayesian analysis with a specified prior
distribution to estimate an unknown exponential distribution mean $\theta$ is
not a model that implies that $\theta$ varies from unit to unit in the
population or process being studied (Hint: Consider what happens to
the posterior pdf when the sample size approaches infinity).
\end{exercise}

%--------------------------------------------
\begin{exercise}
Explain how one would compute $f[g(\thetavec) | \DATA]$,
the marginal posterior pdf of the
a function of the parameters of interest,
using numerical integration. Contrast this with the simulation-based method.
\end{exercise}

%--------------------------------------------
\begin{exercise}
Discuss the advantages and disadvantages of the Monte Carlo method of
computing
the posterior distribution, relative to the use of numerical integration.
\end{exercise}

%--------------------------------------------
\begin{exercise}
A total of 100 new units have been introduced into service.  It is
believed that the underlying time to failure distribution is Weibull.
The analysts have available prior information and censored data on a
sample of $n$ similar units.
\begin{enumerate}
\item
Given a Monte Carlo sample of 2000 pairs of $\mu^{*},\sigma^{*}$
values from the posterior pdf $f(\mu,\sigma | \DATA)$, show how to
compute marginal posterior predictive distribution for $T_{(1)}$,
the time of the first failure out of the 100 units. Also show how to
get a 95\% lower Bayesian prediction bound for $T_{(1)}$.
\item
Suppose that you can compute (but perhaps not write in closed form)
the joint posterior pdf $f(\mu,\sigma |\DATA)$. Explain how
you could, using numerical integration and other numerical (but not
simulation) methods, compute marginal posterior predictive
pdf  and a 95\% lower Bayesian prediction bound for $T_{(1)}$.
\end{enumerate}
\end{exercise}

%--------------------------------------------
\begin{exercise1}
\label{exercise:lognormal.bayes}
The model for failure time $T$ of an electronic component is
$\LOGNOR(\mu,\sigma)$ with $\mu$ unknown and $\sigma$ known.  The
uncertainty in $\mu$ can be described by a $\NOR(a_{1},b_{1})$
distribution where the prior distribution parameters $a_{1}$ and
$b_{1}$ are also known.  Suppose that a sample of size $n$ will be
used, along with the prior information, to make inferences on $\mu$
(and thus other quantities of interest like quantiles of the
distribution of $T$). The sample will provide a realization of
$\DATA=(T_{1},\ldots, T_{n})$.
\begin{enumerate}
\item
Find the conditional pdf of
$\Ybar = \frac{1}{n}\sum_{i=1}^{n} Y_{i} 
	=  \frac{1}{n}\sum_{i=1}^{n} \log(T_{i})$
for a specified fixed value of $\mu$ (which, when viewed as a function
of $\mu$ for fixed $\DATA$, is also the likelihood).
\item
Combining the variability in $\Ybar$ with the uncertainty in $\mu$,
find the joint pdf of $\mu$ and $\Ybar$. 
\item
Show that the marginal distribution of $\Ybar$ is $\NOR \left( a_{1},
\sqrt{\sigma^{2}/n+b_{1}^{2}} \, \right) $.  What is the practical interpretation
of this distribution, relative to this application?
\item
\label{part:post.of.mean}
Show that, for a given $\DATA$ realization, the posterior distribution
of $\mu$ is normal with mean and variance
\begin{eqnarray*}
\E(\mu | \DATA) &=&  \frac{\sigma^{2}/n   }{b_{1}^{2} 
	+ \sigma^{2}/n } \times a_{1}
	+\frac{b_{1}^{2}}{b_{1}^{2} 
	+ \sigma^{2}/n } \times \Ybar 
   \\[2ex]
   \var(\mu | \DATA) &=& \frac{\sigma^{2}/n   }{b_{1}^{2} 
	+ \sigma^{2}/n }\times b_{1}^{2}.
\end{eqnarray*}
\item
Consider what happens to the expressions in
part~\ref{part:post.of.mean}
as $n \rightarrow \infty$.
\item
Consider the expression for $\var(\mu | \DATA)$ in
part~\ref{part:post.of.mean}. Use this expression to explain how prior
information might be related quantitatively to information from a
previous sample of a certain size (note that this pseudo-sample size
does not have to be an integer).
\end{enumerate}
\end{exercise1}

%--------------------------------------------
\begin{exercise}
Refer to
Exercise~\ref{exercise:lognormal.bayes}.
Derive a simple expression for the posterior predictive pdf of $T$
given $\DATA$.
\end{exercise}

%--------------------------------------------
\begin{exercise}
Consider the following quantities used in
Exercise~\ref{exercise:lognormal.bayes}: $\Ybar$, $\mu$, $a_{1}$, and
$b_{1}$. 
\begin{enumerate}
\item
Use the setting of Exercise~\ref{exercise:lognormal.bayes}
and these quantities to explain the important differences between
variability and uncertainty in physical processes and statistical
inference.
\item
Explain how one could generalize the model in
Exercise~\ref{exercise:lognormal.bayes} to allow for batch-to-batch
variability in the reliability of the electronic component and
describe a sampling plan that could be used to combine prior
information with data to estimate the parameters of this more general
model.
\end{enumerate}
\end{exercise}

%--------------------------------------------
\begin{exercise1}
\label{exercise:alt.prior.bcage}
When the prior pdf for $\log(\rvquan_{p})$ is uniform,
\begin{displaymath}
f[\log(\rvquan_{p})]=\frac{1}{\log(b_{1}/a_{1})}, \quad 
a_{1} \le \rvquan_{p} \le b_{1}.
\end{displaymath}
When the prior pdf for $\log(\sigma)$ is triangular
\begin{displaymath}
f[\log(\sigma)]=
\left \{
\begin{array}{lcrcl}
\frac{4[\log(\sigma/a_{2})]}
     { [\log(b_{2}/a_{2})]^{2}}& \mbox{if}& a_{2} \le& \sigma &\le \sqrt{a_{2} b_{2}}
\\[1ex]
\frac{4[\log(b_{2}/\sigma)]}
     {[\log(b_{2}/a_{2})]^{2}}& \mbox{if}& \sqrt{a_{2} b_{2}} < & \sigma &\le  b_{2}
\end{array}.
\right.
\end{displaymath}
Under these specifications, 
\begin{enumerate}
\item
Show that the prior pdf for $\sigma$ is
$
f(\sigma)=(1/\sigma) \, f[\log(\sigma)].
$
\item
Show that the joint pdf for $(\rvquan_{p}, \sigma)$ is
\begin{displaymath}
f(\rvquan_{p}, \sigma)=
\frac{f[\log(\rvquan_{p})]\, f[\log(\sigma)]}{\rvquan_{p}\, \sigma},
\quad 
a_{1} \le \rvquan_{p} \le b_{1}, \,\, a_{2} \le \sigma \le  b_{2}.
\end{displaymath}
\item
Use the transformation $\mu=\log(\rvquan_{p})-\Phi^{-1}_{\sev}(p) \sigma, \sigma=\sigma$
to show that the joint prior pdf for
$(\mu, \sigma)$ is
\begin{displaymath}
f(\mu, \sigma) = f\left [\log \left (\rvquan_{p}\right ) \right ]\, \,\frac{
 f[\log(\sigma)]}{ \sigma}=
\frac{1}{\log(b_{1}/a_{1})}\, \, \frac{
 f[\log(\sigma)]}{ \sigma}
\end{displaymath}
where 
$
\log(a_{1})-\Phi^{-1}_{\sev}(p) \sigma \le   \mu   \le \log(b_{1})-\Phi^{-1}_{\sev}(p) \sigma,
$
and
$
a_{2} \le   \sigma \le  b_{2}.
$
\item
Show that the region in which $f(\mu, \sigma)>0$
is South-West to North-East oriented.
\end{enumerate}
\end{exercise1}

%--------------------------------------------
\begin{exercise1}
For the prior pdf for $(\rvquan_{p}, \sigma)$ 
given in Exercise~\ref{exercise:alt.prior.bcage}, show the following:
\begin{enumerate}
\item
The prior cdf for $\rvquan_{p}$ is
\begin{displaymath}
F(\rvquan_{p})=\frac{\log(\rvquan_{p}/a_{1})}{\log(b_{1}/a_{1})}, \quad 
a_{1} \le \rvquan_{p} \le b_{1}.
\end{displaymath}
\item
The prior cdf for $\sigma$ is
\begin{displaymath}
F(\sigma)=
\left \{
\begin{array}{lcrcl}
\frac{2[\log(\sigma/a_{2})]^{2}}
     { [\log(b_{2}/a_{2})]^{2}}& \mbox{if}& a_{2} \le& \sigma &\le \sqrt{a_{2} b_{2}}
\\[1em]
1-\frac{2[\log(b_{2}/\sigma)]^{2}}
     {[\log(b_{2}/a_{2})]^{2}}& \mbox{if}& \sqrt{a_{2} b_{2}} < & \sigma &\le  b_{2}
\end{array}.
\right.
\end{displaymath}\item
Invert these cdfs to verify that the 
the following formulas provide 
random numbers from the prior $(\rvquan_{p},\sigma)$.
\begin{displaymath}
(\rvquan_{p})_{i}=a_{1}\times (b_{1}/a_{1})^{U_{1i}},
\quad i=1, \ldots, M
\end{displaymath}
where $U_{11}, \ldots, U_{1M}$ is a random sample
from a $\UNIF(0,1)$.
\item
For $\sigma$, 
\begin{displaymath}
\sigma_{i}=
\left \{
\begin{array}{lcl}
a_{2} \times \left (b_{2}/a_{2}\right )^{\sqrt{U_{2i}/2}}
& \mbox{if}& U_{2i} \le 1/2
\\[1em]
b_{2} \times \left (a_{2}/b_{2}\right )^{\sqrt{(1-U_{2i})/2}}
& \mbox{if}& U_{2i} > 1/2
\end{array}
\right.
\quad i=1, \ldots, M
\end{displaymath}
where $U_{21}, \ldots, U_{2M}$ is another
independent random sample
from a $\UNIF(0,1)$.
\item
Then $\thetavec_{i}=(\mu_{i}, \sigma_{i})$ with
$\mu_{i}=\log[(\rvquan_{p})_{i}]-\Phi^{-1}_{\sev}(p)\sigma_{i}$
is a random sample from the $(\mu, \sigma)$ prior distribution.
\end{enumerate}
\end{exercise1}

 
%--------------------------------------------
\begin{exercise1}
Consider the prior pdfs $f[\log(\sigma)]$
and $f(\sigma)$.
\begin{enumerate}
\item
Plot the pdf $f[\log(\sigma)]$ in a log-scale 
for the following choices of the parameters:
$[a_{2}, b_{2}]=[1,7], [1, 10]$.
\item
For the same choices of $[a_{2}, b_{2}]$
plot the pdf $f(\sigma)$ in an arithmetic 
scale.
\item
Show analytically that the mode of $f[\log(\sigma)]$ occurs at 
$\sigma=\sqrt{a_{2}b_{2}}$.
\item 
Show, however, that the mode of $f[\log(\sigma)]$
occurs at $\sigma=\sqrt{a_{2}b_{2}}$ if 
$\exp(1) \ge \sqrt{b_{2}/a_{2}}$ and at 
$\sigma=a_{2} \exp(1)<\sqrt{a_{2}b_{2}}$ otherwise.
\item
Indicate the modes for the graphs in 
parts~a and b above. Explain what you observe on the
plots.
\end{enumerate}
\end{exercise1}

\begin{exercise1}
Consider the pdf $f[\realrv_{(k)}|\thetavec]$
and cdf $F[\realrv_{(k)}|\thetavec]$
for the 
$k$th largest observation
given in Section~\ref{section:pred.dist.order.stat}.
\begin{enumerate}
\item
Integrate the pdf $f[\realrv_{(k)}|\thetavec]$
to obtain the cdf $F[\realrv_{(k)}|\thetavec]$.
(Hint: use integration by parts).
\item
Use general formula given for $F[\realrv_{(k)}|\thetavec]$
to derive the predictive cdf
when $k=1$ given in Section~\ref{section:pred.dist.order.stat}.
(Hint: the general formula
is a sum of binomial probabilities).
\end{enumerate}
\end{exercise1}

\begin{exercise}
Show that for the special case $k=m=1$,
(\ref{equation:pred.pdf.order}) and  (\ref{equation:pred.cdf.order})
reduce to the pdf and cdf given in (\ref{equation:basic.cdf}).
\end{exercise}

