---
title: "`r SMRD:::info('book')`"
subtitle: "`r SMRD:::info('chapter3')`"
author: "`r SMRD:::info('authors')`"
date: "`r format(Sys.Date(), '%d %B %Y')`"
footer: "`r paste('SMRD: ', SMRD:::info('chapter3'))`"
output:
  slidy_presentation:
    smart: false
    fig_caption: yes
graphics: yes
---

```{r echo=FALSE,message=FALSE, warning=FALSE, jkf_par=TRUE}
source('scripts/R/setup.R')
library(DT)
library(SMRD)
```

# OVERVIEW

```{r}
shiny::includeCSS('scripts/css/flat-slidy.css')
shiny::includeScript(path = "scripts/js/audiojs/audiojs/audio.min.js",
                     "audiojs.events.ready(function() {audiojs.createAll();});")
shiny::includeScript("scripts/js/jquery.min.js")
shiny::includeScript("scripts/js/jkf-scroll.js")
```

## This chapter explains...

- Statistical methods, based on the binomial distribution, to estimate $F(t)$ from interval & singly right-censored data, without assuming an underlying parametric distribution.

- Standard errors of the nonparametric estimator and approximate confidence intervals for $F(t)$.

- Methods to extend nonparametric estimation to allow for combinations of interval-censored & multiply right-censored data.

- The Kaplan-Meier nonparametric estimator for data with observations reported as exact failure times

- A generalized nonparametric estimator of $F(t)$ for arbitrary censoring

# 3.1 - INTRODUCTION

## Nonparametric estimation

- At the beginning of any statistical study, it's best to allow the data to speak for itself - before assuming any parametric distribution

- Chapter 3 outlines methods to compute nonparametric estimates for 

$$\hat{F}(t), \hat{S}(t), se(\hat{F}(t))$$

- First, we'll work with singly censored data before extending to multiply censored data and then to data with arbitrary censoring

- The goal will be to compute $(1-\alpha)\times 100\%$ confidence intervals on point estimates for the true, but unknown, values

## Nonparametric techniques

- Can be used when we don't know the underlying distribution that governs the failure process that produced the data

- Are common in survivability (biostatistics and actuarial sciences)

- Are gaining traction in reliability studies

- Can simplify analyses by quickly revealing poor-fitting parametric models

- Can be used as a benchmark to compare accuracy of parametric analyses

- Cannot be used for prediction outside the range of observed data

# 3.2 - ESTIMATION FROM SINGLY CENSORED DATA

## Singly censored data 

- Tests with only one censoring event occurring at time $t_c$ 

- For a <red>singly right censored observation</red>, the actual failure time is unknown because the unit had not failed when the censoring event occurs

- A right censored observation results when testing ends before a failure is observed for a test unit 

    + Product-to-market schedule limits the allocated test time for a new mobile device
    
    + Non-safety critical aircraft components replaced at each depot cycle whether or not they have failed
    
```{r, echo=FALSE,fig.width=13,fig.align='center', fig.height=1.5}
par(mar = c(0,0,0,0)) ; set.seed(42)
plot(NA,
     axes = F,
     xlab = "",
     ylab = "",
     xlim = range(0,105),
     ylim = range(16,18))
segments(x0 = c(0,70),
         y0 = c(18,18),
         x1 = c(0,70),
         y1 = c(17,17),
         lwd = 2)
arrows(x0 = 0,
       y0 = 17.5,
       x1 = 100,
       y1 = 17.5,
       lwd = 2)
text(x = c(0,70,104),
     y = c(16.8,16.8,17.5),
     labels = c(0,
                expression(t[c]),
                expression(infinity)),
     cex=c(1.5,1.5,2))
text(x = c(runif(20,5,65),runif(5,75,95)),
     y = c(rep(17.5,25)),
     labels = c(rep("x",20),rep("?",5)), 
     col = c(rep(1,20),rep(2,5)), 
     cex = 2)
```
    
- For a <blue>singly left censored observation</blue> the actual failure time is unknown because the unit had already failed when the censoring event occurs

- Left censored observations can result when it is difficult or expensive to inspect samples, if the test was set up improperly, or if an unknown flaw causes an infant mortality failure

    - Tests conducted in remote locations (polar regions, space, desert) 
    
    - Inspection requires extensive maintenance to gain access

```{r, echo=FALSE,fig.width=13,fig.align='center', fig.height=1.5, jkf_par=TRUE}
par(mar = c(0,0,0,0)) ; set.seed(42)
plot(NA,
     axes = FALSE,
     xlab = "",
     ylab = "",
     xlim = c(0,105),
     ylim = c(16,18))
segments(x0 = c(0,70),
         y0 = c(18,18),
         x1 = c(0,70),
         y1 = c(17,17),
         lwd = 2)
arrows(x0 = 0,
       y0 = 17.5,
       x1 = 100,
       y1 = 17.5,
       lwd = 2)
text(x = c(0,70,104,runif(25,5,65)),
     y = c(16.8,16.8,17.5,rep(17.5,25)),
     labels = c(0,expression(t[c]),
                expression(infinity),
                rep("?",25)),
     cex = c(1.5,1.5,rep(2,26)),
     col = c(1,1,rep('blue',26)))
```

- We'll focus on tests that produce <red>singly right censored observations</red> as they often occur in practice

- Tests that produce <blue>singly left censored observations</blue> rarely occur since no failures would have been observed

## Examples 3.1 & 3.2 - Plant 1 Heat Exchanger Data

```{r, out.width='100%'}
teachingApps::includeApp('example3_1', height = '800px')
```

# 3.2 - ESTIMATION FROM SINGLY CENSORED DATA

## Recall, the cdf $F(t)$ is defined everywhere in $\mathbb{R}$, i.e. $(-\infty, \infty)$

- <focus>But,</focus> if inspections occur in discrete intervals, $F(t)\in (t_{i-1},t_{i})$ may be unknown

    + If no failures occur in $(t_{i-1},t_{i})$, $F(t)$ is constant 

    + If one or more failures occur in $(t_{i-1},t_{i})$, we can estimate $F(t_{i})-F(t_{i-1})$ But we won't know when the failures occured in the interval

- Convention for this text - there is insufficient information to assign a value of $\hat{F}(t)\in (t_{i-1},t_{i})$ for censored observations occuring between inspections

# 3.3.2 - Confidence Intervals

## Sampling errors

- We usually can't inspect every unit in a population, but we can inspect a random sample taken from the population

- Regardless of how the sample was drawn, sampling errors will be present

- The amount of sampling error depends on the size of the sample (relative to the size of the population) 

- For small samples

- As the sample size increases the properties of the sample approach the properties of the overall population
    
    + The sampling errors become smaller 

<div class='notes'>
In the heat exchanger example, if the number of tubes at each Plant 1 were 10 rather than 100, and we used the observations from Plant 1 as the basis for our estimate of the reliability for the overall population of heat exchanger tubes at every plant we should expect our estimate to have a lot more uncertainty.

Some of the tubes may experience higher usage.  And if our sample of 10 tubes included several of these tubes might could conclude that the reliability of the tubes is lower than reality. By increasing the size of the sample we can more better account for any variability that may exist.
</div>

## Sources of uncertainty

- Limited understanding of how a system interacts with its environment

- Manufacturing flaws & deviations between units in a sample

- Inability of chosen parametric model to fit data

- Small sample sizes amplify the uncertainty 

- Confidence intervals (CI) are useful tools for quantifying the uncertainy associated with a point estimate due to sampling errors

<div class='example'>
### Example - Understanding Confidence Intervals

- What does it mean to say "...the $100\times(1-\alpha)\%$ CI for $\theta =(\underline{\hat{\theta}},\overline{\hat{\theta}})$..."

- For $\alpha=0.05,$ a $95\%$ CI on $\theta$ this means 

    + If an experiment were run $i$ times (each run producing a distinct data set) 

    + The true (but unknown) value of $\theta$ would be in the interval $(\underline {\hat {\theta_{i}}},\overline{\hat{\theta_{i}}})$ in $95\%$ of those runs.

- A $95\%$ CI on $\theta$ <focus>does not mean</focus> $P(\underline{\hat{\theta_{i}}} <\theta<\overline{\hat{\theta_{i}}})=0.95$

    + For each experiment the probability $P(\underline{\hat{\theta_{i}}} <\theta<\overline{\hat{\theta_{i}}})$ is either $1$ or $0$ 

    + This is because $\theta$ is either in the interval, or it isn't


- This Shiny app illustrates this process of generating many confidence intervals

```{r, out.width='100%'}
teachingApps::includeApp(height = '800px', appName = 'confidence_intervals')
```
</div>

## Exact CI vs Approximate CI

- There are many different 'classes' of values that we often want to estimate

    + Values that can be positive or negative

    + Values that are strictly positive or negative

    + Values that are defined between 0 and 1

    + Values that can only be integers

- Specific procedures have been developed to estimate a $100\times (1-\alpha)\%$ CI's for values in each of these classes 

- Most of the CI estimation procedures produce <focus>approximate</focus> confidence intervals

    + For confidence level $\alpha$, $1-\alpha$ is the <red><u>__nominal__</u></red> coverage probability

    + The <red><u>actual</u></red> proportion of runs that contain the true value being estimated is less than a $1-\alpha$

    + Approximate CI's result if the procedure used to estimate the CI is based on a different distribution than that of the value being estimated

    + Approximate CI's result in the majority of data sets that contain censored observations

- For some data sets, CI estimation procedures exist to produce exact confidence intervals

    + For confidence level $\alpha$, $1-\alpha$ is the <red><u>__nominal__</u></red> coverage probability

    + The <red><u>actual</u></red> proportion of runs that contain the true value being estimated is equal to $1-\alpha$

    + Exact CI's result if the procedure used to estimate the CI is based on a same distribution as that of the value being estimated

    + Exact CI's result for complete data sets and some data sets with Type-II right-censored observations

- <focus>Coverage probability</focus> is a measure used to compare the performance of different CI estimation procedures

# 3.4.1 - Pointwise binomial-based CI for $F(t_i)$

## Clopper-Pearson CI For Proportions

- Eq. 3.2 shows the Clopper-Pearson formulae for calculating exact binomial confidence intervals for $F(t)$ 

<smaller>
$$(\underline{F}(t_i),\overline{F}(t_i))=\left(1+\frac{(n-n\hat{F}+1)\mathcal{F}_{(1-\alpha/2,2n-2n\hat{F}+2,2n\hat{F})}}{n\hat{F}}\right)^{-1},
\left(1+\frac{n-n\hat{F}}{(n\hat{F}+1)\mathcal{F}_{(1-\alpha/2,2n\hat{F}+2,2n-2n\hat{F})}}\right)^{-1}$$
</smaller>

- Where 

    + $\mathcal{F}_{c;d_{1},d_{2}}$ is the value of the F-distribution quantile function 
    
    + $c \in [0,1]$ represents the specific quantile value 
    
    + $d_{1}$ and $d_{2}$ are the respective degrees of freedom

- Note

    + The presence of the F-distribution in (3.2) does not preclude the C-P interval from being exact 

    + Rather, the C-P interval is based on the relationship between the binomial and F distributions

<div class='example'>
### Example 3.3 - Binomial CI for $F(t_i)$

- For the Plant 1 heat exchanger data at $t_{i}=2$ we have

    + Sample size  $n=100$

    + Number of failures in $(0,2]$ is $d=3$

    + $\mathcal{F}_{_{(.975;200-6+2,6)}}=`r qf(.975,196,6)`$

    + $\mathcal{F}_{_{(.975;6+2,200-6)}}=`r qf(.975,8,194)`$

- Therefore the $95\%$ confidence interval for $F(2)$ <blue>based on the data from this inspection<blue> is expressed as

$$
\begin{aligned}
\underline{F}(2)&=\left(1+\frac{(100-100\times\frac{3}{100}+1)\mathcal{F}_{_{(.975;200-6+2,6)}}}{100\times \frac{3}{100}}\right)^{-1}=0.0062\\\\\\
\overline{F}(2)&=\left(1+\frac{100-100\times\frac{3}{100}}{(100\times\frac{3}{100}+1)\mathcal{F}_{_{(.975;6+2,200-6)}}}\right)^{-1}=0.0852
\end{aligned}
$$

```{r,echo=TRUE, results='markup'}
cp.ci<-function(n,d,a)  {
    Fhat<-d/n
    f1<-2*n-2*n*Fhat
    f2<-2*n*Fhat
    cp.lower<-(1+(n-n*Fhat+1)*qf(1-a/2,f1+2,f2)/(n*Fhat))^-1
    cp.upper<-(1+(n-n*Fhat)/((n*Fhat+1)*qf(1-a/2,f2+2,f1)))^-1
return(c(cp.lower,cp.upper))  }

cp.ci(100,3,.05)
```
</div>

# 3.4.2 - Pointwise Normal-approximate CI for $F(t_i)$

## Confidence Intervals Based on the Normal Distribution

- Are the approach used most often in this course for computing CI for $F(t_{i})$ 

- Assumes the sampling errors are normally distributed WRT the binomial point estimate, i.e.
$$Z_{\hat{F}}=\frac{\hat{F}(t_i)-F(t_i)}{\hat{se}_{\hat{F}}}\xrightarrow d N(0,1) \text{ as } n\rightarrow\infty$$

- Where 

    + $F(t_{i})$ is the true (but unknown) value of the CDF at time $t_i$

    + $\hat{F}(t_{i})=\sum_{j=1}^{i}d_{i}/n$  is the binomial point estimate of $F(t_{i})$

    + $\hat{se}_{\hat{F}}=\sqrt{\hat{F}(t_{i})[1-\hat{F}(t_{i})]/n}$ the standard error of $\hat{F}$ based on the sample

<div class='example'>
### Example 3.4 - Normal-approx CI for Plant 1 $F(t_i)$

- Consider again the heat exchanger data for Plant 1

- At the end of Year 3, five failures were observed, thus
$$\widehat{F}(3)=\frac{5}{100}=0.05 \;\;\;\text{and}\;\;\; \widehat{se}_{\hat{F}(3)}=\sqrt{\frac{.05(1-.05)}{100}}=.02179$$

- The $95\%$ CI for $F(3)$ using the normal approximation is then
$$[\underline{F(3)}, \overline{F(3)}]=.05\pm F^{-1}_{_{NOR}}(P = 0.975)\times .02179 = [.0073,.0927]$$

- Where 

<center>$F^{-1}_{_{NOR}}(P = .975)\equiv z_{.975}\equiv$ `qnorm(.975)`</center>
</div>

## Table 3.1

- Compares the CI's produced by the Binomial and the Normal approximation for the Plant 1 data over years $1, 2 \;\&\; 3$.  

- The CI's produced by the binomial method are more conservative for each time interval, particularly in the upper limit.

- The CI's produced using the Normal approximation method have negative values for the lower limit in the first and second time intervals. 

<center><red>Why?</red></center>

In years 1 and 2 the probability of failure is very low, and the $n\times p$ rule may not be met until year three when the probability of failure is increased.

<div class='example'>
### Example 3.5 - Integrated Circuit Life Test Data

## Motivation for this example

- As the number of inspections increase, the interval widths $(t_{i-1},t_{i}]$ approach zero

    + In this experiment $n=4156$ integrated circuits were tested to failure up to 1370 hours

    + We dont know how the circuits were tested, just that $d=28$ 'exact' failures were observed

## Data set `lfp1370` & Nonparametric Plots

```{r, out.width='100%'}
teachingApps::includeApp(height = '800px', appName = 'lfp1370_data')
```

- The presence of ties is a clear indication that this is grouped read-out data as defined in Chapter 1 and the failures were discovered at periodic inspections.  

- The inspection interval may have been .05 hours at the beginning of the test and then extended as the test progressed.
</div>

# 3.5 - ESTIMATION FROM MULTIPLY CENSORED DATA

## Multiple censoring results when

- A unit is removed from the test, before observing the event of interest

- An item fails due to an unexpected failure mode that is not of interest

- A test item is removed from the test to fill a real-world need

- Test items enter the test at different times

- An error affects some of the items after testing begins

- __Example:__ Pooling the observations from all three plants in the heat exchanger data

## Figure 3.4 - Pooled heat exchanger data

<div class="notes">
The pooled data set has an initial sample size of 300 tubes.  At the end of year one, 4 tubes have failed across all of the plants.  Since plant 3 was only in operation for 1 year we don't know when the 99 tubes functioning tubes would have failed, but they must be removed from the risk set.  
</div>

```{r echo=FALSE,fig.height=4.5,fig.width=8.5,fig.align='center'}
par(family="serif",font=2,bg=NA)
plot(NA,axes=FALSE,xlab="",ylab="",xlim=range(-50,350),ylim=range(-10,150))
segments( c(0,0,0,100,200,300),c(0,100,0,0,0,0),
          c(350,350,0,100,200,300),c(0,100,100,100,100,100),
            lwd=c(rep(2,6)))
text(-35,50,"All Plants",cex=1.15)
text(seq(75,275,100),rep(-8,3),c("4/300","5/197","2/97"))
text(seq(50,250,100),rep(108,3),c("Year 1","Year 2","Year 3"),cex=rep(1.15,3))
text(seq(140,340,100),rep(140,3),c("99","95","95"),cex=rep(1.15,3))
text(seq(15,215,100),rep(90,3),c("300","197","97"),cex=rep(1,3))
text(75,140,"Uncracked tubes:",cex=1.15)
arrows(seq(100,300,100),rep(100,3),seq(130,330,100),rep(125,3),length=rep(0.25,3))
```

# 3.5 - Estimation from multiply censored data

- For singly censored data, $F(t_{i})$ was estimated assuming $n$ was constant

- For multiply censored data, the number of units at risk in $(t_{i-1},t_{i}]$ is expressed as

$$n_{i}=n-\sum_{j=0}^{i-1}d_{j}-\sum_{j=0}^{i-1}r_{j}, i=1,...,m$$

- where

    + $d_{j} \equiv$ the number of failed units in interval $j, j=0,...,i-1$

    + $r_{j} \equiv$ the number of censored units in interval $j, j=0,...,i-1$

    + $m \equiv$ the number of intervals (need not be of equal length)
 
- It follows that $\hat{p}_{i}$ (the conditional probability that a unit will fail in interval $i$, given that it has survived each of the previous intervals) is expressed as

$$\hat{p}_{i}=\frac{d_{i}}{n_{i}}, i=1,...,m$$

- And
$$
\begin{aligned}
\hat{S}(t_{i})&=\prod_{j=1}^{i}\left[1-\hat{p_{j}}\right], i=1,...,m\\\\
\hat{F}(t_{i})&=1-\prod_{j=1}^{i}\left[1-\hat{p_{j}}\right], i=1,...,m
\end{aligned}
$$

## Fig 1.7 - Heat exchanger crack inspection data

```{r echo=FALSE,fig.height=5,fig.width=7.5,fig.align='center'}
par(family="serif",font=2,pin=c(6.5,5))
plot(NA,axes=FALSE,xlab="",ylab="",xlim=range(-50,350),ylim=range(-10,300))
segments( c(0,0,0,0,0,rep(0,15),100,200,300),
          c(0,100,200,300,0,seq(232,280,12),seq(132,180,12),seq(32,80,12),0,0,0),
          c(350,350,350,350,0,rep(300,5),rep(200,5),rep(100,5),100,200,300),
          c(0,100,200,300,300,seq(232,280,12),seq(132,180,12),seq(32,80,12),300,300,300),
            lwd=c(rep(2,5),rep(1,18)))
text(rep(-25,3),seq(56,256,100),c("Plant 3","Plant 2","Plant 1"),cex=rep(0.9,3))
text(c(rep(50,3),rep(150,2),250),c(90,190,290,190,rep(290,2)),
     c("1 failure","2 failures","2 failures","2 failures","3 failures","1 failure"),
     cex=rep(0.8,6))
text(seq(50,250,100),rep(-8,3),c("1981","1982","1983"),cex=rep(0.9,3))
text(c(110,210,310),seq(56,256,100),rep("95",3),cex=rep(0.9,3))
segments(c(120,220,320),seq(56,256,100),c(345,345,345),seq(56,256,100),lty=rep(2,3))
arrows(c(rep(295,5),rep(195,5),rep(95,5)),
       c(seq(232,280,12),seq(132,180,12),seq(32,80,12)),
       c(rep(300,5),rep(200,5),rep(100,5)),length=rep(0.1,15))
arrows(rep(345,3),seq(56,256,100),rep(350,3),length=rep(0.1,3))
```

## Table 3.2 - Pooled Heat Exchanger Data

```{r,echo=FALSE}
Year<-c("(0-1]","(1-2]","(2-3]")
t.i<-c(1,2,3)
d.i<-c(4,5,2)
r.i<-c(99,95,95)
n.i<-c(300,197,97)
p.i<-c("4/300","5/197","2/97")
p.i.1<-c("296/300","192/197","95/97")
S.t.i<-c(.9867,.9616,.9418)
F.t.i<-1-S.t.i
Table.3.2<-data.frame(Year,t.i,d.i,r.i,n.i,p.i,p.i.1,S.t.i,F.t.i)
colnames(Table.3.2)<-c("Year","t_i","d_i","r_i","n_i","p_i","1-p_i","S(t_i)","F(t_i)")
```

<small>
$$
`r SMRD::xarray(Table.3.2, include.rownames = FALSE, digits = c(0,0,0,0,0,0,0,0,4,4))`
$$
</small>

- Pooling the data assumes the usage pattern is consistent across all three plants

- If this is a valid assumption, then the "test" begins with an sample set of 300 tubes 

# 3.6 - POINTWISE CI FROM MULTIPLY CENSORED DATA

- For multiply censored data, no method exists to compute exact CI's 

    + We must rely on normal approximations

    + In general, the normal approximation will be of the form

$$Z_{\hat{F}}=\frac{\hat{F}(t_{i})-F(t_{i})}{\hat{se}_{\hat{F}}}$$

- Computing the standard error term in the above equation will require us to use the Delta Method

- <red>By the end of this course you should know the Delta Method in your sleep</red>

## The Delta Method (B.2)

- Suppose we can find the variance of  $\mathbf{\hat{\theta}}=\hat{\theta}_{1},...,\hat{\theta}_{r}$

- But, we want to find the variance of some function of
$\mathbf{\theta}$, say $g(\mathbf{\hat{\theta}})$

    + $g(\mathbf{\hat{\theta}})=\log[\theta]$

    + $g(\mathbf{\hat{\theta}})=\theta^2$

- If we can find $\frac{dg(\theta)}{d\theta}$, the Delta Method can help us estimate $\widehat{Var}[g(\mathbf{\hat{\theta}})]$ from $\widehat{Var}[\mathbf{\hat{\theta}}]$ 

## Using The Delta Method Requires Us To Find/Compute 4 Things

1) <red>__A parameter for which we know the variance - $\theta$__</red>

2) <purple>__The variance of the paramter - $Var[\theta]$__</purple>

2) <blue>__A function of the parameter - $g(\theta)$__</blue>

3) <green>__The partial derivatives - $\frac{\partial g(\theta)}{\partial \theta_{i}}, \;\; i=1,...m$__</green>


## The General Delta Method Equation

<small>
$$ 
Var\left[g(\hat{\theta})\right]\approx \sum_{i=1}^{r} \left[\frac{\partial g(\mathbf{\theta})}{\partial \theta_{i}}\right]^{2} Var(\hat{\theta_{i}})+\sum_{i=1}^r \mathop{\sum^{r}_{j=1}}_{i\ne j}\left[\frac{\partial g(\mathbf{\theta})}{\partial \theta_{i}}\right]\left[\frac{\partial g(\mathbf{\theta})}{\partial \theta_{j}}\right] Cov(\hat{\theta}_{i}, \hat{\theta}_{j})
$$
</small>

# 3.6.1 - Approximate Variance of $\hat{F}(t_{i})$

- The text states that using the Delta Method with $\theta=q_j$ and $g(\theta)=S(t_i)$ results in
$$\displaystyle \hat{S}(t_{i})\approx S(t_i)+ \sum_{j=1}^{i}\frac{\partial S}{\partial q_{j}}\vert_{q_{j}}(\hat{q}_{j}-q_{j})$$

- However, there's not much detail showing how this result was achieved

- The following example walks through the derivation of Equation 3.8

<div class='example'>
### Example - Derivation of Equation 3.8

## First, We Must Identify The Parameter <red>$\theta$</red>

- Our goal is to find a expression for $\widehat{Var}[\hat{F}(t_i)]$
    
    + From Equation 3.6, we see that $\hat{S}(t_i)$ is a function of $\hat{p_j}$

    + Since $\hat{F}(t_{i})=1-\hat{S}(t_{i})$, we know that $Var[\hat{F}(t_{i})]=Var[\hat{S}(t_{i})]$ 

    + Similarly, $\hat{q_j}=1-\hat{p_j}$ and $Var[\hat{q_j}]=Var[\hat{p_j}]$ 

- Therefore, <red>$\theta = p_{j} \;\text{or}\;q_{j}$</red>

## Next, Find The Variance of The Parameter <purple>$Var[\theta]$</purple>

- Since $q_{j}=1-p_{j}$, we know that $Var[q_{j}]=Var[p_{j}]$

- Recall that $\hat{p}=\frac{x}{n}$ where $x$ is the number of observed "successes"

- Also, note that $\hat{p}$ is an unbiased estimator for $p$

$$E[\hat{p}]=E\left[\frac{x}{n}\right]=\frac{E[x]}{n}=\frac{np}{n}=p$$

- Since $\hat{p}$ is an unbiased estimator for $p$,

$$
\begin{aligned}
Var[\hat{p}]&=Var[p]\\\\
&=Var\left[\frac{x}{n}\right]=\frac{1}{n^{2}}Var[x]=\frac{Var[x]}{n^{2}}\\\\\\
&=\frac{np(1-p)}{n^{2}}=\frac{p(1-p)}{n}\\
\end{aligned}
$$

- Therefore, <purple>$Var[p]=\frac{p(1-p)}{n}=\frac{1-q(q)}{n}$</purple>

## Now, What is The Function of the Parameter<blue>$g(\theta)?$</blue>

- From Equation 3.6

$$
\begin{aligned}
S(t_{i})&=\prod_{j=1}^{i}[1-p_{j}], i=1,...,m \\
&=(1-p_{1})(1-p_{2})...(1-p_{i})\\
&=(q_{1})(q_{2})...(q_{i})
\end{aligned}
$$

- Therefore, <blue>$g[p]=\prod_{j=1}^{i}[1-p_{j}]=\prod_{j=1}^{i}[q_{j}], i=1,...,m$</blue> 

## Finally, What is <green>$\frac{\partial g(\theta_{i})}{\partial \theta_{i}}, \;\; i=1,...m$?</green>

- In this case, it's easier to compute the derivatives $\frac{\partial S(t_{i})}{\partial q_{i}}, \;\; i=1,...m$

- We know that $\forall i \in 1,2,...m$

$$
\begin{aligned}
\frac{\partial S(t_{i})}{\partial q_{i}}&=\frac{\partial \left( q_{1}q_{2}...q_{i-1}q_{i}\right)}{\partial q_{i}}\\\\\\
&=q_{1}q_{2}...q_{i-1}\\\\\\
&=\frac{S(t_{i})}{q_{i}}\\
\end{aligned}
$$

- Therefore, <green>$\frac{\partial g(\theta_{i})}{\partial \theta_{i}}, \;\; i=1,...m=\frac{S(t_{i})}{q_{i}}$</green>

## Putting Everything Together

$$
\begin{aligned}
Var[S(t_{i})]&=\sum_{j=1}^{i}\left[\frac{\partial S(t_{i})}{\partial q_{j}}\right]^{2}Var(q_{j})\\
&=\sum_{j=1}^{i}\left[\frac{S(t_{i})}{q_{j}}\right]^{2}\frac{p_{j}(1-p_{j})}{n_{j}}\\
&=S(t_{i})^{2}\sum_{j=1}^{i}\frac{p_{j}(1-p_{j})}{n_{j}(1-p_{j})^{2}}\\
&=S(t_{i})^{2}\sum_{j=1}^{i}\frac{p_{j}}{n_{j}(1-p_{j})}\\
\end{aligned}
$$
</div>

# 3.6.2 - Greenwood's formula

Substituting the estimated values into (3.8) gives what is known as Greenwood's formula

$$\displaystyle \widehat{Var}\left[\hat{F}(t_{i})\right]=\widehat{Var}\left[\hat{S}(t_i)\right]=\left[\hat{S}(t_{i})\right]^{2}\sum_{j=1}^{i}\frac{\hat{p}_{j}}{n_{j}(1-\hat{p_{j}})}$$

Greenwood's formula can then be used to estimate the standard error of $\hat{F}(t_{i})$ as

$$\hat{se}_{\hat{F}}=\sqrt{\widehat{Var}[\hat{F}(t_{i})]}$$

# 3.6.3 - Pointwise Normal-Approximate CI for $F(t_i)$

- The pointwise $1-\alpha$ confidence intervals for multiply censored data are then expressed as
$$\left[\underline{F}(t_i),\overline{F}(t_i)\right]=\hat{F}(t_i)\pm z_{(1-\alpha/2)}\hat{se}_{\hat{F}}$$

- Under the assumption that for large sample sizes

$$Z_{\hat{F}}=\frac{\hat{F}(t_i)-F(t_i)}{\hat{se}_{\hat{F}}}\xrightarrow d N(0,1) \text{ as } n\rightarrow\infty$$

- However, for small sample sizes this assumption may fail if the distribution of $Z_{\hat{F}}$ is highly skewed

- A better approximation for $\hat{F}(t_{i})$ results from a logit transformation such that
$$Z_{\text{logit}(\hat{F})}=\frac{\text{logit}(\hat{F}(t_i))-\text{logit}(F(t_i))}{\hat{se}_{\text{logit}(\hat{F})}}$$

- The transformed pointwise $1-\alpha$ CI for multiply censored data are then

<font size="5">
$$
\left[\underline{\underline{F}}(t_i),\overline{\overline{F}}(t_i)\right]=\left[\frac{\hat{F}(t_i)}{\hat{F}(t_i)+(1-\hat{F}(t_i)\times w}, \frac{\hat{F}(t_i)}{\hat{F}(t_i)+(1-\hat{F}(t_i)/ w}\right]
$$
</font>

- where 
$$w=\exp\left[\frac{z_{(1-\alpha/2)}\hat{se}_{\hat{F}}}{\hat{F}(1-\hat{F})}\right]$$

<div class='example'>
### Example 3.7 - CI for Pooled Heat Exchanger Data

## Recall Table 3.2 

<small>
$$
`r SMRD::xarray(Table.3.2, include.rownames = FALSE, digits = c(0,0,0,0,0,0,0,0,4,4))`
$$
</small>

- Therefore

$$
\begin{aligned}
\widehat{Var}(1)&=[\hat{S}(t_{i})]^{2}\sum_{j=1}^{i}\left[\frac{\hat{p}_{j}}{n_{j}(1-p_{j})}\right]\\
&=(.9867)^{2}\left[\frac{.0133}{300(.9867)}\right]\\
&=.0000438
\end{aligned}
$$

- The estimated standard error is then found by $\widehat{se}_{\hat{F}}=\sqrt{.0000438}=.00662$

- And the $95\%$ pointwise CI is 

$$\left[\underline{F}(1),\overline{F}(1)\right]=.0133\pm 1.960\times .00662=[.0003, .0263]$$

- Applying the logit transformation to $\hat{F}(t_{i})$ gives

<small>
$$
\begin{aligned}
\left[\underline{\underline{F}}(1),\overline{\overline{F}}(1)\right]&=\left[\frac{.0133}{.0133+(1-.0133)\times 2.6878}, \frac{.0133}{.0133+(1-.0133)/ 2.6878}\right]\\\\\\
&=[.0050, .0350]\\\\
\end{aligned}
$$
</small>

- where 

$$w=\exp\left[\frac{1.960(.00662)}{.0133(1-.0133)}\right]=2.86687816$$

- A table of values for $\hat{F}(t_{i})$ can be produced from the SMRD package using

```{r}
HeatExchanger.ld <- frame.to.ld(heatexchanger,
                                response.column = c(1,2), 
                                censor.column = 3,
                                case.weight.column = 4,
                                time.units = 'Years')
DT::datatable(print(cdfest(HeatExchanger.ld))$table)
```

This output may then be converted to a $LaTeX$ table using

```{r, eval=FALSE}
xtable::xtable(print(cdfest(HeatExchanger.ld))$table, include.rownames = FALSE, digits = c(0,0,0,5,5,5,5))
```

$$
`r SMRD::xarray(print(cdfest(HeatExchanger.ld))$table, include.rownames = FALSE, digits = c(0,0,0,5,5,5,5))`
$$
</div>

# 3.7 - Estimation From Multiply Censored Data With Exact Failures

## Multiply Censored Data With Exact Failures

- "Exact" failures are an abstraction, but can be considered to have occurred in reality if

    + Test units are under continuous inspection (some electronic systems)

    + A large number of inspections are performed at closely spaced intervals  

- In a test with exact failures, most inspections will not discover a failure, therefore

    + The value of $F(t)$ will not change

    + The plot of $\hat{F}(t)$ will appear as a step-function

<div class='example'>
### Example 3.9 - Shock Absorber Data

```{r, out.width='100%'}
teachingApps::includeApp(height = '800px', appName = 'table3_4')
```

## Table 3.4

```{r,echo=c(2,3),fig.height=4.5, fig.align='center'}
par(family = 'serif', font = 2, cex = 1.15)
ShockAbsorber.ld <- frame.to.ld(shockabsorber, 
                                response.column = 1, 
                                censor.column = 3,
                                time.units = 'Kilometers')
plot(ShockAbsorber.ld, band.type = 'Pointwise')
```

<smaller>
$$
`r SMRD::xarray(print(cdfest(ShockAbsorber.ld))$table, include.rownames = FALSE, digits = c(0,0,0,5,5,5,5))`
$$
</smaller>

</div>

# 3.8 - SIMULTANEOUS CONFIDENCE BANDS

# 3.8.1 - Motivation

## Pointwise confidence intervals $(\S \;\;\text{3.6.3})$ 

- Quantify the sampling uncertainty about $F(t_{i}$ at a set of individual points

- Have a <u>collective</u> coverage probability that's less than any individual interval

## Simultaneous confidence intervals 

- Quantify the sampling uncertainty along the entire range of $F(t_{i})$

- Are generally wider than pointwise confidence intervals

- More accurately reflect the uncertainty over the displayed time range

- Should be used with the logit transformation

## Simultaneous & Pointwise CI's

```{r,echo=FALSE,fig.align='center', fig.width=9, fig.height=5.25}
hours <- c(0.00001,1000) ; N <- 5000 ; par(family = 'serif')
FM  <- function(x) 1-exp(-(x/198.6))
FU  <- function(x) 1-exp(-(x/125.99))
FL  <- function(x) 1-exp(-(x/482.58))
FSU <- function(x) FL(x)/(FL(x)+(1-FL(x))*(3.21*(FL(x)*(1-FL(x)))^(-.5)))
FSL <- function(x) FU(x)/(FU(x)+(1-FU(x))/(3.21*(FU(x)*(1-FU(x)))^(-.5)))

curve(FM, xlim = hours, 
      xlab = 'Hours', 
      ylab = 'Fraction Failing', 
      lwd = 2, las = 1, ylim = c(0,1), n = N)
curve(FL, lwd = 2, col = 2, add = TRUE, n = N)
curve(FU, lwd = 2, col = 2, add = TRUE, n = N)
curve(FSU, lwd = 2, col = 3, add = TRUE, n = N)
curve(FSL, lwd = 2, col = 3, add = TRUE, n = N)

legend('bottomright',
       c(expression(hat('F')*'(t)'[MLE]),
         expression('95% Pointwise CI'),
         expression('95% Simultaneous CI')),
       lwd = c(rep(1.5,1.25,1.25)),
       col = c(1,2,3),
       cex = 1, bty = 'n',xjust = 0.5)
```

# 3.8.2 - Large-Sample Simultaneous CI for $F(t)$

An approximate $100(1-\alpha)\%$ simultaneous CI for $F(t)$ is obtained from

$$\left[\underline{\underline{F}}(t),\overline{\overline{F}}(t)\right]=\hat{F}(t)\pm (e_{a,b,1-\alpha/2)}\hat{se}_{\hat{F}(t)}, \;\;\; \forall t\in [t_{L}(a), t_{U}(b)]$$

where the range $[t_{L}(a), t_{U}(b)]$ is a function of the censoring pattern in the data

Note that $e_{a,b,1-\alpha/2)}$ replaces

$$\left[\underline{\underline{F}}(t),\overline{\overline{F}}(t)\right]=\left[\frac{\hat{F}(t)}{\hat{F}(t)+[1-\hat{F}(t)]\times w},\frac{\hat{F}(t)}{\hat{F}(t)+[1-\hat{F}(t)]/w}\right]$$

## Table 3.5 - Factors for Simultaneous CI

```{r, out.width='100%'}
teachingApps::includeApp(height = '800px', appName = 'table3_5')
```

# 3.8.3 - Time Range for Simultaneous CI for $F(t)$

## stuff

- more stuff
- even more
