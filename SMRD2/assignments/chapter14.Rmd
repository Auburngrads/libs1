---
output: html_document
---

```{r echo=FALSE}
ex <- 0
```


# From the Text Exercise 14.`r ex = ex+1; ex`

 Starting with the  traditional form of Bayes' rule, show that it can be expressed in terms of relative likelihoods. 


# From the Text Exercise 14.`r ex = ex+1; ex`

 Explain why doing a Bayesian analysis with a specified prior distribution to estimate an unknown exponential distribution mean $\theta$ is not a model that implies that $\theta$ varies from unit to unit in the population or process being studied (Hint: Consider what happens to the posterior pdf when the sample size approaches infinity). 


# From the Text Exercise 14.`r ex = ex+1; ex`

 Explain how one would compute $f[g(\thetavec) | \DATA]$, the marginal posterior pdf of the a function of the parameters of interest, using numerical integration. Contrast this with the simulation-based method. 


# From the Text Exercise 14.`r ex = ex+1; ex`

 Discuss the advantages and disadvantages of the Monte Carlo method of computing the posterior distribution, relative to the use of numerical integration. 


# From the Text Exercise 14.`r ex = ex+1; ex`

 A total of 100 new units have been introduced into service.  It is believed that the underlying time to failure distribution is Weibull. The analysts have available prior information and censored data on a sample of $n$ similar units. 
 

a. Given a Monte Carlo sample of 2000 pairs of $\mu^{*},\sigma^{*}$ values from the posterior pdf $f(\mu,\sigma | \DATA)$, show how to compute marginal posterior predictive distribution for $T_{(1)}$, the time of the first failure out of the 100 units. Also show how to get a 95\% lower Bayesian prediction bound for $T_{(1)}$. 

a. Suppose that you can compute (but perhaps not write in closed form) the joint posterior pdf $f(\mu,\sigma |\DATA)$. Explain how you could, using numerical integration and other numerical (but not simulation) methods, compute marginal posterior predictive pdf  and a 95\% lower Bayesian prediction bound for $T_{(1)}$.  


# From the Text Exercise 14.`r ex = ex+1; ex`

 Refer to Exercise~\ref{exercise:lognormal.bayes}. Derive a simple expression for the posterior predictive pdf of $T$ given $\DATA$. 


# From the Text Exercise 14.`r ex = ex+1; ex`

 Consider the following quantities used in Exercise~\ref{exercise:lognormal.bayes}: $\Ybar$, $\mu$, $a_{1}$, and $b_{1}$.  
 

a. Use the setting of Exercise~\ref{exercise:lognormal.bayes} and these quantities to explain the important differences between variability and uncertainty in physical processes and statistical inference. 

a. Explain how one could generalize the model in Exercise~\ref{exercise:lognormal.bayes} to allow for batch-to-batch variability in the reliability of the electronic component and describe a sampling plan that could be used to combine prior information with data to estimate the parameters of this more general model.  


# From the Text Exercise 14.`r ex = ex+1; ex`

 Show that for the special case $k=m=1$, (\ref{equation:pred.pdf.order}) and  (\ref{equation:pred.cdf.order}) reduce to the pdf and cdf given in (\ref{equation:basic.cdf}). 


