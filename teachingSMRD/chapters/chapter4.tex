%chapter 4
%\batchmode
%original by wqmeeker  12 Jan 94
%edited by wqmeeker  19 mar 94 
%edited by wqmeeker  26 mar 94 
%edited by driker 28 mar 94
%edited by driker 30 mar 94
%edited by wqmeeker  30/31 mar 94 adding logistic and other stuff
%edited by lae  april 10, correct typos on exercises 5 and 7 (a)
%edited by wqmeeker  1 june 94
%edited by driker 13 july 94
%edited by wqmeeker  2 aug 94
%edited by wqmeeker  7 aug 94
%edited by wqmeeker  9 aug 94
%edited by wqmeeker  19/21 sept 94  many minor editing changes
%edited by wqmeeker  22 sept 94 minor editing changes
%edited by wqmeeker  19 oct 94 luis' suggestions and other changes
%edited by wqmeeker  21 oct 94 minor changes
%edited by wqmeeker  28 nov 94 moment equations
%edited by driker    20 dec 94
%edited by driker 2 feb 95
%edited by driker 7 feb 95
%edited by wqmeeker 26 may 95 remove _{T} and \special
%edited by wqmeeker 24 june 95 pseudo random and other minor stuff
%edited by driker 22 august 95
%edited by driker 11 nov 95
%edited by driker 1 july 96
%edited by driker 20 nov 96
%edited by wqmeeker  9 mar 97 india changes

\setcounter{chapter}{3}

\chapter{Location-Scale-Based
Parametric Distributions}
\label{chapter:ls.parametric.models}

\input{\chapterhome/common_heading.tex}

%----------------------------------------------------------------------
%----------------------------------------------------------------------

\section*{Objectives}
This chapter explains:
\begin{itemize} 
\item 
Important ideas behind parametric models in the analysis of
reliability data.
\item
Motivation for important functions of model parameters that are of
interest in reliability studies.
\item
The location-scale family of
probability distributions.
\item 
Properties and the importance of the exponential distribution.
\item 
Properties and the importance of log-location-scale
distributions such as the Weibull, lognormal, and
loglogistic distributions.
\item 
How to generate pseudorandom data from a specified distribution (such
random data are used in simulation evaluations in subsequent
chapters).
\end{itemize}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Overview}
This chapter introduces some basic ideas of parametric modeling and
the most important parametric distributions. Parametric
distributions are used extensively in subsequent chapters.
Section~\ref{section:par.dist.intro} explains some of the basic
concepts and motivation for using parametric models.
Section~\ref{section:quantities.of.interest} describes important
functions of parameters like failure probabilities and
distribution quantiles. Section~\ref{section:location.scale}
introduces the important location-scale family of distributions.
Section~\ref{section:exponential.distribution} through
\ref{section:loglogistic.distribution.definition} give detailed
information on these and the  important log-location-scale distributions.
Subsequent chapters require at least a basic understanding of the
characteristics and notation for the exponential, Weibull, and
lognormal distributions. Applications for the other distributions
follow without difficulty. Physical motivation for these and the other
distributions is helpful in practical modeling applications.
Section~\ref{section:parameterization.choice} describes alternative
choices for parameters.
Section~\ref{section:generating.random.numbers} describes methods for
generating simulated values from a specified distribution. In various
parts of this book we will use simulation to develop and explore data
analysis methods.


%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Introduction}
\label{section:par.dist.intro}
As we saw in
Chapter~\ref{chapter:nonparametric.estimation}, it is possible to make
certain kinds of inferences without having to assume a particular 
parametric form
for a failure-time distribution. There are, however, many problems
in reliability data analysis where it is either useful or essential to
use a parametric distribution form.  This chapter describes a number of
simple probability distributions that are commonly used to model
failure-time processes.  Chapter
\ref{chapter:other.parametric.models} does the same for other
important and useful, but more complicated, distributions.  The
discussion in these chapters concerns underlying continuous-time
models, although much of the material also holds for discrete-time
models.

As explained in Chapter~\ref{chapter:np.models.censoring.likelihood},
a natural model for a continuous random variable, say $\rv$, is the
cumulative distribution function (cdf).  Specific examples given in
this chapter and in Chapter~\ref{chapter:other.parametric.models}
are of the form
$\Pr(\rv \leq \realrv)=F(\realrv;\thetavec)$
where $\thetavec$ is a vector of parameters.
In this book, we use $\rv$ to denote positive random variables 
like failure time, 
so that $\rv>0$; correspondingly, we will use $\grv$ 
to denote unrestricted random variables so that $-\infty <
\grv=\log(\rv) < \infty$.  Unlike the ``basic parameters'' in $\pivec$
and $\pvec$ used in the ``nonparametric'' formulation in
Chapters~\ref{chapter:np.models.censoring.likelihood} and
\ref{chapter:nonparametric.estimation}, the parametric models
described in this chapter will have a $\thetavec$ containing a small
fixed number of parameters. The most commonly used parametric
probability distributions have between 1 and 4 parameters, although
there are some distributions with more than 4 parameters.  More
complicated models could contain many more parameters involving
mixtures, competing failure modes, or other combinations of
distributions or models that include explanatory variables.  One
simple example that we will use later in this chapter is the
exponential distribution for which
\begin {equation}
\label{equation:expo.definition}
\Pr(\rv \leq \realrv)=F(\realrv;\theta) = 
1 - \exp \left( - \frac{\realrv}{\theta} \right), \quad \realrv > 0
\end {equation}
where $\theta$ is the single scalar parameter of the distribution
(equal to the mean or first moment, in this example).

Use of parametric distributions complements nonparametric
techniques and provides the following advantages:
\begin{itemize}
\item
Parametric models can be described concisely with just a few
parameters, instead of having to report an entire curve.
\item
It is possible to use a parametric model to extrapolate (in time)
to the lower or upper tail of a distribution.
\item
Parametric models provide smooth estimates of failure-time distributions.
\end{itemize}
In practice it is often useful to do various parametric and nonparametric
analyses of a data set.


%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Quantities of Interest in Reliability Applications}
\label{section:quantities.of.interest}
Starting in Chapter~\ref{chapter:parametric.ml.one.par}, we will focus
on the problem of {\em estimating} the parameters $\thetavec$ and
important functions of $\thetavec$. In this section
we describe ideas behind parameterization of a probability
distribution
and describe a number of particular functions of parameters
that are of interest for reliability analysis.


%----------------------------------------------------------------------

%----------------------------------------------------------------------
\label{section:functions.of.parameters}
In most practical problems, interest centers on quantities that are
functions of $\thetavec$ and the ML estimates of these functions will
{\em not} depend on the particular parameterization that is used to
specify the parametric model. The quantities of interest discussed
here extends the list introduced in
Chapter~\ref{chapter:np.models.censoring.likelihood}, and now these
quantities will be expressed as functions of the small set of
parameters $\thetavec$. Specifically, for distributions of positive
and continuous random variables (there are similar definitions for
discrete and/or nonpositive random variables)

\begin{itemize}
\item
The ``probability of failure'' 
$p=\Pr(T \leq \realrv)= F(\realrv;\thetavec)$ by a
specified $\realrv$. For
example, if $\rv$ is the time of failure of a unit, then
$p$ is the probability that the unit will fail before $\realrv$.
\item
The ``$p$ quantile'' of the distribution of $\rv$ is the smallest
value $\rvquan$ such that $F(\rvquan;\thetavec) \ge p$. We will
express the $p$ quantile as $\rvquan_{p}=F^{-1}(p;\thetavec)$. For the
failure time example, $\rvquan_{p}$ is the time at which 100$p$\% of
the units in the product population will have failed. The median is
equal to $t_{.5}$.
\item
The ``hazard function''  (hf) is defined as
\begin{equation}
\label{equation:hazard.definition}
h(\realrv)=\frac{f(\realrv;\thetavec)}
{1-F(\realrv;\thetavec)}.
\end{equation}
As described in Section~\ref{section:ttf.functions}, the hazard
function is of particular interest in reliability applications because
it indicates, for surviving units, the propensity to fail in the following
small interval of time, as a function of age.

\item
The mean life (also known as the ``average,'' ``expectation,'' or ``first
moment'') of $\rv$  
\begin{equation}
\label{equation:mttf}
\E(\rv)=\int_{0}^{\infty} \realrv f(\realrv;\thetavec)\, d\realrv
=\int_{0}^{\infty}[1-F(t)]dt
\end{equation}
is a measure of the center of $f(t;\thetavec)$. When $f(t;\theta)$ is
highly skewed, the mean may differ appreciably from other measures of
central tendency like the median. The mean is sometimes, but not
always, one of the distribution parameters. For some pdfs, the value
of the integral will be infinite. Then it is said that the mean of
$\rv$ ``does not exist.'' When $\rv$ is time to failure, the mean is
sometimes referred to as the MTTF, for Mean Time To Failure.
\item
The variance (also known as the ``second central moment'') of $\rv$
\begin{displaymath}
\var(\rv)=\int_{0}^{\infty} 
[\realrv- \mbox{E}(\rv)]^{2} f(\realrv;\thetavec)\, d\realrv
\end{displaymath}
is a measure of spread of the distribution of $\rv$.  $\var(\rv)$ is
the average squared deviation of $\rv$ from its mean.  Again, if the
value of the integral is infinite, it is said that the variance of
$\rv$ ``does not exist.'' The quantity $\sd(\rv)=\sqrt{\var(\rv)}$,
known as the ``standard deviation'' of $T$, is easier to interpret
because it has the same units as $T$.
\item
The unitless quantity $\gamma_{2} =\sd(\rv)/\E(\rv)$, known as
the ``coefficient of variation'' of $\rv$, is useful for comparing the
relative amount of variability in different distributions.  The
quantity $1/\gamma_{2}=\E(\rv)/\sd(\rv)$ is sometimes known as
the ``signal-to-noise-ratio.''
\item
The unitless quantity 
\begin{displaymath}
\gamma_{3} =  \frac{\int_{0}^{\infty} [\realrv- \mbox{E}(\rv)]^{3}
f(\realrv;\thetavec)\, d\realrv}{\left[\var(\rv) \right]^{\frac{3}{2}}},
\end{displaymath}
known as the ``standardized third central moment'' or ``coefficient of
skewness'' of $\rv$, is a measure of the skewness in the distribution
of $\rv$.  When a distribution is symmetric, $\gamma_{3}=0$. It is,
however, possible to have $\gamma_{3}=0$ for a distribution that is
not perfectly symmetric (e.g., the Weibull distribution, discussed in
in Section~\ref{section:weibull.distribution}, has $\gamma_{3}=0$ when
$\beta=3.602$, but the distribution is only approximately symmetric).
Usually, however, when $\gamma_{3}$ is positive (negative), the
distribution of $\rv$ is skewed to the right (left).
\end{itemize}
For reliability applications, quantiles, failure probabilities, and
the hazard function are typically of higher interest than distribution
moments.  In subsequent chapters we will describe {\em point
estimation} and, at the same time, emphasize methods of obtaining {\em
confidence intervals} (for scalars) and {\em confidence regions} (for
simultaneous inference on a vector of two or more quantities) for
parameters and important functions of parameters.  Confidence
intervals and regions quantify the uncertainty in parameter estimates
arising from the fact that inferences are generally based on only a finite
number of observations from the process or population of interest.

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Location-scale and Log-location-scale Distributions}
\label{section:location.scale}
A random variable $\grv$ belongs to the location-scale
family of distributions if its cdf $\grv$
can be expressed as
\begin{displaymath}
\Pr(\grv \leq \grealrv) =  
F(\grealrv;\mu,\sigma) = 
	\Phi\left(\frac{\grealrv-\mu}{\sigma}\right)
\end{displaymath}
where $\Phi$ does not depend on any unknown parameters.
In this case
we say that $-\infty < \mu < \infty$ is a location parameter
and that $\sigma> 0 $ is a scale parameter. Substitution shows that
$\Phi$ is the cdf of $\grv$ when $\mu=0$ and $\sigma=1$.
Also, $\Phi$ is the cdf of  $(\grv-\mu)/\sigma$.
Location-scale distributions are important for a number
of reasons including:
\begin{itemize}
\item
Many of the widely used statistical distributions are either
location-scale distributions or closely related.
These distributions include the exponential, normal, Weibull,
lognormal, loglogistic, logistic, and extreme value distributions.
\item
Methods of data analysis and
inference, statistical theory, and computer software
developed for the location-scale family can be applied to any of
the members of the family.
\item
Theory for location-scale distributions is relatively simple.
\end{itemize}
In cases where $\Phi$ does depend on one or more unknown parameters
(as with a number of the distributions described in
Chapter \ref{chapter:other.parametric.models}),
$\grv$ is not a member of the location-scale family, but the
location-scale structure and notation will still be useful for us.

A random variable $T$ belongs to the log-location-scale family
distribution if $Y=\log(T)$ is a member of the location-scale
family. The Weibull, lognormal, and loglogistic distributions are
the most important members of this family.


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Exponential Distribution}
\label{section:exponential.distribution}
When $\rv$ has an exponential distribution, we
indicate this by $\rv \sim \EXP(\theta,\threshold)$.
The two-parameter exponential distribution (to distinguish it from
the more commonly used one-parameter exponential distribution) has
cdf, pdf, and hf
\begin{eqnarray*}
 F(\realrv;\expmean,\gamma)&=&1-
\exp \left (-\,\frac{\realrv-\threshold}{\expmean} \right )
\\
 f(\realrv;\expmean,\gamma)&=& \frac{1}{\expmean} 
\exp \left (-\,\frac{\realrv-\threshold}{\expmean} \right ) \\ 
h(\realrv;\expmean,\gamma)&=&
\frac{1}{\expmean}, \quad \realrv > \gamma\\
\end{eqnarray*}
where $\expmean > 0$ is a scale parameter and $\threshold$ is both a
location and a threshold parameter. For $\threshold=0$ this is the
well-known one-parameter exponential distribution (and often known
simply as the exponential distribution).  When $\rv$
has this simpler distribution, we indicate it by $\rv \sim
\EXP(\theta)$. The cdf, pdf, and hf are graphed in
Figure~\ref{figure:distplot.exp.ps} for $\expmean=.5, 1$, and $2$ and
$\threshold=0$.

For integer $m > 0$, $\E[(\rv-\threshold)^{m}] = m!\, 
\theta^{m} $. Thus the mean and variance of the exponential distribution
are, respectively, $\E(\rv)= \threshold + \expmean$ and $\var(\rv)= 
\expmean^{2}$.
The $p$ quantile of the exponential distribution is
$\rvquan_{p}=\threshold -\log(1-p)\, \expmean$. 

The one-parameter exponential distribution, where $\gamma=0$, is the
simplest distribution that is commonly used in the analysis of
reliability data.  The exponential distribution has the important
characteristic that its hf is constant (does not depend
on time $t$). A constant hf implies that, for an unfailed unit,
the probability of failing in the next small interval of time is
independent of the unit's age. Physically, a constant hf suggests
that the population of units under consideration
is not wearing out or otherwise aging.  The exponential
distribution is a popular distribution for some kinds of electronic
components (e.g., capacitors or robust, high-quality integrated
circuits). This exponential distribution would {\em not} be
appropriate for a population of electronic components having
failure-causing quality defects (such defects are difficult to rule
out completely and are a leading cause of electronic system
reliability problems). On the other hand, the exponential distribution
might be useful to describe failure times for components that exhibit
physical wearout if the wearout does not show up until long after the
expected technological life of the system in which the component would
be installed (e.g., electronic components in computing equipment
having failures caused by random external events).

Under very special circumstances, the exponential distribution may be
appropriate for the times between system failures, arrivals
in a queue, and other interarrival time distributions. Specifically,
the exponential distribution is the distribution of interval times of
a homogeneous Poisson process. See Chapter~3 of Thompson~(1988)
and Chapter~\ref{chapter:repairable.system} 
for more information on homogeneous Poisson processes.

The exponential distribution is usually {\em inappropriate}
for modeling the life of mechanical components (e.g., bearings) subject
to some combination of fatigue, corrosion, or wear. It is also
usually inappropriate for  electronic
components that exhibit wearout properties over their technological
life (e.g., lasers and filament devices).  A distribution with an
increasing hf is, in such applications, usually more
appropriate.  Similarly, for populations containing mixtures of good
and bad units the population
hf may decrease with life because, as
the bad units fail and leave the population, only the stronger units
are left.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.exp.ps}
\caption{Exponential cdf, pdf, and hf
for $\expmean=$ .5, 1, and 2 and $\gamma=0$.}
\label{figure:distplot.exp.ps}
\end{figure}
%-------------------------------------------------------------------


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Normal Distribution}
\label{section:normal.distribution.definition}

%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.nor.ps}
\caption{Normal cdf, pdf, and hf with 
location parameter (mean)
$\mu=5$ and scale parameter (standard deviation) $\sigma=$ .3, .5, and .8.}
\label{figure:distplot.nor.ps}
\end{figure}
%-------------------------------------------------------------------
When $\grv$ has a normal distribution, we indicate this by $\grv
\sim \NOR(\mu, \sigma)$.  The normal distribution is a location-scale
distribution with cdf and pdf
\begin{eqnarray*}  
 F(\grealrv;\mu, \sigma)&=&
\Phi_{\nor}\left (\frac{\grealrv-\mu}{\sigma}
\right )
\\
 f(\grealrv;\mu, \sigma)&=&\frac{1}{ \sigma } \phi_{\nor}
\left ( 
\frac{\grealrv-\mu}{\sigma} \right), \quad -\infty < \grealrv < \infty
 \end{eqnarray*} 
where $\phi_{\nor}(z)=(1/\sqrt{2\pi})\exp(-z^2/2)$ and 
$\Phi_{\nor}(z)=\int^{z}_{-\infty}\phi_{\nor}(w) \, dw$ 
are, respectively,
the pdf and cdf for the standardized $\NOR (\mu=0, \sigma=1$)
distribution. Here $-\infty< \mu <
\infty$ is a location parameter and $\sigma > 0$ is a scale parameter.
When there is no useful simplification of the hf
definition in (\ref{equation:hazard.definition}), as with the normal
distribution, the definition will not be repeated.  The normal
distribution pdf, cdf, and hf are graphed in
Figure~\ref{figure:distplot.nor.ps} for $\mu=5$ and $\sigma=.3,
.5,.8$.

For integer $m > 0$, $\E[(\grv-\mu)^{m}]=0$ if $m$ is odd and
$\E[(\grv-\mu)^{m}]= m! \sigma^{m}/[2^{m/2} \, (m/2)!]$ if $m$ is
even.  From this, the mean and variance of the normal
distribution are, respectively, $\E(\grv)= \mu$ and $\var(\grv)=
\sigma^{2}$.  The $p$ quantile  of the normal distribution is
$\grvquan_{p}=\mu +
\Phi^{-1}_{\nor}(p) \sigma$, 
where $\Phi^{-1}_{\nor}(p)=\norquan_{p}$ is the $p$ quantile of the
standard normal distribution.

As a model for variability, the normal distribution has a long history
of use in many areas of application. This is due to the simplicity of
normal distribution theory and the central limit theorem.  The central
limit theorem states that the distribution of the sum of a large
number of independent identically distributed random quantities has,
approximately, a normal distribution.  In reliability data analysis,
the use of the normal distribution is, however, less common.  As seen
from Figure~\ref{figure:distplot.nor.ps}, the normal distribution has
an increasing hf that begins to increase rapidly near, but before, the
point of median life.  The normal distribution has proven to be a
useful distribution for certain life data when $\mu>0$ and the
coefficient of variation ($\sigma/\mu$) is small.  Examples include
electric filament devices (e.g., incandescent light bulbs and toaster
heating elements) and strength of wire bonds in integrated circuits
(component strength is often used as an easy-to-obtain surrogate
measure or indicator of eventual reliability).  Also, as described in
Section~\ref{section:lognormal.distribution.def}, the normal
distribution is often a useful model for the logarithms of failure
times (see the next section).

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Lognormal Distribution}
\label{section:lognormal.distribution.def}
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.lnor.ps}
\caption{Lognormal cdf, pdf, and hf for scale parameter
$t_{.5}=\exp(\mu)=1$ and for shape parameter $\sigma=$ .3, .5,  and .8.}
\label{figure:distplot.lnor.ps}
\end{figure}
%-------------------------------------------------------------------
When $\rv$ has a lognormal distribution, we indicate this by $\rv
\sim \LOGNOR (\mu,\sigma)$.  If $\rv \sim \LOGNOR (\mu,\sigma)$ then
$\grv=\log(\rv) \sim \NOR(\mu, \sigma)$. 

The lognormal cdf and pdf are
\begin{eqnarray}  
\label{equation:lognormal.cdf}
 F(\realrv;\mu, \sigma)&=&\Phi_{\nor}\left [\frac{\log(\realrv)-\mu}{\sigma}
\right ]\\
 f(\realrv;\mu, \sigma)&=&\frac{1}{ \sigma \realrv} \phi_{\nor}
\left [ 
\frac{\log(\realrv)-\mu}{\sigma} \right], \quad \realrv > 0
 \end{eqnarray} 
where $\phi_{\nor}, \Phi_{\nor}$ are pdf and cdf for the
standardized normal. The median $t_{.5}=\exp(\mu)$ is a scale parameter and
$\sigma > 0$ is a shape parameter.
The lognormal cdf, pdf, and hf
are graphed in Figure~\ref{figure:distplot.lnor.ps}
for $\sigma=.3, .5,.8$ and $\mu=0$, corresponding to the 
median $t_{.5}=\exp(\mu)=1$.


The most common definition of the lognormal distribution uses base $e$
(natural) logarithms. Base 10 (common) logarithms are also used in
some areas of application. Bottom-line answers for important
reliability metrics (e.g., estimates of failure probabilities, failure
rates, and quantiles) will not depend on the base that is used.  The
definition of the parameters $\mu$ (mean of the {\em logarithm} of
$\rv$) and $\sigma$ (standard deviation of the {\em logarithm} of
$\rv$), will however, depend on the base that is used. For this reason
it is important to make consistent use of one particular base. In this
book we will generally use base $e$ (natural) logarithms for the
lognormal distribution definition.

For integer $m > 0$, $E(\rv^{m})=\exp \left (m \mu + m^{2}
\sigma^{2}/2 \right )$.  From this it follows that the mean and
variance of the lognormal distribution are, respectively, $\E(\rv)=
\exp \left (\mu+.5 \sigma^{2} \right )$ and $\var(\rv)= \exp \left
(2\mu + \sigma^{2}
\right) \left [\exp(\sigma^{2})-1 \right]$.
The quantile function of the lognormal distribution is
$\rvquan_{p}=\exp \left [\mu + \Phi^{-1}_{\nor}(p)\sigma \right ]$.

The lognormal distribution
described in this section is sometimes referred to as the
``two-parameter lognormal distribution'' to distinguish it from the
three-parameter lognormal distribution that is described in
Section~\ref{section:gets.special.cases}. 

The lognormal distribution is a common model for failure times.
Following from the central limit theorem (mentioned in
Section~\ref{section:normal.distribution.definition}), application of
the lognormal distribution could be justified for a random variable
that arises from the product of a number of identically distributed
independent positive random quantities.  It has been suggested that
the lognormal is an appropriate model for time to failure caused by a
degradation process with combinations of random rate constants that
combine multiplicatively (see, for example, the models in
Chapter~\ref{chapter:degradation.data}). The lognormal distribution is
widely used to describe time to fracture from fatigue crack growth in
metals. As shown in Figure~\ref{figure:distplot.lnor.ps} (also see
Exercise
\ref{exercise:lognormal.hazard}), the lognormal $h(t)$ starts at
0, increases to a point in time, and then decreases eventually to
zero. For large $\sigma$, $h(t)$ reaches a maximum early in life and
then decreases. For this reason, the lognormal distribution is often
used as a model for a population of electronic components that
exhibits a decreasing hf.  For smaller values of $\sigma$, $h(t)$
increases more gradually before decreasing. It has been suggested that
early-life ``hardening'' of certain kinds of materials or components
might lead to such an hf. The lognormal distribution also arises as the
time to failure distribution of certain degradation processes, as
described in Chapter~\ref{chapter:degradation.data}.


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Smallest Extreme Value Distribution}
\label{section:sev.distribution.definition}
When the random variable $\grv$ has a smallest extreme value
distribution, we indicate this by $\grv \sim \SEV(\mu, \sigma)$. The
SEV cdf, pdf, and hf are
\begin{eqnarray*}  
 F(\grealrv;\mu, \sigma)&=& 	\Phi_{\sev}\left
(\frac{\grealrv-\mu}{\sigma}
\right )
\\
 f(\grealrv;\mu, \sigma)&=&\frac{1}{ \sigma } \phi_{\sev}
\left (
\frac{\grealrv-\mu}{\sigma} \right)
\\
h(\grealrv;\mu, \sigma)&=& \frac{1} {\sigma} \exp \left (
\frac{\grealrv-\mu}{\sigma} \right), \quad -\infty < \grealrv <
\infty
\end{eqnarray*} 
where $\Phi_{\sev}(z)=1-\exp[-\exp(z)]$ and
$\phi_{\sev}(z)=\exp[z-\exp(z)]$ are the cdf and pdf, respectively, for
standardized SEV ($\mu=0, \sigma=1$). Here $-\infty < \mu < \infty$ is
the location parameter and $\sigma > 0$ is the scale parameter.  The
$\SEV$ cdf, pdf and hf
are graphed in Figure~\ref{figure:distplot.sev.ps} for
$\mu=50$ and $\sigma=$5, 6, and 7.
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.sev.ps}
\caption{Smallest extreme value 
cdf, pdf, and hf with $\mu=50$ and  $\sigma=$ 5, 6, and 7.}
\label{figure:distplot.sev.ps}
\end{figure}
%-------------------------------------------------------------------

The mean, variance, and quantile functions of the smallest extreme
value distribution are $\E(\grv)= \mu - \sigma \gamma$, $\var(\grv)=
\sigma^{2} \pi^{2}/6$, and $\grvquan_{p}=\mu + \Phi^{-1}_{\sev}(p) \,
\sigma$ where $\Phi^{-1}_{\sev}(p)=\log \left [-\log(1-p) \right ]$
and $\gamma \approx .5772$ is Euler's constant.

Figure~\ref{figure:distplot.sev.ps} shows that the smallest extreme
value distribution pdf is skewed to the left. Although most
failure-time distributions are skewed to the right, distributions
of strength will sometimes be skewed to the left (because of a few
weak units in the lower tail of the distribution, but a sharper upper
bound for the majority of units in the upper tail of the strength
population).  The SEV distribution may have physical justification arising
from an extreme value theorem. Namely, it is the limiting 
standardized distribution of the minimum of
a large number of random variables from a certain class of
distributions (this class includes the normal distribution as a
special case).  If $\sigma$ is small relative to $\mu$ the $\SEV$
distribution can be used as a life distribution. The
exponentially increasing hf suggests that the $\SEV$
would be suitable for modeling the life of
a product that experiences very rapid
wearout after a certain age.  The distributions of logarithms of
failure times can often be modeled with the SEV distribution; see
Section~\ref{section:weibull.distribution}.  Also see the closely
related Gompertz-Makeham distribution in Section
\ref{section:gompertz.makeham}. 

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Weibull Distribution}
\label{section:weibull.distribution}
The Weibull distribution cdf is often written as
\begin{equation}
\label{equation:weibull.cdf}
\Pr(\rv \leq \realrv;\weibscale,\beta ) = 1-
\exp \left [-\left (\frac{\realrv}{\weibscale} \right )^{\beta}
\right ], \quad \realrv > 0.
\end{equation}
For this parameterization, $\beta > 0$ is a shape parameter and
$\weibscale > 0$ is a scale parameter as well as the $.632$ quantile.
The practical value of the Weibull distribution stems from its ability
to describe failure distributions with many different commonly
occurring shapes.  As illustrated in
Figure~\ref{figure:distplot.weib.ps}, for $0<\beta<1$, the Weibull
has a decreasing hf. With $\beta>1$, the Weibull has an
increasing hf.




%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.weib.ps}
\caption{Weibull cdf, pdf, and hf for
$t_{.632}=\weibscale=\exp(\mu)=1$ and  $\beta=1/\sigma=$ .8, 1, and 1.5}
\label{figure:distplot.weib.ps}
\end{figure}
%-------------------------------------------------------------------

For integer $m > 0$, $\E(\rv^{m})=\weibscale^{m}
\Gamma(1+{m}/{\beta})$ where
$\Gamma(\gammashape)=\int^{\infty}_{0} z^{\gammashape-1} \exp(-z) dz$
is the gamma function. 
From this it follows that the mean and
variance of the Weibull distribution are, respectively, $\E(\rv)=
\weibscale
\Gamma \left (1+1/\beta \right )$, and
$\var(\rv)=
\weibscale^{2} \left [\Gamma(1+2/\beta)
-\Gamma^{2}(1+1/\beta) \right]$. The Weibull $p$ quantile
is $\rvquan_{p}=\weibscale \left [-\log(1-p)
\right ]^{1/\beta}$.   Note that when $\beta=1$, the cdf in
(\ref{equation:weibull.cdf}) reduces to an exponential distribution
with scale parameter $\expmean=\eta$.
The Weibull distribution
described in this section is sometimes referred to as the
``two-parameter Weibull distribution'' to distinguish it from the
three-parameter Weibull distribution that is described in
Section~\ref{section:gets.special.cases}.

It is convenient to use a simple alternative parameterization for the
Weibull distribution.  This alternative parameterization is based on
the relationship between the Weibull distribution and the
smallest extreme value distribution described in
Section~\ref{section:sev.distribution.definition}.  In particular, if
$\rv$ has a Weibull distribution, then $\grv=\log(\rv) \sim
\SEV(\mu, \sigma)$ where $\sigma=1/\beta$ is the scale parameter and
$\mu=\log(\weibscale)$ is the location parameter.  Thus when $\rv$
has a Weibull distribution, we indicate this by $ \rv \sim
\WEIB(\mu, \sigma)$. In this form, the Weibull cdf, pdf, and hf
can be written as
\begin{eqnarray}
\label{equation:weibull.cdf.mu.sigma}
 F(\realrv;\mu,\sigma)&=&\Phi_{\sev}\left [\frac{\log(\realrv)-\mu}{\sigma}
\right ] \\
 f(\realrv;\mu,\sigma)&=&\frac{1}{ \sigma \realrv} \, \phi_{\sev}
\left [ 
\frac{\log(\realrv)-\mu}{\sigma} \right]=
\frac{\beta}{\weibscale}
\left (\frac{\realrv}{\weibscale} \right )^{\beta-1}
\exp \left [-\left (\frac{\realrv}{\weibscale} \right )^{\beta}
\right ] \nonumber
\\
 h(\realrv;\mu, \sigma)&=&
\frac{1}{\sigma\exp(\mu)}\left[\frac{t}{\exp(\mu)}  \right]^{\frac{1}{\sigma}-1} =
\,\, \frac{\beta}{\weibscale}
\left (\frac{\realrv}{\weibscale} \right )^{\beta-1},  \quad \realrv > 0.
\nonumber
\end{eqnarray} 
Then the Weibull $p$ quantile is $\rvquan_{p}=\exp[\mu +
\Phi^{-1}_{\sev}(p) \, \sigma]$.  The Weibull/$\SEV$ relationship
parallels the lognormal/normal relationship. The $\SEV$
parameterization is useful because location-scale distributions are
easier to work with.  As mentioned in
Section~\ref{section:location.scale}, transforming the Weibull
distribution into an $\SEV$ distribution allows the use of
general results for location-scale distributions which apply
directly to all such distributions, including the
Weibull, lognormal, and some other distributions.

The theory of extreme values shows that the Weibull distribution can
be used to model the minimum of a large number of independent positive
random variables from a certain class of distributions. Thus, extreme value
theory also suggests that
the Weibull distribution may be suitable.  The more common 
justification for its use is
empirical: the Weibull distribution can be used to model
failure-time data with decreasing or increasing hf. 

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Largest Extreme Value Distribution}
When $\grv$ has a largest extreme value distribution, we indicate this
by $ \grv \sim \LEV(\mu, \sigma)$. The largest extreme value
distribution cdf, pdf, and hf are
\begin{eqnarray*}  
F(\grealrv;\mu, \sigma)&=&
\Phi_{\lev}\left (\frac{\grealrv-\mu}{\sigma}
\right )
\\
 f(\grealrv;\mu, \sigma)&=&\frac{1}{ \sigma } \phi_{\lev}
\left ( 
\frac{\grealrv-\mu}{\sigma} \right)
\\
h(\grealrv;\mu, \sigma)&=& \frac{\exp
        \left (
        -\,\frac{\grealrv-\mu}{\sigma}
        \right )
      }
{ \sigma  \left \{
           \exp 
          \left [
           \exp
          \left (
           -\,\frac{\grealrv-\mu}{\sigma}
          \right )
          \right ]
            -1
          \right \}
}, \quad  -\infty < \grealrv < \infty
 \end{eqnarray*} 
where 
$\Phi_{\lev}(z)=\exp[-\exp(-z)]$ 
and
$\phi_{\lev}(z)=\exp[-z-\exp(-z)]$
are cdf and pdf for the
standardized $\LEV(\mu=0, \sigma=1)$ distribution. Here 
$-\infty <\mu < \infty$ is a location parameter and
$\sigma > 0$ is a scale parameter.
The $\LEV$ cdf, pdf, and hf are graphed in Figure~\ref{figure:distplot.lev.ps}
for $\mu=10$ and $\sigma=5,6,7$.

%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.lev.ps}
\caption{Largest extreme value 
cdf, pdf, and hf with  $\mu=10$ and
 $\sigma=$ 5, 6, and 7.}
\label{figure:distplot.lev.ps}
\end{figure}
%-------------------------------------------------------------------

The mean, variance, and quantile functions of the 
largest extreme value distribution are
$\E(\grv)= \mu + \sigma \gamma$,
$\var(\grv)= \sigma^{2} \pi^{2}/6$, and
$\grvquan_{p}=\mu + \Phi^{-1}_{\lev}(p) \, \sigma$
where $\Phi^{-1}_{\lev}(p)=
-\log \left [-\log(p) \right]$.
Note the close relationship between LEV and SEV: if $\grv \sim
\LEV(\mu, \sigma)$ then $-\grv \sim
\SEV(-\mu, \sigma)$ and $\Phi^{-1}_{\lev}(p) = -\Phi^{-1}_{\sev}(1-p)$.

The theory of extreme values shows that
the LEV distribution can be used to model the
maximum of a large number of random variables from a certain class of
distributions (which includes the normal distribution).  
As shown in Figure~\ref{figure:distplot.lev.ps}, the largest
extreme value pdf is skewed to the right.  The LEV hf
always increases but is bounded in the sense that $\lim_{\realrv
\to \infty} h(\realrv;\mu, \sigma)=1/\sigma$.  Although most
failure-time distributions are skewed to the right, the LEV
distribution is not commonly used as a model for failure times. This
is because the LEV distribution (like the SEV and normal
distributions) has positive probability of negative observations and there
are a number of other right-skewed distributions that do not have this
property.  Nevertheless, the LEV distribution could be used as a model
for life if $\sigma$ is small relative to $\mu>0$.

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Logistic Distribution}
\label{section:logistic.distribution.definition}

%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.logis.ps}
\caption{Logistic cdf, pdf, and hf with
$\mu=15$ and $\sigma=$ 1, 2, and 3.}
\label{figure:distplot.logis.ps}
\end{figure}
%-------------------------------------------------------------------
When $\grv$ has a logistic distribution, we indicate this by $\grv
\sim \LOGIS(\mu, \sigma)$.  The logistic distribution is a
location-scale distribution with cdf, pdf, and hf
\begin{eqnarray*}   
F(\grealrv;\mu, \sigma)&=&
\Phi_{\logis}\left (\frac{\grealrv-\mu}{\sigma}
\right )\\
 f(\grealrv;\mu, \sigma)&=&\frac{1}{ \sigma } \phi_{\logis}
\left ( 
\frac{\grealrv-\mu}{\sigma} \right) \\
h(\grealrv;\mu, \sigma)&=& \frac{1}{\sigma }\Phi_{\logis}\left (\frac{\grealrv-\mu}{\sigma}
\right ), \quad -\infty < \grealrv < \infty
 \end{eqnarray*} where $\Phi_{\logis}(z)=\exp(z)/[1+\exp(z)]$, and $\phi_{\logis}(z)=\exp(z)/\left [1+\exp(z)
\right ]^{2}$ are the cdf
and pdf, respectively, for a standardized $\LOGIS(\mu=0,
\sigma=1$). Here $-\infty< \mu < \infty$ is a location parameter and
$\sigma > 0$ is a scale parameter. The logistic cdf, pdf, and hf
are graphed in
Figure~\ref{figure:distplot.logis.ps} for location parameter $\mu=15$
and scale parameter $\sigma=1, 2$, $ and 3$.

For integer $m > 0$, $\E[(\grv-\mu)^{m}]=0$ if $m$ is odd, and
$\E[(\grv-\mu)^{m}]=2\sigma^{m}\, (m!)  		 \left [1-(1/2)^{m-1}
\right ] \sum_{i=1}^{\infty} (1/i)^{m}$ if $m$ is even. From this  
$\E(\grv)= \mu$ and $\var(\grv)= \sigma^{2} \pi^{2}/3$.
The $p$ quantile is
$\grvquan_{p}=\mu + \Phi^{-1}_{\logis} (p)\sigma $, where
$\Phi^{-1}_{\logis}(p)=\log[p/(1-p)]$ is the $p$ quantile of the
standard logistic distribution.

The shape of the logistic distribution is very similar to that of the
normal distribution; the logistic distribution has slightly ``longer
tails.'' In fact, it would require an extremely large number of
observations to assess whether data come from a normal or logistic 
distribution. The main difference between the distributions is in the
behavior of the hf in the upper tail of the
distribution, where the logistic hf levels off, approaching
$1/\sigma$ for large $y$.  For some purposes, the logistic
distribution has been preferred to the normal distribution because its
cdf can be written in a simple closed form.  With modern
software, however, it is not any more difficult to compute
probabilities from a normal cdf.



%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Loglogistic Distribution}
\label{section:loglogistic.distribution.definition}
%-------------------------------------------------------------------
\begin{figure}
\splusbookfigure{\figurehome/distplot.loglogis.ps}
\caption{Loglogistic cdf, pdf, and hf for
$t_{.5}=\exp(\mu)=1$ and $\sigma=$ .2, .4, and .6.}
\label{figure:distplot.loglogis.ps}
\end{figure}
%-------------------------------------------------------------------
When $\rv$ has a loglogistic distribution, we indicate this by
$\rv \sim \LOGLOGIS(\mu,\sigma)$.  If $\rv \sim \LOGLOGIS(\mu,\sigma)$
then $\grv=\log(\rv) \sim \LOGIS(\mu, \sigma)$.  The loglogistic cdf,
pdf, and hf are
\begin{eqnarray*}  
 F(\realrv;\mu, \sigma)&=&\Phi_{\logis}\left [\frac{\log(\realrv)-\mu}{\sigma}
\right ]\\
 f(\realrv;\mu, \sigma)&=&\frac{1}{ \sigma \realrv} \phi_{\logis}
\left [ 
\frac{\log(\realrv)-\mu}{\sigma} \right] \\
h(\realrv;\mu, \sigma)&=& \frac{1}{ \sigma \realrv} \Phi_{\logis}
\left [ 
\frac{\log(\realrv)-\mu}{\sigma} \right], \quad \realrv > 0
 \end{eqnarray*} 
where $\phi_{\logis}$ and $\Phi_{\logis}$
are the pdf and cdf, respectively, for a standardized 
$\LOGIS$, defined in Section~\ref{section:logistic.distribution.definition}. 
The median $t_{.5}=
\exp(\mu)$ is a scale parameter and $\sigma > 0$ is a shape
parameter.  The $\LOGLOGIS$ cdf, pdf and hf
are graphed in
Figure~\ref{figure:distplot.loglogis.ps} for scale parameter
$\exp(\mu)=1$ and $\sigma=.2,.4,$ and $.6$.

For integer $m>0, \E(\rv^{m})= \exp( m \mu) \,
\Gamma(1+m \sigma)\, \Gamma(1- m \sigma)$
where $\Gamma(x)$ is the gamma function. 
From this $\E(\rv) = \exp( \mu) \, \Gamma(1+\sigma) \,
\Gamma(1-\sigma)$ and $\var(\rv)= \exp( 2 \mu) \, 	\left [
\Gamma(1+2\sigma) \, \Gamma(1-2\sigma) -\Gamma^{2}(1+\sigma) \,
\Gamma^{2}(1-\sigma)\right ]$. 
Note that for values of $\sigma \geq 1$, the mean of
$\rv$ does not exist and for $\sigma \geq 1/2$, the variance of $\rv$
does not exist. The $p$ quantile function is
$\rvquan_{p}=\exp \left [\mu +
\Phi^{-1}_{\logis}(p)\, \sigma \right]$
where $\Phi^{-1}_{\logis}(p)$ is defined in
Section~\ref{section:logistic.distribution.definition}.

Corresponding to the similarity between the logistic and the normal
distributions, the shape of the loglogistic distribution is similar to
that of the lognormal distribution.


\section{Parameters and Parameterization}
\label{section:parameterization.choice}
The choice of $\thetavec$, a set of parameters (the values of which
are usually unknown) to describe a particular model, is
somewhat arbitrary and may depend on tradition, on physical
interpretation, or on having a model parameterization with desirable
computational properties for estimating parameters.  For example, the
exponential distribution can be written in terms of its mean $\theta$,
as in (\ref{equation:expo.definition}) 
or its constant hazard $\lambda=1/\theta$.
The $\mu,\sigma$ notation for the Weibull distribution allows us to
see connections with other location-scale-based distributions.
The traditional parameters of a normal distribution are
$\theta_{1}=\mu$ and $\theta_{2}=\sigma>0$, the mean and standard
deviation, respectively. An alternative with no restrictions on the
range of the parameters would be $\theta_{1}=\mu$ and
$\theta_{2}=\log(\sigma)$.  Another parameterization, which may have
better numerical properties for estimation with heavily censored data sets, is
$\theta_{1}=\mu+\norquan_{p}\sigma$ and $\theta_{2}=\log(\sigma)$
where $\norquan_{p}$ is the $p$ quantile of the standard normal
distribution.  The best value of $p$ to use depends on the amount
of censoring. In particular, if the sample contains
failure times with no censoring, 
choose $p$=.5 with $\norquan_{p}=0$ because then
$\thetahat_{1}$ (the maximum likelihood estimate of the mean) and
$\thetahat_{2}$ (the maximum likelihood estimate of the log standard
deviation) would be statistically independent (this is a well-known
result from statistical theory). 
Exercise~\ref{exercise:mu.sig.corr} explores this
issue more thoroughly.



\section{Generating Pseudorandom Observations from a Specified
Distribution} 
\label{section:generating.random.numbers}
Simulation (or Monte Carlo simulation) methods are becoming
increasingly important for many applications of statistics and,
indeed, quantitative analysis in general. In particular, it is
possible to determine, through simulation, numerical quantities that
are difficult or impossible to compute by purely analytical means.
This book uses a simulation approach in a number of methods, examples,
and exercises. A pseudorandom number generator is the basic building
block of any simulation application.  This section will show some
simple methods for generating pseudorandom numbers from specified
probability distributions. The bibliographic notes at the end of this
chapter give references for more technical details and more advanced
methods of generating pseudorandom numbers from specified
distributions.

\subsection{Uniform pseudorandom number generator}
Most computers, data analysis software, and spreadsheets provide a
pseudorandom number generator for the uniform distribution on
$(0,1)$
[denoted by $\UNIF(0,1)$].
This distribution has its probability distributed uniformly from
$(0,1)$. The cdf and pdf of the $\UNIF(0,1)$ distribution are $
F_{U}(u)=u$ and $f_{U}(u)=1, \quad 0 < u < 1.$ Pseudorandom numbers
from the $\UNIF(0,1)$ distribution can be used  easily to generate random
numbers from other distributions, both discrete and continuous.

\subsection{Pseudorandom observations from continuous distributions}
\label{section:prandom.cont.dist}
Suppose $U_{1},\dots,U_{n}$ is a pseudorandom sample from a
$\UNIF(0,1)$. Then if $\rvquan_{p}=F_{T}^{-1}(p)$ is the quantile
function for the distribution of the random variable $T$ from which
a sample of pseudorandom numbers is desired,
$T_{1}=F_{T}^{-1}(U_{1})$, \dots, $T_{n}=F_{T}^{-1}(U_{n})$ is a
pseudorandom sample from $F_{T}$. For example, to generate a
pseudorandom sample from the Weibull distribution for specified
parameters $\weibscale$ and $\beta$, first obtain the $\UNIF(0,1)$
pseudorandom sample $U_{1},\dots,U_{n}$ and then compute
$T_{1}=\weibscale \left [-\log(1-U_{1})
\right ]^{1/\beta},$ \ldots, $T_{n}=\weibscale \left [-\log(1-U_{n})
\right ]^{1/\beta}$. Similarly, for the lognormal distribution the
pseudorandom sample can be obtained from $T_{1}=\exp[\mu + \Phi_{\nor}
^{-1}(U_{1})\sigma], \ldots, T_{n}=\exp[\mu + \Phi_{\nor}
^{-1}  (U_{n})\sigma  ]$.

\subsection{Efficient generation of  censored pseudorandom samples}
\label{section:pseudo.random.censored}
This section shows how to generate pseudorandom {\em censored} samples
from a specified cdf $F(\realrv; \thetavec)$.  Such samples are
useful for implementing simulations like those used throughout the
book and for bootstrap methods like those described in
Chapter~\ref{chapter:bootstrap}.
\subsubsection{General Approach}
Let $U_{(i)}$ denote the $i$th order statistic from a random sample
of size $n$ from a $\UNIF(0,1)$ distribution.  Using the properties of
order statistics, the conditional distribution of $U_{(i)}$ given
$U_{(i-1)}$ is
\begin{eqnarray*}
\Pr  \left [ U_{(i)}\leq u|U_{(i-1)}=u_{(i-1)}\right ]=1 -
\left [\frac{1-u } { 1-u_{(i-1)} }
\right ]^{(n-i+1)}, \quad u \ge u_{(i-1)}.
\end{eqnarray*}
Let $U$ be a pseudorandom $\UNIF(0,1)$ variable.  Then using the method
described in Section~\ref{section:generating.random.numbers}, given
$U_{(i-1)}$ (where $U_{(0)} =  0$), a pseudorandom observation
$U_{(i)}$ is
\begin{displaymath}
U_{(i)}=1-[1-U_{(i-1)}] \times \left (1 - U \right )^{1/(n-i+1)}, 
\quad i=1,\dots, n.
\end{displaymath}
Pseudorandom uniform order statistics generated in this way can be
used to generate failure and time censored samples.
\subsubsection{Failure censored samples}
\label{section:failcensam}
The algorithm to generate a pseudorandom failure-censored sample
(Type~II censored) with $n$ units and $r$ failures is as follows:
\begin{enumerate}
\item
Generate $U_{1}, \dots, U_{r}$ pseudorandom observations from
a $\UNIF(0,1)$.
\item
Compute the uniform pseudorandom order statistics
\begin{eqnarray*}
U_{(1)}&=& 1-[1-U_{(0)}]\times(1-U_{1})^{1/n}
\\
U_{(2)}&=& 1-[1-U_{(1)}]\times (1-U_{2})^{1/(n-1)}
\\
& \vdots &
\\
U_{(r)}&=& 1-[1-U_{(r-1)}] \times (1-U_{r})^{1/(n-r+1)}
\end{eqnarray*}
\item
The pseudorandom sample from $F(\realrv;\thetavec)$ is
\begin{displaymath}
\rv_{(i)}=F^{-1}[U_{(i)};\thetavec], \quad i=1, \dots, r.
\end{displaymath}
For example, for a log-location-scale based
cdf with  $F(\realrv; \thetavec)=\Phi \left [ 
(\log(\realrv)-\mu)/\sigma
		            \right ]$,
\begin{displaymath}
\rv_{(i)}=\exp \left \{
\mu+\sigma \Phi^{-1}[U_{(i)}]
	       \right \}, \quad i=1, \dots, r.
\end{displaymath}
\end{enumerate}
\subsubsection{Time censored samples}
The algorithm to generate a pseudorandom time-censored sample
(Type~I censored) with $n$ units and censoring time $\censortime$
uses the formulas in Section~\ref{section:failcensam} to generate
$\rv_{(1)},\rv_{(2)}, \dots$ sequentially. The process continues
until a failure time observation, say $\rv_{(i)}$, exceeds
$\censortime$. This yields a censored sample consisting of
$\rv_{(1)}, \dots,
\rv_{(i-1)}$ failure times and $(n-i+1)$ censored observations. Specifically,
define $U_{(0)} = 0$, start with $i=1$, and generate the sequence as follows:
\begin{enumerate}
\item
Generate a new pseudorandom observation $U_{i}$ from a $\UNIF(0,1)$.
Compute $U_{(i)}=1-[1-U_{(i-1)}] \times \left (1-U_{i} \right )^{1/(n-i+1)}$ 
and  $\rv_{(i)}=F^{-1}[U_{(i)}; \thetavec ]$.
\item
If $\rv_{(i)} > \censortime$, stop and the sample consist 
of the failure time $\rv_{(1)}, \dots, \rv_{(i-1)}$ 
and $(n-i+1)$ censored observations. 
\item
If $\rv_{(i)} \le \censortime$ increment $i$ and return to step 1.
\end{enumerate}
Note that if $\rv_{(1)} >
\censortime$,
there are no failures before $\censortime$.



\subsection{Pseudorandom observations for discrete distributions}
The same general idea 
used in Section~\ref{section:prandom.cont.dist} 
can be used to generate data from a discrete
distribution. The process, however, can be a little more
complicated if the discrete distribution quantiles cannot be computed
directly. The multinomial distribution described in
Section~\ref{section:multinomial.failure.model} is a good example.
Starting the values $\pi_{1},\dots, \pi_{m}$, compute $F(\realrv_{i})$
using (\ref{equation:f.is.sum.of.pi}). Then for each $U_{i},
i=1,\dots,n$, $T_{i}$ is the smallest value of $t$ such that
$F(\realrv) \geq U_{i}$. If $m$ is not too large, it is possible to use a
look-up-table to determine $T_{i}$ as a function of $U_{i}$.
Otherwise, it is necessary to search through the possible values of
$F(\realrv)$ to find the first one exceeding $U_{i}$.


%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section*{Bibliographic Notes}
Johnson, Kotz and Balakrishnan~(1994, 1995) provide detailed
information on a wide range of different continuous probability
distributions functions.  Evans, Hastings, and Peacock~(1993) provide
a brief description and summary of properties of a large number of
parametric distribution including most, but not all, of the
distributions outlined in this chapter.  Crow and Shimizu~(1988)
provide detailed information on the lognormal distribution.
Galambos~(1978) is an important reference for the asymptotic theory of
extreme value distributions, providing extensive theory and
background.  Balakrishnan~(1991) gives detailed information on the
logistic distribution. Kennedy and Gentle~(1980) provide detailed
information on generation of pseudorandom numbers from the uniform
and a variety of other special distributions.  Morgan~(1984) and
Ripley~(1987) do the same and also provide useful material on how to
do stochastic simulations. Kennedy and Gentle~(1980, pages 225-227)
and Castillo~(1988, pages 58-63) describe methods for
generating pseudorandom order statistics.

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\section*{Exercises}


\begin{exercise}
Show that for a continuous $F(t)$,
$h(\realrv) = 1/\theta, t > 0$ is a constant (i.e., not depending
on $\realrv$) if and only if $\rv \sim \EXP(\theta)$.
\end{exercise}



\begin{exercise}
Derive expressions for the mean, variance,
and quantile functions of the exponential distribution.
\end{exercise}


\begin{exercise}
Derive the expression for $\Phi_{\sev}^{-1}(p)$
based on the expression for 
$\Phi_{\sev}(z)$ in
Section~\ref{section:sev.distribution.definition}.
\end{exercise}


\begin{exercise}
 Show that if $\grv$ has a SEV$(\mu, \sigma)$ distribution then
$-\grv$ has a LEV$(-\mu, \sigma)$ distribution.
\end{exercise}

\begin{exercise1}
Let $\rv \sim \WEIB(\mu,\sigma)$, $\weibscale=\exp(\mu)$,
and $\beta=1/\sigma$.
\begin{enumerate}
\item 
\label{exercise.part:weibull.moment.function}
For $m>0$, show that	 
$
\E(\rv^{m} )= \weibscale^{m} \Gamma(1+m/\beta)
$
where $\Gamma(x)$
is the gamma function.
\item Use the result in 
\ref{exercise.part:weibull.moment.function}
to show that $
\E(\rv)=\weibscale \Gamma(1+1/\beta)
$ and $
\var(\rv)=\weibscale^{2} 		 \left [
\Gamma(1+2/\beta)-\Gamma^{2}(1+1/\beta) \right ].
$
\end{enumerate}
\end{exercise1}


\begin{exercise}
Consider the Weibull distributions with parameters $\eta=10$ years,
and $\beta=$.5, 1, 2, and 4.
\begin{enumerate} 
\item 
Compute (using a computer if available) and graph the Weibull
hf for $t$ ranging between 0 and 10.
\item
Explain the practical interpretation of the hf
at $t=1$ and $t=10$ years.
\item
Compute and plot the Weibull cdfs over the same range of $t$. For
which shape parameter value
is the probability of failing the largest at 1 year? At 10 years? Explain.	 
\end{enumerate}
\end{exercise}
\begin{exercise}
Consider the Weibull $h(\realrv)$.  Note that
when $\beta=1$, $h{(\realrv)}$ is constant and that when $\beta=2$,
$h(\realrv)$ increases linearly.  	 Show that if
\begin{enumerate} 
\item 
$0< \beta < 1$, then $h(\realrv)$ is decreasing in $t$.  	 
\item 
$1< \beta < 2$, then $h(\realrv)$ is concave increasing.
\item 
$\beta > 2$, then $h(\realrv)$
is convex increasing.  	 
\end{enumerate}
\end{exercise}

\begin{exercise}
Starting with equation (\ref{equation:weibull.cdf}), show that the
distribution of $\grv=\log(\rv)$ is $\SEV[\log(\weibscale),1/\beta]$.
\end{exercise}

\begin{exercise}
Derive the expression for $\Phi_{\logis}^{-1}(p)$ based on the
expressions for $\Phi_{\logis}(z)$ given in
Section~\ref{section:logistic.distribution.definition}.
\end{exercise}

\begin{exercise}
Even though, theoretically, the SEV, LEV, LOGIS, and NOR distributions
can take on negative values, the probability of nonpositive outcomes is 
negligible for certain combinations of parameters.
\begin{enumerate}
\item
For the combinations of parameter values for the SEV distribution
shown in Figure~\ref{figure:distplot.sev.ps}, compute $\Pr(\grv \leq
0)$.
\item
For the SEV, LEV, LOGIS, and NOR distributions, derive a general
expression relating
$\mu$ and $\sigma$, and guaranteeing that $\Pr(\grv \leq 0) \leq
\epsilon $.
\end{enumerate}
\end{exercise}


\begin{exercise}
Show that if $\rv$ is $\LOGNOR(\mu,\sigma)$ then
      $1/\rv$ is $\LOGNOR(-\mu, \sigma)$.
\end{exercise}

\begin{exercise}
The exponential distribution is said to possess a ``memoryless''
property.  This memoryless property implies that a used unit is just
as reliable as one that is new---that there is no wear out.
Probabilistically this memoryless property can be stated as $\Pr(T
\leq \delta) = \Pr(T\leq t_{0}+ \delta|T>t_{0})$ for any $t_{0}>0$.
Show that for a continuous random variable, this memoryless property
holds if and only if $T \sim\EXP(\theta)$.
\end{exercise}


\begin{exercise1}
Show that if $\grv$ is $\SEV(\mu, \sigma)$ then 
$\E(\grv)=\mu-\sigma \gamma$
and $ \var(\grv)=\sigma^{2}\pi^{2}/6$
where $\gamma \approx 0.5772$, in this
context, is known as Euler's constant.  Observe that from
integral tables one gets $\int^{\infty}_{0} \log(x) \exp(-x) dx =
-\gamma$ and $\int^{\infty}_{0} [\log(x)]^{2} \exp(-x) dx =
\pi^{2}/6+\gamma^{2}$.
\end{exercise1}

\begin{exercise1}
Assume that $\rv$ is $\LOGNOR(\mu, \sigma)$ and $m$ is an arbitrary
real number.
\begin{enumerate} 		
\item 
\label{exercise.part:lognormal.moment.function}
Show that $\E(\rv^{m} )= \exp\left (\mu m + .5 \sigma^{2} m^{2} \right )$.
\item 
Use the result
in \ref{exercise.part:lognormal.moment.function} to show that 	 
$\E(\rv)=\exp \left
(\mu + .5 \sigma ^{2} \right )$ and $\var(\rv)= \exp
\left [2 \mu + \sigma^{2} \right ] 		 \left
[\exp(\sigma^{2}) -1 \right ]$.
\end{enumerate}
\end{exercise1}

\begin{exercise}
\label{exercise:weibullcv}
The coefficient of variation, $\gamma_{2},$ is a useful scale-free
measure of relative variability for a random variable.
\begin{enumerate}
\item
Derive an expression for the coefficient of variation for the
Weibull distribution.
\item
Compute $\gamma_{2}$ for all combinations of $\beta=.5, 1, 3, 5$ and
$\eta=50, 100.$ Also, draw (or use the computer to draw) a graph of
the Weibull pdfs for the same combinations of parameters.
\item
Explain the effect that changes in $\eta$ and $\beta$ have on the
shape of the Weibull density and the effect that they have on
$\gamma_{2}.$
\end{enumerate}
\end{exercise}

\begin{exercise}
The coefficient of skewness, $\gamma_{3},$ is a useful scale-free
measure of skewness in the distribution of a random variable. Do
Exercise~\ref{exercise:weibullcv} for the coefficient of skewness.
\end{exercise}

\begin{exercise}
\label{exercise:weibull.sim}
Generate $500$ pseudorandom observations from a ``parent'' lognormal
distribution with $\mu=5$ and $\sigma=.8$.
\begin{enumerate}
\item
Compare the histogram of the observations with a plot of the
parent lognormal density function.
\item
Compute the sample median of the $500$ observations and compare it with
the median of the parent lognormal distribution.
\item
Compute the sample mean of the $500$ observations and compare it with
the mean of the parent lognormal distribution.
\item
Compute the sample standard deviation of the $500$ observations and
compare with the standard deviation of the parent lognormal distribution.
\end{enumerate}
\end{exercise}

\begin{exercise}
Repeat Exercise~\ref{exercise:weibull.sim} using a Weibull
distribution with $\eta=100$ and $\beta=.5.$  Comment on how the results
differ from those with the lognormal distribution.
\end{exercise}


\begin{exercise1}
\label{exercise:lognormal.hazard}
Consider a lognormal distribution with cdf $F(\realrv;\mu,\sigma)$.
\begin{enumerate}
\item     
Show that for any values of $\mu$ and $\sigma$, $h(\realrv)$ always
has the following characteristics: $\lim_{\realrv
\rightarrow \infty}h(\realrv)=0$, $\lim_{\realrv
\rightarrow 0}h(\realrv)=0$, and $h(\realrv)$ has a
unique maximum at a point $\realrv_{\max}$, with $0 <
\realrv_{\max} < \infty$.
\item
\label{exercise.part:max.lognormal.hazard.function} 
Show that $\realrv_{\max}$ satisfies the relationship
\begin{displaymath}
h(\realrv_{\max})=\frac{1}{\realrv_{\max}} \left [
1+\frac{\log(\realrv_{\max})-\mu}{\sigma^{2}} \right ].
\end{displaymath}
\item 
\label{exercise.part:max.lognormal.hazard.bounds} 
Use the result in 
\ref{exercise.part:max.lognormal.hazard.function} to show that
\begin{displaymath} 	 
\exp(\mu-\sigma^{2}) \leq
t_{\max} \leq \exp(\mu-\sigma^{2}+1)
\end{displaymath} 	 
and
\begin{displaymath} 		
\Phi_{\nor}(-\sigma) \leq
F(\realrv_{\max};\mu, \sigma) \leq \Phi_{\nor} \left
(-\sigma+\frac{1}{\sigma} \right ).
\end{displaymath}
\item
Comment on the effect that $\sigma$ has on how ``early'' or ``late''
in time the lognormal hf reaches its maximum. In particular, show
that (i) for large values of $\sigma$, $h(t)$ is increasing only on
an interval of negligible probability, (ii) for small values of
$\sigma$, $h(t)$ is increasing in an interval that has at least a
probability of about $50\%$.
\item 
Plot the hf when the parameters $(\mu,
\sigma)$ values are $(0, 5);$ $ (0, 1/5); (1, 5); (1, 1/5)$.
Comment on the adequacy of the probability bounds given in 
\ref{exercise.part:max.lognormal.hazard.bounds}.
\end{enumerate}
\end{exercise1}

\begin{exercise1}
Show that for any values of the parameters $(\mu, \sigma)$,
      the normal $h(\grealrv)$
      is always increasing.
\end{exercise1}


