---
title: 'STAT 687 Homework #2 - Solution'
fontsize: 11in
geometry: margin=1in
output:
  html_document:
    css: ../css/homework.css
    fig_cap: yes
    highlight: pygments
    theme: readable
    toc: yes
  pdf_document:
    fig_cap: yes
    includes:
      in_header: ../css/jkf.header.tex
citation_package: natbib
---

```{r,echo=FALSE,message=FALSE, warning=FALSE}
source('../css/setup.R')
output <- getYAML('stat687-hw2-soln.Rmd')$output
if(!    'SMRD'%in%library()$results) install.packages('SMRD')     ; library(SMRD)
if(!'survival'%in%library()$results) install.packages('survival') ; library(survival)
if(!  'actuar'%in%library()$results) install.packages('actuar')   ; library(actuar)
if(!     'evd'%in%library()$results) install.packages('evd')      ; library(evd)
```

## From the text problem # 3.5

The supplier of an electromechanical control for a household appliance ran an accelerated life test on sample controls.  In the test, 25 controls were put on test and run until 30 thousand cycles had been accumulated.  Failures occurred at 5, 21,and 28 thousand cycles.  The other 22 controls did not fail by the end of the test.
  
(a) Compute and plot a nonparametric estimate for $F(t)$.

> A nonparametric estimate for $F(t)$ can be easily produced using a 'life data' object from the SMRD pacakge.  Life data objects are created with the function `frame.to.ld` which requires the data to be formatted into a data frame.  

> If the data set is pre-loaded as part of the SMRD package (as is the case for the data set in this exercise), it has already been formatted into a data frame.  In this case the life data object can be created by simply entering the name of the data set into `frame.to.ld`, for example

```{r, echo=TRUE}
prob3_5.ld <- frame.to.ld(frame = prob3_5,
                          response.column = 1,
                          censor.column = 2,
                          time.units = "Kilocycles")
```


> NOTE: The complete list of pre-loaded data sets in the SMRD package, can be found by running `data(package = 'SMRD')` in the R console.

> A plot of the nonparametric estimate for $F(t)$ is produced by plotting the life.data object, as shown in Figure 1.  This exercise does not require us to plot the confidence bands along with the estimate, so we set the `band.type` argument to "none". The `y.range` argument is added because of the way R dynamically sets axis values for a plot.  Try running following code without the `y.range` argument.  This problem could easily be fixed such that the `y.range` argument would not need to be set...but the fix would cause bigger issues for other exercises. 

```{r,echo=2, fig.cap="Plot of the nonparametric estimate of the cdf for Problem 3.5"}
par(family="serif")
plot(prob3_5.ld, band.type = "none", ylim = c(0,0.1))
```

> To compute the values of $\hat{F(t_{i})}$ and present them in a \LaTeX table the following code is used. __NOTE: to render the \LaTeX table you must add the chunk option `results='asis'`.__ 


```{r, echo=TRUE}
xtab <- xtable::xtable(print(cdfest(prob3_5.ld))$table,
                       align = c('ccccccc'),
                       digits = rep(c(0,4), c(3,4)),
                       caption = "Nonparametric CDF Estimate for Problem 3.5")
print(xtab, 
      include.rownames = F,
      caption.placement = 'top',
      type = switch(output, 'html' = 'html', 'pdf' = 'latex'),
      table.placement = 'h',
      comment = F)
```


(b) Compute an approximate 95% confidence interval for the probability that an electromechanical device from the same production process, tested in the same way, would fail before 30 thousand cycles.  Use the conservative binomial approach.

> In Example 3.1 we see that the nonparametric estimate for $\hat{F}(\text{30 kilocycles})$ with singly censored data is
$$\frac{\#\text{ failed up to 30 kilocycles}}{\#\text{ at risk}}=\frac{3}{25}.$$ 

> The Clopper-Pearson 'exact' confidence interval based on the cumulative probabilities of the binomial distribution (i.e., the exact distribution rather than an approximation) gives a lower 95\% confidence bound of
$$\left(1+\frac{(25-3+1)\times \mathcal{F}(.975,50-6+2,6)}{3}\right)^{-1}=0.02546$$

> and an upper 95% confidence bound of
$$\left(1+\frac{25-3}{(3+1)\times \mathcal{F}(.925,6+2,50-6)}\right)^{-1}=0.31219$$

> where $\mathcal{F}(p,df_{1},df_{2})$ denotes the $p^{th}$ quantile of an $F$ distribution with $df_{1}$ and $df_{2}$ degrees of freedom.
    
* * * *

(c) Compute an approximate 95% confidence interval for the probability that an electromechanical device from the same production process, tested in the same way, would fail before 30 thousand cycles.  Use the normal-approximation method based on $Z_{\hat{F}(30)}\sim NOR(0,1)$ 

> Using the $Z_{\hat{F}(30)}\sim NOR(0,1)$ appoximate method results in a lower 95\% confidence bound of
$$\frac{3}{25}-1.96\times \sqrt{\frac{3}{25}\times\left(1-\frac{3}{25}\right)\times\frac{1}{25}}=-0.007$$

> and an upper 95% confidence bound of
$$\frac{3}{25}+1.96\times \sqrt{\frac{3}{25}\times\left(1-\frac{3}{25}\right)\times\frac{1}{25}}=0.247$$

* * * *

(d) Explain why, in this situation, the approach in part (b) would be preferred to the approach in part (c)

> The CI in part (b) is conservative as it based on the exact distribution of the value of $F(t)$ (i.e. the binomial) while the method used in part (c) is an approximation of the binomal distribution. Further, the approximation in part (c) has a negative lower bound which is not possible.

****

(e) The appliance manufacturer is really interested in the probability of the number of days to failure for its product.  Use-rate differs from household to household, but the average rate is 2.3 cycles per day.  What can the manufacturer say about the proportion of devices that would fail in 10 years of operation (the expected technological life of the product)?

> We can convert usage in days to usage in cycles by making the following conversion
$$10 \text{ years}\times\left(365\hspace{3pt} \frac{\text{days}}{\text{year}}\right) \times\left(2.3\hspace{3pt} \frac{\text{cycles}}{\text{day}}\right) \approx 8.4\times10^3 \text{ cycles}.$$

> Therefore, we want to compute a 95% CI for $F(8.4)$ where usage is measured in kilocycles. From the plot in part a) the manufacturer can say that with 95% confidence the proportion of electromechanical control failures after 10 years of use at a use rate of 2.3 cycles per day is in the interval (0,0.24)

> Alternatively, if the manufacturer uses the binomial (Clopper-Pearson) approximation, the reported 95% confidence interval would be $(0.01,0.204).$

> If the manufacturer uses the normal approximation, the reported 95% confidence interval would be $(0.01,0.204).$

> Finally, if the manufacturer used R to compute the 95% confidence interval by plotting the Kaplan-Meier estimate from a "Surv" object, the reported interval would be $(0,0.114).$
  
> If the electromechanical controls experienced only one of two usage rates $r_{1} \text{ or } r_{2}, \text{ where } r_{1}>r_{2}$ such that the lifetime of any control device follows either $F_{1}(t|\theta_{1})$ or $F_{2}(t|\theta_{2})$.  The lifetime of the overall population would then depend on the proportion of units experiencing each usage rate denoted $\zeta_{i}, i=1,2,\hspace{3pt} \text{where}\hspace{3pt} \zeta_{2}=1-\zeta_{1}$. The probability of failure for the population would then be expressed as
$$F_{_T}(t)=\sum_{i=1}^{2}\zeta_{i}F_{i}(t|\theta_{i})$$

* * * *

(f) Refer to part (e).  Describe an appropriate model to use when use-rate varies in the population of units.  To simplify, start by assuming that there are only two differnt use-rates.  Discuss, using appropriate expressions

`r if(output=='pdf') '\\newpage'`

## From the text problem # 3.7

Consider the Plant 1 heat exchanger data in Figure 3.1

(a) Write the likelihood for these data in terms of $\pi_{1},\pi_{2},\pi_{3},$ and $\pi_{4}$

> In terms of $\boldsymbol{\pi}=(\pi_{1},\pi_{2},\pi_{3},\pi_{4})$, the likelihood can be written as
$$L(\boldsymbol{\pi})=C[\pi_{1}]^1[\pi_{2}]^2[\pi_{3}]^2[\pi_{4}]^{95}$$

> where $C$ is a constant that does not depend on $\mathbf{\pi}$.

* * * *

(b) Write the likelihood for these data in terms of $p_{1},p_{2},p_{3},$ and $p_{4}$

> In terms of $\mathbf{p}=(p_{1},p_{2},p_{3},p_{4})$, the likelihood can be written as
$$L(\mathbf{p})=C[p_{1}]^1[p_{2}(1-p_{1})]^2[p_{3}(1-p_{1})(1-p_{2})]^2[(1-p_{1})(1-p_{2})(1-p_{3})]^{95}$$
 
> because $p_{4}=1$ in this case.
    
`r if(output=='pdf') '\\newpage'` 

## From the text problem # 3.18

Explain why the nonparametric estimate of $F(t)$ is a set of points for the heat exchanger data in Example 3.6 but a step-function for the shock absorber data in Example 3.9

> The difference in these examples is what is known when a failure occurs.  In the heat exchanger problem, failures are only known to occur during the inspections at the end of the year.  If a heat exchanger pipe failed prior to the inspection, the exact failure time would be unknown.  It would be incorrect to plot the CDF as a step function between the inspections, since zero, one or many failures may have occured during the year, we just don't know when.

> The battery data set lists exact failure times, so if no failures were observed between times $t_{1}$ and $t_{2}$, we know that no units failed between these two times and it is reasonable to assume that the value of nonparametric estimate of the cdf is constant between these times.  

`r if(output=='pdf') '\\newpage'`

## From the text problem # 3.23

Consider the relationship $S(t_{i})=\exp[-H(t_{i})],$ where $H(t)$ is  the cumulative hazard function.  Note that a nonparametric ML estimator (based on the product-limit estimator) of $H(t)$ without assuming a distributional form is

$$\hat{H}(t_{i})=-\sum_{j=1}^{i} \log(1-\hat{p_{j}})\approx \sum_{j=1}^{i}\hat{p}_{j}=\sum_{j=1}^{i}\frac{d_{j}}{n_{j}}=\hat{\hat{H}}(t_i)$$

$\hat{\hat{H}}(t_{i})$ is known as the Nelson-Aalen estimator of $H(t_{i}).$  Thus $\hat{\hat{F}}(t_{i})=1-\exp[-\hat{\hat{H}}(t_{i})]$ is another nonparametric estimator for $F(t_{i}).$

* * * *

(a) Give conditions to assure a good agreement between $\hat{H}(t_{i})$ and $\hat{\hat{H}}(t_{i})$ and thus between $\hat{F(t_{i})}$ and $\hat{\hat{F}}(t_{i}).$  

> To have good agreement we must have
$$-\sum_{j=1}^{i}\log\left(1-\hat{p}_j\right) \approx \sum_{j=1}^{i} \hat{p}_j.$$

> Looking at the Taylor series expansion 
$$\log(1-\hat{p}_j) = -\sum_{n=1}^{\infty} \frac{\hat{p}_j^n}{n} \text{, if } \hat{p}_j<1$$

> From this we can see if $\hat{p}_j$ is small the first term in the Taylor series will accurately approximate it.  Therefore we see that we'll have good agreement if these probabilities are relatively small.

* * * *

(b) Use the delta method to compute approximate expressions for $Var[\hat{H}(t_{i})]$ and $Var[\hat{\hat{H}}(t_{i})].$  Comment on the expressions you get. 

> From equation 3.9 (Greenwood's Formula) we know 
$$\hat{Var}\left(\hat{F}(t_{i})\right)=\left(\hat{S}(t_{i})\right)^{2}\sum_{j=1}^{i}\frac{\hat{p}_{j}}{n_{j}(1-\hat{p_{i}})}.$$  

> From the delta method we also know $\hat{F}(t_{i})$ and $\hat{H}(t_{i})$ are related by 
$$\hat{H}(t_{i})=g\left(\hat{F}(t_{i})\right)=-\text{log}(1-\hat{F}(t_{i})),$$ 

> thus
$$
\begin{aligned}
Var\left(\hat{H}(t_{i})\right)\approx\left(g'(\hat{F}(t_{i}))\right)^{2}\hat{Var}\left(\hat{F}(t_{i})\right)&=\frac{1}{\left(\hat{S}(t_{i})\right)^{2}}\left(\hat{S}(t_{i})\right)^{2}\sum_{j=1}^{i}\frac{\hat{p}_{j}}{n_{j}(1-\hat{p}_{j})}\\
&=\sum_{j=1}^{i}\frac{\hat{p}_{j}}{n_{j}(1-\hat{p}_{j})}
\end{aligned}
$$

> Using similar arguments as on top of pg. 55 we can find the approximate variance for $\hat{H}(t_{i})$
$$Var\left(\hat{\hat{H}}(t_{i})\right)=Var\left(\sum_{j=1}^{i}\hat{p}_{j}\right)\approx\sum_{j=1}^{i}Var\left(\hat{p}_{j}\right)=\sum_{j=1}^{i}\frac{\hat{p}_{j}(1-\hat{p}_{j})}{n_{j}}$$

> These look pretty similar.

****

(c) Compare the Nelson-Aalen estimate of $F(t)$ and compare with the estimate computed in Exercise 3.20.  Describe similarities and differences.

> Run the following R-Code to obtain the plots and tables. (the fleming-harrington method is the Nelson-Aalen estimate).  The similaries occur at the beginning, and after a few failures they estimates start to diverge.  As expected, the Nelson-Aalen estimate for $\hat{\hat{S}}(t_{i})$ are always greater than (or equal to) the Kaplan-Meier estimate, see part (d)

```{r,echo=TRUE, fig.cap="Comparison of Nelson-Aalen and Kaplan-Meier Estimates for the Diesel Engine Fan Data Set (Table C.1)", fig.pos='h'}
fan_dat <- matrix(c(
450.000,1,460.000,0,1150.000,1,1150.000,1,1560.000,0,1600.000,1,
1660.000,0,1850.000,0,1850.000,0,1850.000,0,1850.000,0,1850.000,0,
2030.000,0,2030.000,0,2030.000,0,2070.000,1,2070.000,1,2080.000,1,
2200.000,0,3000.000,0,3000.000,0,3000.000,0,3000.000,0,3100.000,1,
3200.000,0,3450.000,1,3750.000,0,3750.000,0,4150.000,0,4150.000,0,
4150.000,0,4150.000,0,4300.000,0,4300.000,0,4300.000,0,4300.000,0,
4600.000,1,4850.000,0,4850.000,0,4850.000,0,4850.000,0,5000.000,0,
5000.000,0,5000.000,0,6100.000,0,6100.000,0,6100.000,0,6100.000,1,
6300.000,0,6450.000,0,6450.000,0,6700.000,0,7450.000,0,7800.000,0,
7800.000,0,8100.000,0,8100.000,0,8200.000,0,8500.000,0,8500.000,0,
8500.000,0,8750.000,0,8750.000,0,8750.000,1,9400.000,0,9900.000,0,
10100.000,0,10100.000,0,10100.000,0,11500.000,0),
byrow = T, ncol = 2)

s1 <- Surv(fan_dat[,1],fan_dat[,2],type='right')

f1 <- survfit(s1~1,type="kaplan-meier")
f2 <- survfit(s1~1,type="fleming-harrington")

plot(f1, 
     ylim = c(.5,1), 
     xlab = 'Time (t)',
     ylab = 'Fraction Surviving',
     las = 1)
par(new = T)
plot(f2,
     col = 2,
     ylim = c(.5,1),
     axes = FALSE)

legend("bottomleft", 
       c(expression("Kaplan-Meier"),
         expression("Nelson-Aalen")),
       col = 1:2, bty = "n", 
       lwd = 2, xjust = 0.5)
```

```{r,echo=TRUE, results='markup'}
summary(f1)
```
`r if(output=='pdf') '\\newpage'`
```{r,echo=TRUE, results='markup'}
summary(f2)
```
* * * *

(d) Show that $\hat{\hat{H}}(t_{i})<\hat{H}(t_{i})$ and that $\hat{\hat{F}}(t_{i})<\hat{F}(t_{i}).$

> We know that $\hat{H}(t_{i})=-\sum_{j=1}^{i}\text{log}(1-\hat{p}_{j})$. Using the the Taylor Series expansion for $\text{log}(1-\hat{p}_{j})$ shown in part (a) results in
$$\hat{H}(t_{i})=\sum_{j=1}^{i}\left(\sum_{n=1}^{\infty}\frac{\hat{p}_{j}^n}{n}\right).$$

> Because $\hat{p}_{j}>0, \forall j$ we know that $\hat{H}(t_{i})>\sum_{j=1}^{i}\hat{p}_{j}=\hat{\hat{H}}(t_{i})$. 

> Therefore,
$$
\begin{aligned}
\hat{H}(t_{i})&>\hat{\hat{H}}(t_{i})\\
-\text{exp}[\hat{H}(t_{i})]&>-\text{exp}[\hat{\hat{H}}(t_{i})]\\
1-\text{exp}[\hat{H}(t_{i})]&>1-\text{exp}[\hat{\hat{H}}(t_{i})]\\
\hat{F}(t_{i})&>\hat{\hat{F}}(t_{i})\\
\end{aligned}
$$

<!--- From the text problem ## 4.4
 
We are given that $Y\sim SEV(\mu,\sigma)$.  Thus, $F_{_Y}(y)=P(Y\le y)=1 -\text{exp}{[-\text{exp}{\frac{(y-\mu)}{\sigma}}]}$
$$f(x) = \frac{1} {\beta} e^{\frac{x-\mu}{\beta}}e^{-e^{\frac{x-\mu}{\beta} }}$$--->

`r if(output=='pdf') '\\newpage'`

## From the text problem # 4.5

Let $T \sim WEIB(\mu, \sigma), \eta=\exp(\mu),$ and $\beta=1/\sigma.$

(a) For $m>0,$ show that $E(T^{m})=\eta^{m}\Gamma(1+m/\beta),$ where $\Gamma(x)$ is the gamma function.

> Need to show that after implementing the parameter substitutions suggested in the text, the $m^{th}$ moment for the random variable $T \sim Weibull(\eta, \beta)$
$$E[T^m]=\int_0^\infty T^{m}\frac{\beta}{\eta}\left(\frac{t}{\eta}\right)^{\beta-1}e^{-\left(\frac{t}{\eta}\right)^{\beta}}dt=\eta^m \int_0^\infty x^{\frac{m}{\beta}}e^{-x}dx$$

> First, define $a=\left(\frac{1}{\eta}\right)^{\beta}$ and distribute $T^m$ giving
$$E[T^m]=\int_0^\infty a\beta\left(t\right)^{m+\beta-1}e^{-at^{\beta}}dt$$

> Next, introduce the substitution $x=at^{\beta} \rightarrow t=\left(\frac{x}{a}\right)^{\frac{1}{\beta}} \rightarrow dt=\frac{1}{a\beta}\left(\frac{x}{a}\right)^{-1+\frac{1}{\beta}}dx$ such that
$$
\begin{aligned}
E[T^m]&=\int_0^\infty a\beta\left(\frac{x}{a}\right)^{\frac{m+\beta-1}{\beta}}\text{exp}(-x)\frac{1}{a\beta}\left(\frac{x}{a}\right)^{-1+\frac{1}{\beta}}dx\\
&=\int_0^\infty \left(\frac{x}{a}\right)^{\frac{m}{\beta}+1-\frac{1}{\beta}-1+\frac{1}{\beta}}\text{exp}(-x)dx\\
&={a}^{-\frac{m}{\beta}}\int_0^\infty x^{\frac{m}{\beta}}\text{exp}(-x)dx\\
&={a}^{-\frac{m}{\beta}}\Gamma\left(\frac{m}{\beta}+1\right)\\
E[T^m]&=\eta^m\Gamma\left(\frac{m}{\beta}+1\right).
\end{aligned}
$$

(b) Use the result in (a) to show that $E(T)=\eta\Gamma(1+1/\beta)$ and $Var(T)=\eta^{2}[\Gamma(1+2/\beta)-\Gamma^{2}(1+1/\beta)].$

> Using the result from (a) for $m=1, E[T]=\eta\Gamma\left(\frac{1}{\beta}+1\right)$. Further, it can be shown that
$$
\begin{aligned}
Var[T]&=E[T^2]-(E[T])^2\\
&=\eta^2\Gamma\left(\frac{2}{\beta}+1\right)-\left(\eta\Gamma\left(\frac{1}{\beta}+1\right)\right)^2\\
&=\eta^2\left[\Gamma\left(\frac{2}{\beta}+1\right)-\Gamma^2\left(\frac{1}{\beta}+1\right)\right]
\end{aligned}
$$

`r if(output=='pdf') '\\newpage'`

## From the text problem # 4.7

Consider the Weibull $h(t)$. Note that when $\beta=1, h(t)$ is constant and that when $\beta=2, h(t)$ increases linearly.  Show that:

(a) If $0<\beta<1$, then $h(t)$ is decreasing in $t$

> First, note that for the Weibull distribution
$$
\begin{aligned}
h(t)&=\frac{\beta}{\eta^{\beta}}t^{\beta-1}, \hspace{67pt} t>0, \beta>0, \eta>0\\
h'(t)&=\frac{\beta(\beta-1)}{\eta^{\beta}}t^{\beta-2}, \hspace{40pt} t>0, \beta>0, \eta>0\\
h''(t)&=\frac{\beta(\beta-1)(\beta-2)}{\eta^{\beta}}t^{\beta-3}, \hspace{9pt} t>0, \beta>0, \eta>0
\end{aligned}
$$

> Thus, for $0<\beta<1, h'(t)<0$, and $h(t)$ is decreasing for $\beta \in (0,1)$

(b) If $1<\beta<2,$ then $h(t)$ is concave increasing.

> For $\beta \in (1,2), h'(t)>0$ and $h''(t)<0,$ thus $h(t)$ is concave increasing.

(c) If $\beta>2,$ then $h(t)$ is convex increasing

> For $\beta \in (2,\infty), h'(t)>0$ and $h''(t)>0,$ thus $h(t)$ is convex increasing 

`r if(output=='pdf') '\\newpage'`

## From the text problem # 4.15

The coefficient of variation, $\gamma_{2},$ is a useful scale-free measure of relative variability for a random variable.

(a) Derive an expression for the coefficient of variation for the Weibull distribution 

> Using the results from Exercise 4.5, we can express the Weibull coefficient of variation as
$$\gamma_2=\sqrt{\frac{\sigma^2}{\mu^2}}=\left[\Gamma\left(1+\frac{2}{\beta}\right)\Gamma^{-2}\left(1+\frac{1}{\beta}\right)-1\right]^{0.5}$$

(b) Compute $\gamma_{2}$ for all combinations of $\beta=0.5,1,3,5$ and $\eta=50,100.$  Also, draw (or use the computer to draw) a graph of the Weibull pdfs for the same combinations of parameters.

> As seen in (a), the value of $\gamma_2$ for the Weibull distribution depends only on the value of the shape parameter $\beta$.  For a given value of $\beta$ the value of $\gamma_2$ can be found using the following R code

```{r,echo=TRUE, results='markup'}
beta <- c(0.5,1,3,5)

CV <- matrix(NA, ncol = 1, nrow = 4)

colnames(CV) <- c("CV");
rownames(CV) <- c("eta(0.5) =","eta(1.0) =","eta(3.0) =","eta(5.0) =")

CV[1:4] <- (gamma(1+2/beta[1:4])*(gamma(1+1/beta[1:4]))^-2-1)^0.5
CV
```
    
> The plots of the respective pdfs may then be found using
    
```{r echo=TRUE, fig.cap="Plots of various Weibull density functions"}
beta   <- rep(c(0.5,1,3,5),2)
eta    <- rep(c(50,100),each=4)

for (i in 1:8) {
  curve(dweibull(x,beta[i],eta[i]),
        from = 1,
        to = 100,
        ylim = c(0,.04),
        col = i,
        lty = i,
        xlab = if(i==1) "Time (t)",
        ylab = if(i==1) "f(t)",
        add = if(i>1) TRUE,
        las = 1)
  }
legend("topright", 
       c(expression(eta~beta=="0.5, 50"),expression(eta~beta=="1.0, 50"),
         expression(eta~beta=="3.0, 50"),expression(eta~beta=="5.0, 50"),
         expression(eta~beta=="0.5,100"),expression(eta~beta=="1.0,100"),
         expression(eta~beta=="3.0,100"),expression(eta~beta=="5.0,100")),
       lty = 1:8,lwd = 2,col = 1:8,
       bty = "n",y.intersp = .9)
```

(c) _Explain the effect that changes in_ $\eta$ _and_ $\beta$ _have on the shape of the Weibull density and the effect that they have on_ $\gamma_{2}.$ 

>In problem 4.7 we discussed the effect changing the shape parameter $\beta$ has on the shape of the Weibull distribution, thus we need not discuss it again here.  Increasing the scale parameter $\eta$ or "characteristic life" increases the skewness of the distriution by shifting the $0.632$ quantile to the right.

>The coefficient of variation for the Weibull distribution is a function of the shape parameter alone.  The figure below illustrates the $\gamma_{2}~\beta$ relationship
    
```{r,echo=FALSE,fig.cap="Weibull coefficient of variation vs shape parameter", fig.pos='h'}
CoV <- function(beta) {
  numer = gamma(1+2/beta)
  denom = gamma(1+1/beta)^2
  cov = (numer/denom-1)^0.5
  return(cov)
}
curve(CoV, from = 0.5, to = 15, 
      xlab = expression(beta), 
      ylab = expression(gamma[2]),
      las = 1)
```

`r if(output=='pdf') '\\newpage'`

## From the text problem # 5.2

In some appliations a sample of failure times comes from a mixture of subpopulations.

(a) Write down the expression for the cdf_ $F(t)$ for a mixture of two exponential distributions with means $\theta_{1}=1$ and $\theta_{2}=10$ (subpopulations 1 and 2, respectively) with $\zeta$ being the proportion from subpopulation 1.

> In reliability, mixture distributions can arise when multiple failure mechanisms generate failures for which the respective distributions are sufficiently distinct.

> For the mixture model discussed in this exercise, the pdf is expressed as
$$f_{_X}(x)=\eta \frac{1}{\theta_{1}}e^{-(\frac{x}{\theta_{1}})}+(1-\eta) \frac{1}{\theta_{2}}e^{-(\frac{x}{\theta_{2}})}$$
 
> Integrating wrt $x$ gives
$$F_{_X}(x)=\int_0^x \!f_{_X}(t)dt=\eta (1-e^{-(\frac{x}{\theta_{1}})})+(1-\eta)(1-e^{-(\frac{x}{\theta_{2}})})$$
 
> and finally,
$$F_{_X}(x)=1-\eta e^{-x}-(1-\eta) e^{-(\frac{x}{10})}.$$

(b) For $\zeta=0,0.1,0.5,0.9,$ and 1, compute the mixture $F(t)$ for a number of values of $t$ ranging between 0 and 30.  Plot these distributions on one graph.

> The figure below shows the CDF plots for the requested mixture distributions.  This plot may be generated with the following R code. 

```{r,echo=TRUE, fig.cap="CDF plots for exponential mixture distributions"}
MIX <- function(zeta,t) 1-zeta*exp(-t)-(1-zeta)*exp(-(t/10))

curve(MIX(0.0,x),from = 0,to = 30,xlab = 't',ylab = 'F(t)', las = 1)
curve(MIX(0.1,x),lty = 2,col = 2, add = T)
curve(MIX(0.5,x),lty = 3,col = 3,add = T)
curve(MIX(0.9,x),lty = 4,col = 4,add = T)
curve(MIX(1.0,x),lty = 5,col = 5,add = T)

legend("bottomright", 
       c(expression(zeta==0.0),
         expression(zeta==0.1),
         expression(zeta==0.5), 
         expression(zeta==0.9),
         expression(zeta==1.0)),
       lty = 1:5,lwd = 2, col = 1:5,
       bty = "n",xjust = 0.5)
```

(c) Plot $\log(t)$ versus $\log[-\log[1-F(t)]]$ for each $F(t)$ computed in part (b).  Comment on the shapes of the mixtures of exponential distributions, relative to a pure exponential distribution or a Weibull distribution.

> The same code shown for (b) can be used for (c) applying the tranformations. 

```{r fig.cap="Weibull-transformed quantile plots for the mixture distributions"}
quant <- function(zeta,t) log(-log(1-MIX(zeta,t)))

curve(quant(0.0,x),from = .1,to = 30,xlab = 'Log(t)',ylab = "log(-log(1-F(t)))", log = 'x', las = 1)
curve(quant(0.1,x),lty = 2,col = 2,add = T)
curve(quant(0.5,x),lty = 3,col = 3,add = T)
curve(quant(0.9,x),lty = 4,col = 4,add = T)
curve(quant(1.0,x),lty = 5,col = 5,add = T)

legend("bottomright", 
       c(expression(zeta==0.0),
         expression(zeta==0.1),
         expression(zeta==0.5), 
         expression(zeta==0.9),
         expression(zeta==1.0)),
       lty = 1:5,lwd = 2,col = 1:5,
       bty = "n",xjust = 0.5)
```

(d) Plot the hazard function $h(t)$ on the mixture distributions in part (b).

> As discussed, the hazard function can be expressed as $h(t)=\frac{f_{_X}(x)}{1-F_{_X}(x)}$ Though this exercise began with the pdf for the mixture model, the following code can be used to find $f_{_X}(x)$ symbolically when only the cdf is known.

```{r,echo=TRUE,eval=TRUE}
DMIX<-function(zeta,t)  {
 d.1<-body(MIX)[[2]]       #Extract equation from function object as a call object
 d.2<-as.expression(d.1)   #Convert extracted equation to expression object
 d.3<-D(d.2,"t")           #Take derivative of expression
 d.4<-as.expression(d.3)   #Convert derivative to expression object
 d.5<-eval(d.4)        }   #Convert derivative expression back to a function object
```

> Adjusting the plotting code used in parts (b) and (c) results in the following plot of $h(t)$.

```{r fig.cap="Hazard plots for exponential mixture distributions"}
haz <- function(zeta,t) DMIX(zeta,t)/(1-MIX(zeta,t))

curve(haz(0.0,x),from = 0,to = 15,xlab = 't',ylab = 'h(t)',las = 1, ylim = c(0,1))
curve(haz(0.1,x),lty = 2,col = 2,add = T)
curve(haz(0.5,x),lty = 3,col = 3,add = T)
curve(haz(0.9,x),lty = 4,col = 4,add = T)
curve(haz(1.0,x),lty = 5,col = 5,add = T)

legend("right", 
       c(expression(zeta==0.0),
         expression(zeta==0.1),
         expression(zeta==0.5), 
         expression(zeta==0.9),
         expression(zeta==1.0)),
       lty = 1:5,lwd = 2,col = 1:5,
       bty = "n",xjust = 0.5)
```

(e) Qualitatively, what do the Weibull plots in part (c) suggest about the hazard fucntion of a mixture of two exponential distributions?

> The plots shown in (b), (c) and (d) suggest that the mixture of two exponential distributions with $\eta \in (0,1)$ is not in the exponential family.

`r if(output=='pdf') '\\newpage'`

## From the text problem # 5.4

Refer to Exercise 5.1. Show that a mixture of two exponential distributions with different $\theta$ values with always have a decreasing hazard function

> For a mixture of exponentials, where $\theta_{1}\ne\theta_{2}, \theta_{1}>\theta_{2}$ this problem asks us to show that
$$\frac{d h(x)}{dt}=\left(\frac{f(x)}{S(x)}\right)^2+\frac{f'(x)}{S(x)}<0, x<0.$$

> Rearranging this expression gives $f(x)^2 < -f'(x)S(x)$.  Substituting the density and survival functions for the mixture distribution in this expression results in
$$\left(\frac{\eta}{\theta_{1}}e^{-\frac{x}{\theta_{1}}}+\frac{(1-\eta)}{\theta_{2}}e^{-\frac{x}{\theta_{2}}}\right)\left(\frac{\eta}{\theta_{1}}e^{-\frac{x}{\theta_{1}}}+\frac{(1-\eta)}{\theta_{2}}e^{-\frac{x}{\theta_{2}}}\right)<\left(\frac{\eta}{\theta_{1}}e^{-\frac{x}{\theta_{1}}}+ \frac{(1-\eta)}{\theta_{2}}e^{-\frac{x}{\theta_{2}}}\right)\left(\eta e^{-\frac{x}{\theta_{1}}}+(1-\eta)e^{-\frac{x}{\theta_{2}}}\right)$$

> Expanding this expression gives
$$\frac{\eta^{2}}{\theta_{1}^2}e^{-\frac{2x}{\theta_{1}}}+\frac{2\eta(1-\eta)}{\theta_{1}\theta_{2}}e^{-\left(\frac{x}{\theta_{1}}+\frac{x}{\theta_{2}}\right)}+\frac{(1-\eta)^{2}}{\theta_{2}^{2}}e^{-\frac{2x}{\theta_{2}}}<\frac{\eta^{2}}{\theta_{1}^{2}}e^{-\frac{2x}{\theta_{1}}}+\frac{\eta(1-\eta)}{1}e^{-\left(\frac{x}{\theta_{1}}+\frac{x}{\theta_{2}}\right)}\left(\frac{1}{\theta_{1}^{2}}+\frac{1}{\theta_{2}^{2}}\right)+\frac{(1-\eta)^{2}}{\theta_{2}^{2}}e^{-\frac{2x}{\theta_{2}}}$$

> Simplifying gives
$$\frac{2}{\theta_{1}\theta_{2}}<\frac{1}{\theta_{1}^2}+\frac{1}{\theta_{2}^2}$$
$$0<\frac{1}{\theta_{1}^2}-\frac{2}{\theta_{1}\theta_{2}}+\frac{1}{\theta_{2}^2}$$
$$\left(\frac{1}{\theta_{1}}-\frac{1}{\theta_{2}}\right)^2$$

> As we have defined $\theta_{1}>\theta_{2}$ the quantity $h'(t)<0$.

`r if(output=='pdf') '\\newpage'`

## From the text problem # 5.12

Let $T_{(1)}$ denote the minimum of $m$ independent Weibull random variables with parameters $\mu_{i}, i=1,...m,$ and constant $\sigma.$ Show that $T_{(1)}$ has a Weibull distribution. 

> Let $T_{(1)}=\min(T_{1},...,T_{m})$ denote the minimum failure time observed from $m$ independent Weibull random variables having distinct scale parameters $\mu_{i}, i=1,...,m$ and an equilvalent shape parameter $\sigma$. Then
$$
\begin{aligned}
F_{T_{(1)}}(t)&=Pr(T_{(1)}<t)\\
&=1-Pr(T_{(1)}>t)\\
&=1-\prod_{j=1}^m Pr(T_{j}>t)\\
&=1-\prod_{j=1}^m \text{exp}\left[-\left(\frac{t}{\text{exp}(\mu_{j})}\right)^\frac{1}{\sigma}\right]\\
&=1-\text{exp}\left[-\sum_{j=1}^m\left(\frac{t}{\text{exp}(\mu_{j})}\right)^\frac{1}{\sigma}\right]\\
&=1-\text{exp}\left[-t^\frac{1}{\sigma} \times \sum_{j=1}^m\left(\frac{1}{\text{exp}(\mu_{j})}\right)^\frac{1}{\sigma}\right]\\
&=1-\text{exp}\left[-\left(\frac{t}{\text{exp}[\mu_{(1)}]}\right)^\frac{1}{\sigma}\right]
\end{aligned}
$$

> which is in the form of a Weibull cdf for which the scale parameter is
$$\text{exp}[\mu_{(1)}]=\left[\sum_{j=1}^m\left(\frac{1}{\text{exp}(\mu_{j})}\right)^\frac{1}{\sigma}\right]^{-\sigma}$$
